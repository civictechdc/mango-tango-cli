{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Welcome to the technical documentation website of CIB Mango Tree, a collaborative and open-source project to develop software that tests for coordinated inauthentic behavior (CIB) in datasets of social media activity.</p> <p>This is the technical documentation, for a user-based perspective see the project main home page: cibmangotree.org</p>"},{"location":"license/","title":"License","text":"<pre><code># PolyForm Noncommercial License 1.0.0\n\n&lt;https://polyformproject.org/licenses/noncommercial/1.0.0&gt;\n\n## Acceptance\n\nIn order to get any license under these terms, you must agree\nto them as both strict obligations and conditions to all\nyour licenses.\n\n## Copyright License\n\nThe licensor grants you a copyright license for the\nsoftware to do everything you might do with the software\nthat would otherwise infringe the licensor's copyright\nin it for any permitted purpose.  However, you may\nonly distribute the software according to [Distribution\nLicense](#distribution-license) and make changes or new works\nbased on the software according to [Changes and New Works\nLicense](#changes-and-new-works-license).\n\n## Distribution License\n\nThe licensor grants you an additional copyright license\nto distribute copies of the software.  Your license\nto distribute covers distributing the software with\nchanges and new works permitted by [Changes and New Works\nLicense](#changes-and-new-works-license).\n\n## Notices\n\nYou must ensure that anyone who gets a copy of any part of\nthe software from you also gets a copy of these terms or the\nURL for them above, as well as copies of any plain-text lines\nbeginning with `Required Notice:` that the licensor provided\nwith the software.  For example:\n\n&gt; Required Notice: Copyright Yoyodyne, Inc. (http://example.com)\n\n## Changes and New Works License\n\nThe licensor grants you an additional copyright license to\nmake changes and new works based on the software for any\npermitted purpose.\n\n## Patent License\n\nThe licensor grants you a patent license for the software that\ncovers patent claims the licensor can license, or becomes able\nto license, that you would infringe by using the software.\n\n## Noncommercial Purposes\n\nAny noncommercial purpose is a permitted purpose.\n\n## Personal Uses\n\nPersonal use for research, experiment, and testing for\nthe benefit of public knowledge, personal study, private\nentertainment, hobby projects, amateur pursuits, or religious\nobservance, without any anticipated commercial application,\nis use for a permitted purpose.\n\n## Noncommercial Organizations\n\nUse by any charitable organization, educational institution,\npublic research organization, public safety or health\norganization, environmental protection organization,\nor government institution is use for a permitted purpose\nregardless of the source of funding or obligations resulting\nfrom the funding.\n\n## Fair Use\n\nYou may have \"fair use\" rights for the software under the\nlaw. These terms do not limit them.\n\n## No Other Rights\n\nThese terms do not allow you to sublicense or transfer any of\nyour licenses to anyone else, or prevent the licensor from\ngranting licenses to anyone else.  These terms do not imply\nany other licenses.\n\n## Patent Defense\n\nIf you make any written claim that the software infringes or\ncontributes to infringement of any patent, your patent license\nfor the software granted under these terms ends immediately. If\nyour company makes such a claim, your patent license ends\nimmediately for work on behalf of your company.\n\n## Violations\n\nThe first time you are notified in writing that you have\nviolated any of these terms, or done anything with the software\nnot covered by your licenses, your licenses can nonetheless\ncontinue if you come into full compliance with these terms,\nand take practical steps to correct past violations, within\n32 days of receiving notice.  Otherwise, all your licenses\nend immediately.\n\n## No Liability\n\n***As far as the law allows, the software comes as is, without\nany warranty or condition, and the licensor will not be liable\nto you for any damages arising out of these terms or the use\nor nature of the software, under any kind of legal claim.***\n\n## Definitions\n\nThe **licensor** is the individual or entity offering these\nterms, and the **software** is the software the licensor makes\navailable under these terms.\n\n**You** refers to the individual or entity agreeing to these\nterms.\n\n**Your company** is any legal entity, sole proprietorship,\nor other kind of organization that you work for, plus all\norganizations that have control over, are under the control of,\nor are under common control with that organization.  **Control**\nmeans ownership of substantially all the assets of an entity,\nor the power to direct its management and policies by vote,\ncontract, or otherwise.  Control can be direct or indirect.\n\n**Your licenses** are all the licenses granted to you for the\nsoftware under these terms.\n\n**Use** means anything you do with the software requiring one\nof your licenses.\n</code></pre>"},{"location":"guides/contributing/analyzers/","title":"Implementing Analyzers","text":""},{"location":"guides/contributing/analyzers/#analyzers-guide","title":"Analyzers Guide","text":"<p>Analyzers are the core data processing components of the platform. They follow a three-tier architecture that separates data processing, analysis, and presentation concerns.</p>"},{"location":"guides/contributing/analyzers/#architecture-overview","title":"Architecture Overview","text":"<p>The analyzer system consists of three types of components:</p> <ol> <li>Primary Analyzer: Performs the core data analysis and outputs structured results</li> <li>Secondary Analyzer: Processes primary analyzer outputs for specific use cases or exports</li> <li>Web Presenter: Creates interactive dashboards and visualizations</li> </ol> <p>This separation allows for:</p> <ul> <li>Reusable analysis logic</li> <li>Multiple presentation formats for the same analysis</li> <li>Collaborative development where different contributors can focus on different layers</li> </ul>"},{"location":"guides/contributing/analyzers/#primary-analyzers","title":"Primary Analyzers","text":"<p>Primary analyzers perform the core data analysis. They read user input data, process it according to their algorithm, and output structured results in parquet format.</p>"},{"location":"guides/contributing/analyzers/#interface-definition","title":"Interface Definition","text":"<p>Every primary analyzer must define an interface that specifies:</p> <ul> <li>Input columns required from the user</li> <li>Parameters the analyzer accepts</li> <li>Output tables the analyzer produces</li> </ul> <pre><code>from analyzer_interface import (\n    AnalyzerInput,\n    AnalyzerInterface, \n    AnalyzerOutput,\n    AnalyzerParam,\n    InputColumn,\n    OutputColumn,\n    IntegerParam\n)\n\ninterface = AnalyzerInterface(\n    id=\"example_analyzer\",  # Must be globally unique\n    version=\"0.1.0\",\n    name=\"Example Analyzer\",\n    short_description=\"Counts characters in messages\",\n    long_description=\"\"\"\nThis analyzer demonstrates the basic structure by counting \ncharacters in each message and marking long messages.\n    \"\"\",\n    input=AnalyzerInput(\n        columns=[\n            InputColumn(\n                name=\"message_id\",\n                human_readable_name=\"Unique Message ID\", \n                data_type=\"identifier\",\n                description=\"The unique identifier of the message\",\n                name_hints=[\"post\", \"message\", \"tweet\", \"id\"]\n            ),\n            InputColumn(\n                name=\"message_text\",\n                human_readable_name=\"Message Text\",\n                data_type=\"text\", \n                description=\"The text content of the message\",\n                name_hints=[\"message\", \"text\", \"content\", \"body\"]\n            )\n        ]\n    ),\n    params=[\n        AnalyzerParam(\n            id=\"fudge_factor\",\n            human_readable_name=\"Character Count Adjustment\",\n            description=\"Adds to the character count for testing purposes\",\n            type=IntegerParam(min=-1000, max=1000),\n            default=0\n        )\n    ],\n    outputs=[\n        AnalyzerOutput(\n            id=\"character_count\",\n            name=\"Character Count Per Message\", \n            internal=True,  # Not shown in export list\n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\")\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#implementation","title":"Implementation","text":"<p>The main function receives a context object with access to input data and output paths:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import PrimaryAnalyzerContext\nfrom terminal_tools import ProgressReporter\n\ndef main(context: PrimaryAnalyzerContext):\n    # Read and preprocess input data\n    input_reader = context.input()\n    df_input = input_reader.preprocess(pl.read_parquet(input_reader.parquet_path))\n\n    # Access parameters\n    fudge_factor = context.params.get(\"fudge_factor\")\n    assert isinstance(fudge_factor, int), \"Fudge factor must be an integer\"\n\n    # Perform analysis with progress reporting\n    with ProgressReporter(\"Counting characters\") as progress:\n        df_count = df_input.select(\n            pl.col(\"message_id\"),\n            pl.col(\"message_text\")\n            .str.len_chars()\n            .add(fudge_factor)\n            .alias(\"character_count\")\n        )\n        progress.update(1.0)\n\n    # Write output to specified path\n    df_count.write_parquet(context.output(\"character_count\").parquet_path)\n</code></pre>"},{"location":"guides/contributing/analyzers/#declaration","title":"Declaration","text":"<p>Finally, create the analyzer declaration:</p> <pre><code>from analyzer_interface import AnalyzerDeclaration\nfrom .interface import interface\nfrom .main import main\n\nexample_analyzer = AnalyzerDeclaration(\n    interface=interface,\n    main=main,\n    is_distributed=False  # Set to True for production analyzers\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#secondary-analyzers","title":"Secondary Analyzers","text":"<p>Secondary analyzers process the output of primary analyzers to create user-friendly exports or perform additional analysis.</p>"},{"location":"guides/contributing/analyzers/#interface-definition_1","title":"Interface Definition","text":"<p>Secondary analyzers specify their base primary analyzer and their own outputs:</p> <pre><code>from analyzer_interface import AnalyzerOutput, OutputColumn, SecondaryAnalyzerInterface\nfrom ..example_base.interface import interface as example_base\n\ninterface = SecondaryAnalyzerInterface(\n    id=\"example_report\",\n    version=\"0.1.0\", \n    name=\"Example Report\",\n    short_description=\"Adds 'is_long' flag to character count analysis\",\n    base_analyzer=example_base,  # Reference to primary analyzer\n    outputs=[\n        AnalyzerOutput(\n            id=\"example_report\",\n            name=\"Example Report\", \n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\"),\n                OutputColumn(name=\"is_long\", data_type=\"boolean\")  # New column\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#implementation_1","title":"Implementation","text":"<p>Secondary analyzers read primary outputs and create enhanced results:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import SecondaryAnalyzerContext\n\ndef main(context: SecondaryAnalyzerContext):\n    # Read primary analyzer output\n    df_character_count = pl.read_parquet(\n        context.base.table(\"character_count\").parquet_path\n    )\n\n    # Add derived columns\n    df_export = df_character_count.with_columns(\n        pl.col(\"character_count\").gt(100).alias(\"is_long\")\n    )\n\n    # Access primary analyzer parameters if needed\n    fudge_factor = context.base_params.get(\"fudge_factor\")\n\n    # Write enhanced output\n    df_export.write_parquet(context.output(\"example_report\").parquet_path)\n</code></pre>"},{"location":"guides/contributing/analyzers/#web-presenters","title":"Web Presenters","text":"<p>Web presenters create interactive dashboards using either Dash or Shiny frameworks.</p>"},{"location":"guides/contributing/analyzers/#interface-definition_2","title":"Interface Definition","text":"<pre><code>from analyzer_interface import WebPresenterInterface\nfrom ..example_base import interface as example_base\nfrom ..example_report import interface as example_report\n\ninterface = WebPresenterInterface(\n    id=\"example_web\", \n    version=\"0.1.0\",\n    name=\"Message Length Histogram\",\n    short_description=\"Shows distribution of message lengths\",\n    base_analyzer=example_base,\n    depends_on=[example_report]  # Secondary analyzers used\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#shiny-implementation","title":"Shiny Implementation","text":"<p>For more interactive dashboards:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Define UI components\n    analysis_panel = ui.card(\n        ui.card_header(\"Character Count Analysis\"),\n        ui.input_checkbox(\"show_details\", \"Show detailed view\", value=False),\n        output_widget(\"histogram\", height=\"400px\")\n    )\n\n    def server(input, output, session):\n        @render_widget\n        def histogram():\n            # Create interactive plot based on inputs\n            show_details = input.show_details()\n            # ... create plotly figure ...\n            return fig\n\n        @render.text\n        def summary():\n            return f\"Total messages: {len(df)}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", analysis_panel)\n        )\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#api-factory-for-react-dashboards","title":"API Factory for React Dashboards","text":"<p>Web presenters can also implement an <code>api_factory</code> function to provide structured data for React-based frontends through REST API endpoints:</p> <pre><code>from ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Provides structured data for React dashboards via API endpoints.\n\n    Args:\n        context: WebPresenterContext with access to analyzer outputs\n        options: Optional parameters from API requests (filters, etc.)\n\n    Returns:\n        Dict with presenter metadata and processed data arrays\n    \"\"\"\n    # Extract API options/filters\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load data\n    data_frame = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Apply filters if provided\n    if filter_value:\n        # Apply filtering logic based on the filter_value\n        data_frame = data_frame.filter(pl.col(\"message_text\").str.contains(filter_value))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model[\"figure_type\"] = \"histogram\"\n    presenter_model[\"axis\"] = {\n        \"x\": {\"label\": \"Message Character Count\", \"value\": \"message_character_count\"},\n        \"y\": {\"label\": \"Number of Messages\", \"value\": \"number_of_messages\"}\n    }\n\n    # Add data arrays for the frontend\n    presenter_model[\"x\"] = data_frame[\"character_count\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple outputs, return a dictionary with different data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load different data sources\n    df_stats = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_STATS).parquet_path\n    )\n    df_full = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_FULL).parquet_path\n    )\n\n    # Apply filtering to both datasets\n    if filter_value:\n        matcher = create_word_matcher(filter_value, pl.col(COL_NGRAM_WORDS))\n        if matcher is not None:\n            df_stats = df_stats.filter(matcher)\n            df_full = df_full.filter(matcher)\n\n    # Create separate presenter models for each output\n    stats_model = context.web_presenter.model_dump()\n    full_model = context.web_presenter.model_dump()\n\n    # Configure stats view\n    stats_model.update({\n        \"figure_type\": \"scatter\",\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"amplification_factor\": \"N-grams higher up are repeated more times...\"\n        },\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"x\": df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT].to_list(),\n        \"y\": {\n            \"total_repetition\": df_stats[COL_NGRAM_TOTAL_REPS].to_list(),\n            \"amplification_factor\": (\n                df_stats[COL_NGRAM_TOTAL_REPS] / \n                df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT]\n            ).to_list()\n        },\n        \"ngrams\": df_stats[COL_NGRAM_WORDS].to_list()\n    })\n\n    # Configure full data view  \n    full_model.update({\n        \"figure_type\": \"scatter\",\n        \"ids\": df_full[COL_NGRAM_ID].to_list(),\n        \"timestamps\": df_full[COL_MESSAGE_TIMESTAMP].to_list(),\n        \"messages\": df_full[COL_MESSAGE_TEXT].to_list(),\n        \"users\": df_full[COL_AUTHOR_ID].to_list(),\n        # ... additional fields for detailed view\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": OUTPUT_NGRAM_STATS,\n            OUTPUT_NGRAM_STATS: pop_unnecessary_fields(stats_model),\n            OUTPUT_NGRAM_FULL: pop_unnecessary_fields(full_model)\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through REST endpoints:</p> <ul> <li><code>GET /api/presenters</code> - List all presenter data</li> <li><code>GET /api/presenters/{id}</code> - Get specific presenter data</li> <li><code>GET /api/presenters/{id}/download/{format}</code> - Download data as CSV/JSON/Excel</li> </ul> <p>Query parameters:</p> <ul> <li><code>output</code> - Specify which output to return (for multi-output presenters)</li> <li><code>filter_field</code> &amp; <code>filter_value</code> - Apply filtering</li> <li><code>matcher</code> - Text matching filter (passed to api_factory options)</li> </ul> <p>Example API usage:</p> <pre><code># Get basic presenter data\ncurl \"/api/presenters/ngram_repetition_by_poster\"\n\n# Get filtered data\ncurl \"/api/presenters/ngram_repetition_by_poster?filter_value=climate&amp;matcher=climate\"\n\n# Get specific output\ncurl \"/api/presenters/ngram_repetition_by_poster?output=ngram_full\"\n\n# Download as CSV\ncurl \"/api/presenters/ngram_repetition_by_poster/download/csv\"\n</code></pre>"},{"location":"guides/contributing/analyzers/#testing-analyzers","title":"Testing Analyzers","text":""},{"location":"guides/contributing/analyzers/#testing-primary-analyzers","title":"Testing Primary Analyzers","text":"<pre><code>from testing import CsvTestData, test_primary_analyzer\nfrom .interface import interface\nfrom .main import main\n\ndef test_example_analyzer():\n    test_primary_analyzer(\n        interface=interface,\n        main=main,\n        input=CsvTestData(\n            \"test_input.csv\",\n            semantics={\"message_id\": identifier}\n        ),\n        params={\"fudge_factor\": 10},\n        outputs={\n            \"character_count\": CsvTestData(\"expected_output.csv\")\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#testing-secondary-analyzers","title":"Testing Secondary Analyzers","text":"<pre><code>from testing import test_secondary_analyzer, ParquetTestData\n\ndef test_example_report():\n    test_secondary_analyzer(\n        interface=interface,\n        main=main,\n        primary_params={\"fudge_factor\": 10},\n        primary_outputs={\n            \"character_count\": ParquetTestData(\"primary_output.parquet\")\n        },\n        expected_outputs={\n            \"example_report\": ParquetTestData(\"expected_report.parquet\")\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#best-practices","title":"Best Practices","text":""},{"location":"guides/contributing/analyzers/#data-processing","title":"Data Processing","text":"<ul> <li>Always call <code>input_reader.preprocess()</code> on input data in primary analyzers</li> <li>Use <code>ProgressReporter</code> for long-running operations</li> <li>Handle missing or null data gracefully</li> <li>Use appropriate data types (avoid defaulting to small integer types)</li> </ul>"},{"location":"guides/contributing/analyzers/#interface-design","title":"Interface Design","text":"<ul> <li>Choose descriptive, globally unique IDs</li> <li>Provide comprehensive <code>name_hints</code> for better column matching</li> <li>Mark internal outputs that users shouldn't see directly</li> <li>Include helpful parameter descriptions and validation</li> </ul>"},{"location":"guides/contributing/analyzers/#performance","title":"Performance","text":"<ul> <li>Use lazy evaluation with Polars when possible</li> <li>Process data in chunks for large datasets</li> <li>Consider memory usage when designing algorithms</li> <li>Use appropriate file formats (parquet for structured data)</li> </ul>"},{"location":"guides/contributing/analyzers/#error-handling","title":"Error Handling","text":"<ul> <li>Validate parameters and input data</li> <li>Provide meaningful error messages</li> <li>Handle edge cases (empty datasets, missing columns)</li> <li>Use assertions for internal consistency checks</li> </ul>"},{"location":"guides/contributing/analyzers/#adding-to-the-suite","title":"Adding to the Suite","text":"<p>Register all analyzers in <code>analyzers/__init__.py</code>:</p> <pre><code>from analyzer_interface import AnalyzerSuite\nfrom .example.example_base import example_base\nfrom .example.example_report import example_report  \nfrom .example.example_web import example_web\n\nsuite = AnalyzerSuite(\n    all_analyzers=[\n        example_base,\n        example_report, \n        example_web,\n        # ... other analyzers\n    ]\n)\n</code></pre> <p>This creates a complete analysis pipeline that users can run through the application interface, from data input through interactive visualization.</p>"},{"location":"guides/contributing/analyzers/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it would be a good idea to review the sections for each domain. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/contributing/contributing/","title":"Contributor Workflow","text":"<p>Before following this workflow please refer to our Getting Started page for instructions on installing dependencies and setting up your development environment.</p>"},{"location":"guides/contributing/contributing/#contributor-workflow","title":"Contributor Workflow","text":""},{"location":"guides/contributing/contributing/#overview","title":"Overview","text":"<p>All changes should be made in a feature branch, merged into <code>develop</code>, and later merged into <code>main</code> for a new release.</p>"},{"location":"guides/contributing/contributing/#contributing-new-changes","title":"Contributing new changes","text":"<ol> <li>Create a Feature Branch</li> <li>Branch from <code>develop</code> using <code>feature/&lt;name&gt;</code> or <code>bugfix/&lt;name&gt;</code>.</li> <li> <p>Example:</p> <p><code>shell  git checkout develop  git pull origin develop  git checkout -b feature/new-feature</code></p> </li> <li> <p>Make Changes &amp; Push</p> </li> <li>Commit changes with clear messages.</li> <li> <p>Push the branch.</p> <p><code>shell  git add .  git commit -m \"Description of changes\"  git push origin feature/new-feature</code></p> </li> <li> <p>Create a Pull Request</p> </li> <li>Open a PR to merge into <code>develop</code>.</li> <li> <p>Address any review feedback.</p> </li> <li> <p>Merge &amp; Clean Up</p> </li> <li>After approval, merge into <code>develop</code>.</li> <li> <p>Delete the feature branch.</p> </li> <li> <p>Release</p> </li> <li>When develop is clean and ready for a new major release, we will merge <code>develop</code> into <code>main</code>.</li> </ol>"},{"location":"guides/contributing/contributing/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph TD;\n    A[Feature Branch] --&gt;|Commit &amp; Push| B[Pull Request];\n    B --&gt;|Review &amp; Merge| C[Develop Branch];\n    C --&gt;|Release| D[Main Branch];\n</code></pre>"},{"location":"guides/contributing/contributing/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/contributing/logging/","title":"Logging","text":""},{"location":"guides/contributing/logging/#logging","title":"Logging","text":"<p>The application uses a structured JSON logging system that provides consistent logging across all modules. The logging system automatically separates critical alerts from diagnostic information.</p>"},{"location":"guides/contributing/logging/#logging-architecture","title":"Logging Architecture","text":"<ul> <li>Console Output: Only\u00a0<code>ERROR</code>\u00a0and\u00a0<code>CRITICAL</code>\u00a0messages are displayed on stderr</li> <li>File Output: All messages from\u00a0<code>INFO</code>\u00a0level and above are written to log files</li> <li>Log Format: All logs are structured JSON for easy parsing and analysis</li> <li>Log Rotation: Log files automatically rotate at 10MB with 5 backup files retained</li> <li>Log Location:\u00a0<code>~/.local/share/MangoTango/logs/mangotango.log</code>\u00a0(varies by platform)</li> </ul>"},{"location":"guides/contributing/logging/#using-the-logger-in-your-code","title":"Using the Logger in Your Code","text":""},{"location":"guides/contributing/logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from app.logger import get_logger\n\n# Get a logger for your module\nlogger = get_logger(__name__)\n\n# Log at different levels\nlogger.debug(\"Detailed debugging information\")\nlogger.info(\"General information about program execution\")\nlogger.warning(\"Something unexpected happened, but the program continues\")\nlogger.error(\"A serious problem occurred\")\nlogger.critical(\"A very serious error occurred, program may not be able to continue\")\n</code></pre>"},{"location":"guides/contributing/logging/#example-log-output","title":"Example Log Output","text":"<p>Console (stderr) - Only errors:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre> <p>Log File - All info and above:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,910\", \"name\": \"analyzers.hashtags\", \"levelname\": \"INFO\", \"message\": \"Starting hashtag analysis\", \"taskName\": null}\n{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre>"},{"location":"guides/contributing/logging/#logging-in-analyzers","title":"Logging in Analyzers","text":"<p>When developing analyzers, add logging to help with debugging and monitoring:</p> <pre><code>from app.logger import get_logger\n\ndef main(context):\n    logger = get_logger(__name__)\n\n    logger.info(\"Starting analysis\", extra={\n        \"input_path\": str(context.input_path),\n        \"output_path\": str(context.output_path)\n    })\n\n    try:\n        # Your analysis code here\n        result = perform_analysis(context)\n\n        logger.info(\"Analysis completed successfully\", extra={\n            \"records_processed\": len(result),\n            \"execution_time\": time.time() - start_time\n        })\n\n    except Exception as e:\n        logger.error(\"Analysis failed\", extra={\n            \"error\": str(e),\n            \"error_type\": type(e).__name__\n        }, exc_info=True)\n        raise\n</code></pre>"},{"location":"guides/contributing/logging/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li> <p>Use Appropriate Log Levels:</p> <ul> <li><code>DEBUG</code>: Detailed diagnostic information, only useful when debugging</li> <li><code>INFO</code>: General information about program execution</li> <li><code>WARNING</code>: Something unexpected happened, but the program continues</li> <li><code>ERROR</code>: A serious problem occurred</li> <li><code>CRITICAL</code>: A very serious error occurred, program may not be able to continue</li> <li>Include Context with\u00a0<code>extra</code>\u00a0Parameter:</li> </ul> <p><code>python logger.info(\"Processing file\", extra={     \"filename\": filename,     \"file_size\": file_size,     \"record_count\": record_count })</code></p> </li> <li> <p>Log Exceptions Properly:</p> <p><code>python try:     risky_operation() except Exception as e:     logger.error(\"Operation failed\", exc_info=True)  # Includes stack trace</code></p> </li> <li> <p>Avoid Logging Sensitive Information:</p> <ul> <li>Never log passwords, API keys, or personal data</li> <li>Be cautious with user-provided data</li> </ul> </li> </ol>"},{"location":"guides/contributing/logging/#debugging-with-logs","title":"Debugging with Logs","text":"<p>Users can control log verbosity when running the application:</p> <pre><code># Default INFO level\npython -m mangotango\n\n# Verbose DEBUG level for troubleshooting\npython -m mangotango --log-level DEBUG\n\n# Only show warnings and errors in log file\npython -m mangotango --log-level WARNING\n</code></pre>"},{"location":"guides/contributing/logging/#log-file-management","title":"Log File Management","text":"<ul> <li>Log files are automatically rotated when they reach 10MB</li> <li>Up to 5 backup files are kept (<code>mangotango.log.1</code>,\u00a0<code>mangotango.log.2</code>, etc.)</li> <li>Older backup files are automatically deleted</li> <li>Log directory is created automatically if it doesn't exist</li> </ul>"},{"location":"guides/contributing/logging/#testing-with-logs","title":"Testing with Logs","text":"<p>When writing tests that involve logging:</p> <pre><code>import logging\nfrom app.logger import get_logger\n\ndef test_my_function_logs_correctly(caplog):\n    with caplog.at_level(logging.INFO):\n        my_function()\n\n    assert \"Expected log message\" in caplog.text\n</code></pre>"},{"location":"guides/contributing/logging/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/contributing/testing/","title":"Testing","text":""},{"location":"guides/contributing/testing/#testing","title":"Testing","text":"<p>The <code>testing</code> module provides testers for the primary and secondary analyzer modules. See the example for further references.</p>"},{"location":"guides/contributing/dashboards/react/","title":"React (WIP)","text":""},{"location":"guides/contributing/dashboards/react/#react-dashboards-guide","title":"React Dashboards Guide","text":"<p>Web presenters can create modern, client-side dashboards using React for rich interactivity and responsive user experiences. React dashboards consume data through REST APIs and provide smooth, app-like interfaces.</p>"},{"location":"guides/contributing/dashboards/react/#overview","title":"Overview","text":"<p>React dashboards in this platform provide:</p> <ul> <li>Client-side rendering: Fast, responsive user interfaces</li> <li>API-driven: Clean separation between data and presentation</li> <li>Modern UI components: Built with shadcn/ui and Tailwind CSS</li> <li>Rich visualizations: Interactive charts with Deck.gl and Visx</li> <li>Real-time interactions: Immediate feedback and smooth animations</li> </ul>"},{"location":"guides/contributing/dashboards/react/#architecture","title":"Architecture","text":"<p>React dashboards follow a three-tier architecture:</p> <ol> <li>Web Presenter (Python): Implements <code>api_factory</code> to serve structured data via REST API</li> <li>API Layer: Automatically generated endpoints that serve presenter data as JSON</li> <li>React Frontend: TypeScript components that consume the API and render interactive UI</li> </ol> <pre><code>graph TB\n    A[Analyzer Data] --&gt; B[Web Presenter api_factory]\n    B --&gt; C[REST API Endpoints]\n    C --&gt; D[React Components]\n    D --&gt; E[Interactive Dashboard]\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#setting-up-api-factory","title":"Setting Up API Factory","text":"<p>The <code>api_factory</code> function transforms analyzer outputs into structured data for React consumption:</p>"},{"location":"guides/contributing/dashboards/react/#basic-api-factory","title":"Basic API Factory","text":"<pre><code>from typing import Optional, Any\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext\nfrom ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Transform analyzer data for React dashboard consumption.\n\n    Args:\n        context: Access to analyzer outputs and metadata\n        options: Query parameters from API requests (filters, pagination, etc.)\n\n    Returns:\n        FactoryOutputContext with API-formatted data\n    \"\"\"\n    # Extract API options\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    matcher = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"main_output\").parquet_path)\n\n    # Apply filtering based on API parameters\n    if filter_value:\n        df = df.filter(pl.col(\"category\") == filter_value)\n\n    if matcher:\n        df = df.filter(pl.col(\"text_field\").str.contains(matcher, literal=False))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model.update({\n        \"figure_type\": \"scatter\",  # histogram, bar, scatter\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Message Count\", \"value\": \"message_count\"}\n        },\n        \"explanation\": {\n            \"main_view\": \"This chart shows the relationship between users and messages...\"\n        }\n    })\n\n    # Add data arrays for visualization\n    presenter_model[\"x\"] = df[\"x_column\"].to_list()\n    presenter_model[\"y\"] = df[\"y_column\"].to_list()\n    presenter_model[\"labels\"] = df[\"label_column\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Determine which output to return\n    output_type = options.get(\"output\", \"default\") if options else \"default\"\n\n    # Load different datasets\n    df_summary = pl.read_parquet(\n        context.base.table(\"summary_stats\").parquet_path\n    )\n    df_details = pl.read_parquet(\n        context.dependency(detail_analyzer).table(\"full_details\").parquet_path\n    )\n\n    # Apply common filtering\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    if filter_value:\n        df_summary = df_summary.filter(pl.col(\"category\") == filter_value)\n        df_details = df_details.filter(pl.col(\"category\") == filter_value)\n\n    # Create different presenter models for each output\n    base_model = context.web_presenter.model_dump()\n\n    summary_model = base_model.copy()\n    summary_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"Users\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Messages\", \"value\": \"message_count\"}\n        },\n        \"x\": df_summary[\"user_count\"].to_list(),\n        \"y\": df_summary[\"message_count\"].to_list(),\n        \"labels\": df_summary[\"category\"].to_list()\n    })\n\n    details_model = base_model.copy()\n    details_model.update({\n        \"figure_type\": \"table\",\n        \"columns\": [\"user_id\", \"message_text\", \"timestamp\", \"category\"],\n        \"data\": df_details.to_dicts()\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": \"summary\",\n            \"summary\": pop_unnecessary_fields(summary_model),\n            \"details\": pop_unnecessary_fields(details_model)\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#complex-filtering-and-search","title":"Complex Filtering and Search","text":"<pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Load base data\n    df = pl.read_parquet(context.base.table(\"ngram_analysis\").parquet_path)\n\n    # Extract search/filter parameters\n    search_term = options.get(\"matcher\", \"\") if options else \"\"\n    date_range = options.get(\"date_range\") if options else None\n    min_frequency = options.get(\"min_frequency\", 0) if options else 0\n\n    # Apply filters progressively\n    if search_term:\n        # Create word matcher for ngram search\n        search_filter = pl.col(\"ngram_text\").str.contains(\n            search_term.lower(), literal=False\n        )\n        df = df.filter(search_filter)\n\n    if date_range:\n        start_date, end_date = date_range.split(\",\")\n        df = df.filter(\n            pl.col(\"timestamp\").is_between(\n                pl.datetime(start_date), \n                pl.datetime(end_date)\n            )\n        )\n\n    if min_frequency &gt; 0:\n        df = df.filter(pl.col(\"frequency\") &gt;= min_frequency)\n\n    # Sort and limit results for performance\n    df = df.sort(\"frequency\", descending=True).head(1000)\n\n    # Build API response\n    presenter_model = context.web_presenter.model_dump()\n    presenter_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"User Repetition\", \"value\": \"user_repetition\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"user_repetition\": \"N-grams higher up show higher amplification...\"\n        },\n        # Data for visualization\n        \"x\": df[\"user_count\"].to_list(),\n        \"y\": df[\"total_count\"].to_list(),\n        \"ngrams\": df[\"ngram_text\"].to_list(),\n        \"frequencies\": df[\"frequency\"].to_list(),\n        \"rankings\": list(range(1, len(df) + 1))\n    })\n\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through these REST endpoints:</p>"},{"location":"guides/contributing/dashboards/react/#standard-endpoints","title":"Standard Endpoints","text":"<pre><code># List all presenters\nGET /api/presenters\n\n# Get specific presenter data  \nGET /api/presenters/{presenter_id}\n\n# Get specific output (for multi-output presenters)\nGET /api/presenters/{presenter_id}?output=details\n\n# Apply filters\nGET /api/presenters/{presenter_id}?filter_field=category&amp;filter_value=news\n\n# Search/match text\nGET /api/presenters/{presenter_id}?matcher=climate\n\n# Combine parameters\nGET /api/presenters/{presenter_id}?output=summary&amp;matcher=election&amp;min_frequency=5\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#download-endpoints","title":"Download Endpoints","text":"<pre><code># Download as CSV\nGET /api/presenters/{presenter_id}/download/csv\n\n# Download as JSON\nGET /api/presenters/{presenter_id}/download/json\n\n# Download as Excel\nGET /api/presenters/{presenter_id}/download/excel\n\n# Download with filters applied\nGET /api/presenters/{presenter_id}/download/csv?filter_value=news&amp;matcher=climate \n</code></pre>"},{"location":"guides/contributing/dashboards/react/#react-component-architecture","title":"React Component Architecture","text":"<p>The React frontend is organized into reusable components that work together to create cohesive dashboards:</p>"},{"location":"guides/contributing/dashboards/react/#core-component-structure","title":"Core Component Structure","text":"<pre><code>// Main presenter component\nexport default function NgramScatterPlot({ presenter }: ChartContainerProps&lt;NgramPresenterStats&gt;): ReactElement&lt;FC&gt; {\n    const [searchValue, setSearchValue] = useState&lt;string&gt;('');\n    const [selectedItem, setSelectedItem] = useState&lt;string&gt;('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n\n    // Data fetching and state management\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchValue,\n        output: currentTab\n    });\n\n    // Event handlers\n    const handleSearch = (value: string) =&gt; setSearchValue(value);\n    const handleItemSelect = (item: DataPoint) =&gt; setSelectedItem(item.id);\n    const handleTabChange = (tab: string) =&gt; setCurrentTab(tab);\n\n    return (\n        &lt;div className=\"space-y-6\"&gt;\n            {/* Controls */}\n            &lt;div className=\"flex justify-between items-center\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} onClear={() =&gt; setSearchValue('')} /&gt;\n                &lt;DownloadButton presenterID={presenter.id} /&gt;\n            &lt;/div&gt;\n\n            {/* Tabs for different views */}\n            &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                &lt;TabsList&gt;\n                    &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                    &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification&lt;/TabsTrigger&gt;\n                &lt;/TabsList&gt;\n\n                &lt;TabsContent value=\"total_repetition\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('total_repetition')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n\n                &lt;TabsContent value=\"amplification_factor\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('amplification_factor')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n            &lt;/Tabs&gt;\n\n            {/* Data table */}\n            &lt;DataTable \n                data={data}\n                columns={tableColumns}\n                onRowSelect={handleItemSelect}\n                selectedRows={selectedItem ? [selectedItem] : []}\n            /&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#data-fetching-hooks","title":"Data Fetching Hooks","text":"<p>Custom hooks manage API communication and state:</p> <pre><code>// hooks/usePresenterData.ts\nimport { useState, useEffect } from 'react';\nimport { fetchPresenter } from '@/lib/data/presenters';\n\ninterface UsePresenterDataOptions {\n    matcher?: string;\n    output?: string;\n    filter_field?: string;\n    filter_value?: string;\n    enabled?: boolean;\n}\n\nexport function usePresenterData&lt;T extends Presenter&gt;(\n    presenterId: string, \n    options: UsePresenterDataOptions = {}\n) {\n    const [data, setData] = useState&lt;T | null&gt;(null);\n    const [isLoading, setIsLoading] = useState(false);\n    const [error, setError] = useState&lt;string | null&gt;(null);\n\n    useEffect(() =&gt; {\n        if (!options.enabled &amp;&amp; options.enabled !== undefined) return;\n\n        const controller = new AbortController();\n\n        const loadData = async () =&gt; {\n            setIsLoading(true);\n            setError(null);\n\n            try {\n                const result = await fetchPresenter(\n                    presenterId, \n                    controller.signal, \n                    options\n                );\n\n                if (result) {\n                    setData(result);\n                } else {\n                    setError('Failed to load data');\n                }\n            } catch (err) {\n                if (!controller.signal.aborted) {\n                    setError(err instanceof Error ? err.message : 'Unknown error');\n                }\n            } finally {\n                setIsLoading(false);\n            }\n        };\n\n        loadData();\n\n        return () =&gt; controller.abort();\n    }, [presenterId, options.matcher, options.output, options.filter_value, options.enabled]);\n\n    return { data, isLoading, error };\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#chart-components","title":"Chart Components","text":"<p>Interactive visualizations using Deck.gl:</p> <pre><code>// components/charts/scatter.tsx\nimport { useMemo, useRef, useState } from 'react';\nimport { ScatterplotLayer } from '@deck.gl/layers';\nimport { COORDINATE_SYSTEM } from '@deck.gl/core';\nimport DeckGL from '@deck.gl/react';\nimport { AxisLeft, AxisBottom } from '@visx/axis';\nimport useChart from '@/lib/hooks/chart';\n\ninterface ScatterPlotProps {\n    data: Array&lt;DataPoint&gt;;\n    onItemClick?: (item: PickingInfo&lt;DataPoint&gt;) =&gt; void;\n    tooltip: TooltipFunction&lt;DataPoint&gt;;\n    darkMode?: boolean;\n    dimensions?: Dimensions;\n}\n\nexport default function ScatterPlot({\n    data,\n    onItemClick,\n    tooltip,\n    darkMode = false,\n    dimensions = { width: 800, height: 600, margins: { top: 20, right: 40, bottom: 40, left: 60 }}\n}: ScatterPlotProps) {\n    const deckRef = useRef&lt;DeckGLRef | null&gt;(null);\n    const [deckInstance, setDeckInstance] = useState&lt;Deck | null&gt;(null);\n\n    // Custom hook handles coordinate transformation and scaling\n    const { data: plotData, deckProps, axis, viewport } = useChart(\n        data,\n        tooltip,\n        deckInstance,\n        true, // resetZoomOnChange\n        { x: { type: 'log', show: true }, y: { type: 'log', show: true } },\n        dimensions\n    );\n\n    // Create Deck.gl layers\n    const layers = useMemo(() =&gt; [\n        new ScatterplotLayer({\n            id: 'scatter-points',\n            data: plotData,\n            pickable: true,\n            opacity: 0.8,\n            stroked: false,\n            filled: true,\n            radiusScale: 6,\n            radiusMinPixels: 2,\n            radiusMaxPixels: 8,\n            coordinateSystem: COORDINATE_SYSTEM.CARTESIAN,\n            getPosition: (d: any) =&gt; d.position,\n            getFillColor: (d: any) =&gt; d.color,\n            updateTriggers: {\n                getFillColor: [darkMode, viewport.viewState.zoom]\n            },\n            transitions: {\n                getPosition: { duration: 300, type: 'spring' },\n                getFillColor: { duration: 200 }\n            }\n        })\n    ], [plotData, darkMode, viewport.viewState.zoom]);\n\n    return (\n        &lt;div className=\"relative\"&gt;\n            {/* Zoom controls */}\n            &lt;div className=\"absolute top-4 right-4 z-10\"&gt;\n                &lt;ToolBox \n                    features={['zoom', 'restore']}\n                    zoomIncrement={viewport.hooks.increment}\n                    zoomDecrement={viewport.hooks.decrement}\n                    zoomReset={viewport.hooks.reset}\n                /&gt;\n            &lt;/div&gt;\n\n            {/* Main chart area */}\n            &lt;div style={{ position: 'relative', width: dimensions.width, height: dimensions.height }}&gt;\n                &lt;DeckGL\n                    ref={deckRef}\n                    {...deckProps}\n                    layers={layers}\n                    onClick={onItemClick}\n                    onAfterRender={() =&gt; {\n                        if (deckRef.current?.deck &amp;&amp; !deckInstance) {\n                            setDeckInstance(deckRef.current.deck);\n                        }\n                    }}\n                /&gt;\n\n                {/* Axes overlay */}\n                &lt;svg width={dimensions.width} height={dimensions.height}&gt;\n                    &lt;AxisBottom\n                        scale={axis.x.scale}\n                        top={dimensions.height - dimensions.margins.bottom}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'middle'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                    &lt;AxisLeft\n                        scale={axis.y.scale}\n                        left={dimensions.margins.left}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'end'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                &lt;/svg&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#interactive-data-tables","title":"Interactive Data Tables","text":"<p>Rich data tables with selection and filtering:</p> <pre><code>// components/data_table.tsx\nimport { useMemo, useCallback, useState, useEffect } from 'react';\nimport { DataEditor, GridCellKind, CompactSelection } from '@glideapps/glide-data-grid';\n\ninterface DataTableProps&lt;T extends BaseRow&gt; {\n    data: Array&lt;T&gt;;\n    columns: Array&lt;GridColumn&gt;;\n    onRowSelect?: (item: T | null, selection?: GridSelection) =&gt; void;\n    selectedRows?: CompactSelection;\n    darkMode?: boolean;\n    theme?: Partial&lt;Theme&gt;;\n}\n\nexport default function DataTable&lt;T extends BaseRow&gt;({\n    data,\n    columns,\n    onRowSelect,\n    selectedRows,\n    darkMode = false,\n    theme\n}: DataTableProps&lt;T&gt;) {\n    const [gridSelection, setGridSelection] = useState&lt;GridSelection&gt;({\n        columns: CompactSelection.empty(),\n        rows: selectedRows ?? CompactSelection.empty()\n    });\n\n    // Column mapping for data access\n    const columnIds = useMemo(() =&gt; \n        columns.map(col =&gt; col.id).filter(Boolean) as string[]\n    , [columns]);\n\n    // Cell content renderer\n    const getCellContent = useCallback(([col, row]: Item): GridCell =&gt; {\n        const item = data[row];\n        const columnId = columnIds[col];\n        const value = item[columnId];\n\n        // Determine cell type based on value\n        let cellType = GridCellKind.Text;\n        if (typeof value === 'number') cellType = GridCellKind.Number;\n        if (typeof value === 'boolean') cellType = GridCellKind.Boolean;\n\n        return {\n            kind: cellType,\n            allowOverlay: false,\n            displayData: String(value ?? ''),\n            data: value\n        };\n    }, [data, columnIds]);\n\n    // Selection handler\n    const handleSelectionChange = useCallback((selection: GridSelection) =&gt; {\n        setGridSelection(selection);\n\n        const selectedRow = selection.rows.first();\n        const item = selectedRow !== undefined ? data[selectedRow] : null;\n\n        onRowSelect?.(item, selection);\n    }, [data, onRowSelect]);\n\n    // Theme configuration\n    const tableTheme = useMemo(() =&gt; {\n        if (theme) return theme;\n\n        return darkMode ? {\n            accentColor: '#8c96ff',\n            textDark: '#ffffff',\n            textMedium: '#b8b8b8',\n            bgCell: '#16161b',\n            bgHeader: '#212121',\n            borderColor: 'rgba(225,225,225,0.2)',\n            fontFamily: 'Inter, sans-serif'\n        } : {};\n    }, [darkMode, theme]);\n\n    // Sync external selection changes\n    useEffect(() =&gt; {\n        if (selectedRows &amp;&amp; !selectedRows.equals(gridSelection.rows)) {\n            setGridSelection(prev =&gt; ({ ...prev, rows: selectedRows }));\n        }\n    }, [selectedRows]);\n\n    return (\n        &lt;DataEditor\n            width=\"100%\"\n            height=\"50rem\"\n            className=\"rounded-md border shadow-md\"\n            columns={columns}\n            rows={data.length}\n            getCellContent={getCellContent}\n            gridSelection={gridSelection}\n            onGridSelectionChange={handleSelectionChange}\n            theme={tableTheme}\n            rowSelect=\"single\"\n            rowMarkers=\"checkbox-visible\"\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#search-and-filtering-components","title":"Search and Filtering Components","text":"<p>Reusable search components with autocomplete:</p> <pre><code>// components/search.tsx\nimport { useState, useEffect, useMemo } from 'react';\nimport { useVirtualizer } from '@tanstack/react-virtual';\nimport { Input } from '@/components/ui/input';\nimport { Button } from '@/components/ui/button';\nimport { X } from 'lucide-react';\n\ninterface SearchBarProps {\n    searchList: Array&lt;string&gt;;\n    onSubmit: (value: string) =&gt; void;\n    onClear?: () =&gt; void;\n    placeholder?: string;\n    maxSuggestions?: number;\n}\n\nexport default function SearchBar({ \n    searchList, \n    onSubmit, \n    onClear, \n    placeholder = \"Search...\",\n    maxSuggestions = 100\n}: SearchBarProps) {\n    const [value, setValue] = useState('');\n    const [showSuggestions, setShowSuggestions] = useState(false);\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    // Filter suggestions based on input\n    const suggestions = useMemo(() =&gt; {\n        if (!value) return [];\n\n        return searchList\n            .filter(item =&gt; item.toLowerCase().includes(value.toLowerCase()))\n            .slice(0, maxSuggestions);\n    }, [value, searchList, maxSuggestions]);\n\n    // Virtual scrolling for large suggestion lists\n    const virtualizer = useVirtualizer({\n        count: suggestions.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 35\n    });\n\n    const handleSubmit = (e: FormEvent) =&gt; {\n        e.preventDefault();\n        onSubmit(value);\n        setShowSuggestions(false);\n    };\n\n    const handleSuggestionClick = (suggestion: string) =&gt; {\n        setValue(suggestion);\n        onSubmit(suggestion);\n        setShowSuggestions(false);\n    };\n\n    const handleClear = () =&gt; {\n        setValue('');\n        onClear?.();\n    };\n\n    // Show/hide suggestions based on focus and value\n    useEffect(() =&gt; {\n        setShowSuggestions(suggestions.length &gt; 0 &amp;&amp; value.length &gt; 0);\n    }, [suggestions.length, value.length]);\n\n    return (\n        &lt;form onSubmit={handleSubmit} className=\"relative\"&gt;\n            &lt;div className=\"flex items-center space-x-2\"&gt;\n                &lt;Input\n                    type=\"text\"\n                    value={value}\n                    onChange={(e) =&gt; setValue(e.target.value)}\n                    placeholder={placeholder}\n                    className=\"w-80\"\n                    onFocus={() =&gt; suggestions.length &gt; 0 &amp;&amp; setShowSuggestions(true)}\n                    onBlur={() =&gt; setTimeout(() =&gt; setShowSuggestions(false), 150)}\n                /&gt;\n                {value &amp;&amp; (\n                    &lt;Button\n                        type=\"button\"\n                        variant=\"ghost\"\n                        size=\"icon\"\n                        onClick={handleClear}\n                    &gt;\n                        &lt;X className=\"h-4 w-4\" /&gt;\n                    &lt;/Button&gt;\n                )}\n            &lt;/div&gt;\n\n            {/* Suggestions dropdown */}\n            {showSuggestions &amp;&amp; (\n                &lt;div\n                    ref={containerRef}\n                    className=\"absolute z-50 w-80 max-h-96 mt-1 bg-white border rounded-md shadow-lg overflow-auto dark:bg-zinc-950 dark:border-zinc-800\"\n                &gt;\n                    &lt;div\n                        style={{ height: virtualizer.getTotalSize() }}\n                        className=\"relative\"\n                    &gt;\n                        {virtualizer.getVirtualItems().map((item) =&gt; (\n                            &lt;div\n                                key={item.key}\n                                className=\"absolute w-full px-3 py-2 cursor-pointer hover:bg-zinc-100 dark:hover:bg-zinc-800\"\n                                style={{\n                                    height: item.size,\n                                    transform: `translateY(${item.start}px)`\n                                }}\n                                onMouseDown={() =&gt; handleSuggestionClick(suggestions[item.index])}\n                            &gt;\n                                {suggestions[item.index]}\n                            &lt;/div&gt;\n                        ))}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            )}\n        &lt;/form&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#download-components","title":"Download Components","text":"<p>Export functionality for data:</p> <pre><code>// components/download.tsx\nimport { useState } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { DropdownMenu, DropdownMenuContent, DropdownMenuItem, DropdownMenuTrigger } from '@/components/ui/dropdown-menu';\nimport { Sheet, Braces, Table, ChevronDown } from 'lucide-react';\nimport { createDownloadLink } from '@/lib/data/download';\n\ninterface DownloadButtonProps {\n    presenterID: string;\n    queryParams?: PresenterQueryParams;\n    label?: string;\n}\n\nexport function DownloadButton({ \n    presenterID, \n    queryParams, \n    label = \"Export\" \n}: DownloadButtonProps) {\n    const [isOpen, setIsOpen] = useState(false);\n\n    const downloadFormats = [\n        { type: 'excel', icon: Sheet, label: 'Excel', extension: '.xlsx' },\n        { type: 'csv', icon: Table, label: 'CSV', extension: '.csv' },\n        { type: 'json', icon: Braces, label: 'JSON', extension: '.json' }\n    ] as const;\n\n    return (\n        &lt;DropdownMenu open={isOpen} onOpenChange={setIsOpen}&gt;\n            &lt;DropdownMenuTrigger asChild&gt;\n                &lt;Button variant=\"outline\" size=\"sm\"&gt;\n                    {label}\n                    &lt;ChevronDown className={`ml-2 h-4 w-4 transition-transform ${isOpen ? 'rotate-180' : ''}`} /&gt;\n                &lt;/Button&gt;\n            &lt;/DropdownMenuTrigger&gt;\n\n            &lt;DropdownMenuContent align=\"end\"&gt;\n                {downloadFormats.map(({ type, icon: Icon, label: formatLabel }) =&gt; (\n                    &lt;DropdownMenuItem key={type} asChild&gt;\n                        &lt;a\n                            href={createDownloadLink(presenterID, type, queryParams)}\n                            target=\"_blank\"\n                            rel=\"noopener noreferrer\"\n                            className=\"flex items-center w-full\"\n                        &gt;\n                            &lt;Icon className=\"mr-2 h-4 w-4\" /&gt;\n                            {formatLabel}\n                        &lt;/a&gt;\n                    &lt;/DropdownMenuItem&gt;\n                ))}\n            &lt;/DropdownMenuContent&gt;\n        &lt;/DropdownMenu&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#state-management","title":"State Management","text":""},{"location":"guides/contributing/dashboards/react/#local-component-state","title":"Local Component State","text":"<p>For simple interactions, use React's built-in state:</p> <pre><code>function NgramDashboard({ presenter }: DashboardProps) {\n    // UI state\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedTab, setSelectedTab] = useState&lt;TabType&gt;('overview');\n    const [selectedItems, setSelectedItems] = useState&lt;string[]&gt;([]);\n\n    // Data state with custom hook\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchTerm,\n        output: selectedTab\n    });\n\n    // Derived state\n    const filteredData = useMemo(() =&gt; {\n        if (!data || !searchTerm) return data;\n        return data.filter(item =&gt; \n            item.text.toLowerCase().includes(searchTerm.toLowerCase())\n        );\n    }, [data, searchTerm]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedItems([]); // Clear selection on new search\n    }, []);\n\n    return (\n        // Component JSX\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#global-state-with-zustand","title":"Global State with Zustand","text":"<p>For complex dashboards with shared state:</p> <pre><code>// stores/dashboardStore.ts\nimport { create } from 'zustand';\n\ninterface DashboardState {\n    // Data\n    presenters: PresenterCollection;\n    currentPresenter: Presenter | null;\n\n    // UI state\n    sidebarOpen: boolean;\n    theme: 'light' | 'dark' | 'system';\n\n    // Filters\n    globalFilters: {\n        dateRange?: [Date, Date];\n        categories: string[];\n        searchTerm: string;\n    };\n\n    // Actions\n    setCurrentPresenter: (presenter: Presenter) =&gt; void;\n    updateFilters: (filters: Partial&lt;DashboardState['globalFilters']&gt;) =&gt; void;\n    toggleSidebar: () =&gt; void;\n}\n\nexport const useDashboardStore = create&lt;DashboardState&gt;((set, get) =&gt; ({\n    presenters: [],\n    currentPresenter: null,\n    sidebarOpen: true,\n    theme: 'system',\n    globalFilters: {\n        categories: [],\n        searchTerm: ''\n    },\n\n    setCurrentPresenter: (presenter) =&gt; set({ currentPresenter: presenter }),\n\n    updateFilters: (newFilters) =&gt; set(state =&gt; ({\n        globalFilters: { ...state.globalFilters, ...newFilters }\n    })),\n\n    toggleSidebar: () =&gt; set(state =&gt; ({ sidebarOpen: !state.sidebarOpen }))\n}));\n\n// Usage in components\nfunction Dashboard() {\n    const { currentPresenter, globalFilters, updateFilters } = useDashboardStore();\n\n    const handleSearch = (searchTerm: string) =&gt; {\n        updateFilters({ searchTerm });\n    };\n\n    return (\n        // Dashboard JSX\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#styling-and-theming","title":"Styling and Theming","text":""},{"location":"guides/contributing/dashboards/react/#tailwind-css-classes","title":"Tailwind CSS Classes","text":"<p>The project uses Tailwind CSS for utility-first styling:</p> <pre><code>// Layout utilities\n&lt;div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\"&gt;\n    &lt;div className=\"col-span-full lg:col-span-2\"&gt;\n        {/* Main content */}\n    &lt;/div&gt;\n    &lt;div className=\"lg:col-span-1\"&gt;\n        {/* Sidebar */}\n    &lt;/div&gt;\n&lt;/div&gt;\n\n// Component styling\n&lt;Card className=\"p-6 shadow-lg border-zinc-200 dark:border-zinc-800\"&gt;\n    &lt;CardHeader className=\"pb-4\"&gt;\n        &lt;CardTitle className=\"text-lg font-semibold text-zinc-900 dark:text-zinc-100\"&gt;\n            Chart Title\n        &lt;/CardTitle&gt;\n    &lt;/CardHeader&gt;\n    &lt;CardContent&gt;\n        {/* Chart content */}\n    &lt;/CardContent&gt;\n&lt;/Card&gt;\n\n// Interactive states\n&lt;Button \n    variant=\"outline\" \n    className=\"hover:bg-zinc-100 dark:hover:bg-zinc-800 transition-colors\"\n    disabled={isLoading}\n&gt;\n    {isLoading ? &lt;Spinner className=\"mr-2\" /&gt; : null}\n    Load Data\n&lt;/Button&gt;\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#dark-mode-support","title":"Dark Mode Support","text":"<p>Dark mode is handled through CSS custom properties and Tailwind's dark variant:</p> <pre><code>// Theme provider context\nexport function ThemeProvider({ children, defaultTheme = \"system\" }) {\n    const [theme, setTheme] = useState&lt;Theme&gt;(defaultTheme);\n\n    useEffect(() =&gt; {\n        const root = window.document.documentElement;\n        root.classList.remove(\"light\", \"dark\");\n\n        if (theme === \"system\") {\n            const systemTheme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches \n                ? \"dark\" : \"light\";\n            root.classList.add(systemTheme);\n        } else {\n            root.classList.add(theme);\n        }\n    }, [theme]);\n\n    return (\n        &lt;ThemeContext.Provider value={{ theme, setTheme }}&gt;\n            {children}\n        &lt;/ThemeContext.Provider&gt;\n    );\n}\n\n// Usage in components\nfunction Chart({ data }: ChartProps) {\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    return (\n        &lt;ScatterPlot \n            data={data}\n            darkMode={isDark}\n            // Colors adapt automatically through Tailwind dark: variants\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#responsive-design","title":"Responsive Design","text":"<p>Components adapt to different screen sizes:</p> <pre><code>&lt;div className=\"space-y-6\"&gt;\n    {/* Mobile-first responsive grid */}\n    &lt;div className=\"grid grid-cols-1 lg:grid-cols-4 gap-4\"&gt;\n        &lt;div className=\"lg:col-span-3\"&gt;\n            {/* Chart takes full width on mobile, 3/4 on desktop */}\n            &lt;ScatterPlot data={data} /&gt;\n        &lt;/div&gt;\n        &lt;div className=\"lg:col-span-1\"&gt;\n            {/* Controls stack below chart on mobile, sidebar on desktop */}\n            &lt;div className=\"space-y-4\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} /&gt;\n                &lt;FilterControls /&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    {/* Table with horizontal scroll on mobile */}\n    &lt;div className=\"overflow-x-auto\"&gt;\n        &lt;DataTable \n            data={data}\n            className=\"min-w-[600px]\" \n        /&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/contributing/dashboards/react/#memoization-and-optimization","title":"Memoization and Optimization","text":"<pre><code>// Memoize expensive calculations\nconst processedData = useMemo(() =&gt; {\n    if (!rawData) return [];\n\n    return rawData\n        .filter(item =&gt; item.value &gt; threshold)\n        .sort((a, b) =&gt; b.value - a.value)\n        .slice(0, maxItems);\n}, [rawData, threshold, maxItems]);\n\n// Memoize callback functions\nconst handleItemClick = useCallback((item: DataPoint) =&gt; {\n    setSelectedItem(item);\n    onItemSelect?.(item);\n}, [onItemSelect]);\n\n// Memoize complex components\nconst ChartComponent = memo(({ data, options }: ChartProps) =&gt; {\n    return &lt;ExpensiveChart data={data} options={options} /&gt;;\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#virtual-scrolling","title":"Virtual Scrolling","text":"<p>For large datasets, implement virtual scrolling:</p> <pre><code>import { useVirtualizer } from '@tanstack/react-virtual';\n\nfunction VirtualTable({ data }: { data: Array&lt;any&gt; }) {\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    const virtualizer = useVirtualizer({\n        count: data.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 50, // Row height\n        overscan: 10 // Render extra items for smooth scrolling\n    });\n\n    return (\n        &lt;div ref={containerRef} className=\"h-96 overflow-auto\"&gt;\n            &lt;div style={{ height: virtualizer.getTotalSize() }}&gt;\n                {virtualizer.getVirtualItems().map((item) =&gt; (\n                    &lt;div\n                        key={item.key}\n                        style={{\n                            position: 'absolute',\n                            top: 0,\n                            left: 0,\n                            width: '100%',\n                            height: item.size,\n                            transform: `translateY(${item.start}px)`\n                        }}\n                    &gt;\n                        &lt;TableRow data={data[item.index]} /&gt;\n                    &lt;/div&gt;\n                ))}\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#code-splitting","title":"Code Splitting","text":"<p>Split large components with lazy loading:</p> <pre><code>// Lazy load heavy visualization components\nconst AdvancedChart = lazy(() =&gt; import('@/components/charts/advanced-chart'));\nconst ComplexTable = lazy(() =&gt; import('@/components/tables/complex-table'));\n\nfunction Dashboard() {\n    return (\n        &lt;Suspense fallback={&lt;div&gt;Loading chart...&lt;/div&gt;}&gt;\n            &lt;AdvancedChart data={data} /&gt;\n        &lt;/Suspense&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#testing-react-dashboards","title":"Testing React Dashboards","text":""},{"location":"guides/contributing/dashboards/react/#component-testing","title":"Component Testing","text":"<pre><code>// __tests__/components/SearchBar.test.tsx\nimport { render, screen, fireEvent, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport SearchBar from '@/components/search';\n\ndescribe('SearchBar', () =&gt; {\n    const mockSubmit = jest.fn();\n    const mockClear = jest.fn();\n    const searchList = ['apple', 'banana', 'cherry', 'date'];\n\n    beforeEach(() =&gt; {\n        jest.clearAllMocks();\n    });\n\n    test('renders with placeholder', () =&gt; {\n        render(\n            &lt;SearchBar \n                searchList={searchList}\n                onSubmit={mockSubmit}\n                placeholder=\"Search fruits...\"\n            /&gt;\n        );\n\n        expect(screen.getByPlaceholderText('Search fruits...')).toBeInTheDocument();\n    });\n\n    test('shows suggestions when typing', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'a');\n\n        await waitFor(() =&gt; {\n            expect(screen.getByText('apple')).toBeInTheDocument();\n            expect(screen.getByText('banana')).toBeInTheDocument();\n        });\n    });\n\n    test('calls onSubmit when form submitted', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'apple');\n        await user.keyboard('{Enter}');\n\n        expect(mockSubmit).toHaveBeenCalledWith('apple');\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#integration-testing","title":"Integration Testing","text":"<pre><code>// __tests__/integration/Dashboard.test.tsx\nimport { render, screen, waitFor } from '@testing-library/react';\nimport { rest } from 'msw';\nimport { setupServer } from 'msw/node';\nimport Dashboard from '@/components/dashboard';\n\n// Mock API server\nconst server = setupServer(\n    rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n        return res(ctx.json({\n            id: 'test-presenter',\n            name: 'Test Presenter',\n            data: [\n                { x: 1, y: 2, label: 'Point 1' },\n                { x: 3, y: 4, label: 'Point 2' }\n            ]\n        }));\n    })\n);\n\nbeforeAll(() =&gt; server.listen());\nafterEach(() =&gt; server.resetHandlers());\nafterAll(() =&gt; server.close());\n\ntest('loads and displays data', async () =&gt; {\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    // Initially shows loading\n    expect(screen.getByText(/loading/i)).toBeInTheDocument();\n\n    // After API call, shows data\n    await waitFor(() =&gt; {\n        expect(screen.getByText('Test Presenter')).toBeInTheDocument();\n    });\n\n    // Chart renders with data points\n    expect(screen.getByText('Point 1')).toBeInTheDocument();\n    expect(screen.getByText('Point 2')).toBeInTheDocument();\n});\n\ntest('handles API errors gracefully', async () =&gt; {\n    server.use(\n        rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n            return res(ctx.status(500), ctx.json({ error: 'Server error' }));\n        })\n    );\n\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    await waitFor(() =&gt; {\n        expect(screen.getByText(/error loading data/i)).toBeInTheDocument();\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#e2e-testing-with-cypress","title":"E2E Testing with Cypress","text":"<pre><code>// cypress/integration/dashboard.spec.ts\ndescribe('Dashboard Interaction', () =&gt; {\n    beforeEach(() =&gt; {\n        cy.intercept('GET', '/api/presenters/ngram-analysis', { \n            fixture: 'ngram-data.json' \n        }).as('getNgramData');\n\n        cy.visit('/dashboard/ngram-analysis');\n        cy.wait('@getNgramData');\n    });\n\n    it('allows searching and filtering data', () =&gt; {\n        // Search for specific terms\n        cy.get('[data-testid=\"search-input\"]').type('climate');\n        cy.get('[data-testid=\"search-submit\"]').click();\n\n        // Verify results update\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.lessThan', 100);\n        cy.get('[data-testid=\"data-table\"]').should('contain', 'climate');\n\n        // Clear search\n        cy.get('[data-testid=\"search-clear\"]').click();\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.greaterThan', 100);\n    });\n\n    it('supports chart interactions', () =&gt; {\n        // Click on chart point\n        cy.get('[data-testid=\"chart-container\"]').click(300, 200);\n\n        // Verify tooltip appears\n        cy.get('[data-testid=\"tooltip\"]').should('be.visible');\n        cy.get('[data-testid=\"tooltip\"]').should('contain', 'Ranking:');\n\n        // Verify data table selection updates\n        cy.get('[data-testid=\"data-table\"] .selected-row').should('exist');\n    });\n\n    it('downloads data in different formats', () =&gt; {\n        // Open download menu\n        cy.get('[data-testid=\"download-button\"]').click();\n\n        // Download CSV\n        cy.get('[data-testid=\"download-csv\"]').click();\n        cy.readFile('cypress/downloads/data.csv').should('exist');\n\n        // Download JSON\n        cy.get('[data-testid=\"download-button\"]').click();\n        cy.get('[data-testid=\"download-json\"]').click();\n        cy.readFile('cypress/downloads/data.json').should('exist');\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#deployment-and-build-process","title":"Deployment and Build Process","text":""},{"location":"guides/contributing/dashboards/react/#production-build","title":"Production Build","text":"<p>The React dashboard builds as static assets:</p> <pre><code># Build for production\nnpm run build\n\n# Outputs to app/web_templates/build/\n# - bundled/ (JS/CSS assets)\n# - manifest.json (asset mapping)\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#integration-with-backend","title":"Integration with Backend","text":"<p>The Python backend serves the React app:</p> <pre><code># Backend integration\nfrom pathlib import Path\nimport json\n\n# Load build manifest\nmanifest_path = Path(\"web_templates/build/manifest.json\")\nwith open(manifest_path) as f:\n    manifest = json.load(f)\n\n# Serve React app\n@app.route(\"/\")\ndef dashboard():\n    return render_template(\n        \"index.html\",\n        js_files=get_js_files(manifest),\n        css_files=get_css_files(manifest),\n        project_name=config.PROJECT_NAME\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#environment-configuration","title":"Environment Configuration","text":"<pre><code>// Environment variables for different deployments\nconst config = {\n    API_BASE_URL: process.env.REACT_APP_API_URL || 'http://localhost:8050',\n    ENABLE_DEV_TOOLS: process.env.NODE_ENV === 'development',\n    VERSION: process.env.REACT_APP_VERSION || '1.0.0'\n};\n\n// API client configuration\nconst apiClient = axios.create({\n    baseURL: config.API_BASE_URL,\n    timeout: 30000,\n    headers: {\n        'Content-Type': 'application/json'\n    }\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#best-practices","title":"Best Practices","text":""},{"location":"guides/contributing/dashboards/react/#component-design","title":"Component Design","text":"<pre><code>// 1. Keep components focused and single-purpose\nfunction ChartControls({ onFilterChange, onExport }: ChartControlsProps): ReactElement&lt;FC&gt; {\n    // Only handle UI controls, delegate data management\n    return (\n        &lt;div className=\"flex gap-4\"&gt;\n            &lt;SearchBar onSubmit={(term) =&gt; onFilterChange({ search: term })} /&gt;\n            &lt;ExportButton onExport={onExport} /&gt;\n        &lt;/div&gt;\n    );\n}\n\n// 2. Use composition over inheritance\nfunction Dashboard({ children }: PropsWithChildren): ReactElement&lt;FC&gt; {\n    return (\n        &lt;div className=\"dashboard-layout\"&gt;\n            &lt;Sidebar /&gt;\n            &lt;main className=\"main-content\"&gt;\n                {children}\n            &lt;/main&gt;\n        &lt;/div&gt;\n    );\n}\n\n// Usage\n&lt;Dashboard&gt;\n    &lt;ChartContainer&gt;\n        &lt;ScatterPlot data={data} /&gt;\n        &lt;DataTable data={data} /&gt;\n    &lt;/ChartContainer&gt;\n&lt;/Dashboard&gt;\n\n// 3. Extract custom hooks for reusable logic\nfunction useChartData(presenterId: string, filters: Filters) {\n    const [data, setData] = useState(null);\n    const [isLoading, setIsLoading] = useState(false);\n\n    useEffect(() =&gt; {\n        // Data fetching logic\n    }, [presenterId, filters]);\n\n    return { data, isLoading, refetch: () =&gt; setData(null) };\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#complete-example-ngram-analysis-dashboard","title":"Complete Example: Ngram Analysis Dashboard","text":"<p>Here's a complete example showing all concepts together:</p> <pre><code>// components/ngram-dashboard.tsx\nimport { useState, useEffect, useMemo, useCallback } from 'react';\nimport { useTheme } from '@/components/theme-provider';\nimport { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\nimport { Info } from 'lucide-react';\nimport { Tooltip, TooltipContent, TooltipTrigger } from '@/components/ui/tooltip';\n\nimport ScatterPlot from '@/components/charts/scatter';\nimport DataTable from '@/components/data-table';\nimport SearchBar from '@/components/search';\nimport { DownloadButton } from '@/components/download';\nimport { usePresenterData } from '@/hooks/usePresenterData';\n\ninterface NgramDashboardProps {\n    presenter: NgramPresenter;\n}\n\nexport default function NgramDashboard({ presenter }: NgramDashboardProps) {\n    // State management\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedNgram, setSelectedNgram] = useState('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n    const [selectedRows, setSelectedRows] = useState&lt;CompactSelection&gt;(CompactSelection.empty());\n\n    // Theme\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    // Data fetching\n    const { data: summaryData, isLoading } = usePresenterData(presenter.id, {\n        output: 'summary',\n        matcher: searchTerm\n    });\n\n    const { data: detailData } = usePresenterData(presenter.id, {\n        output: 'details',\n        filter_field: 'ngram',\n        filter_value: selectedNgram\n    }, { enabled: !!selectedNgram });\n\n    // Computed values\n    const currentData = useMemo(() =&gt; {\n        if (!summaryData) return [];\n\n        return summaryData.map((item, index) =&gt; ({\n            ...item,\n            ranking: index + 1,\n            y: currentTab === 'total_repetition' \n                ? item.total_repetition \n                : item.amplification_factor\n        }));\n    }, [summaryData, currentTab]);\n\n    const tableColumns = useMemo(() =&gt; {\n        if (selectedNgram &amp;&amp; detailData) {\n            return [\n                { id: 'ngram', title: 'N-gram', width: 200 },\n                { id: 'user', title: 'User', width: 150 },\n                { id: 'userReps', title: 'User Reps', width: 100 },\n                { id: 'message', title: 'Message', width: 400 },\n                { id: 'timestamp', title: 'Timestamp', width: 200 }\n            ];\n        }\n\n        return [\n            { id: 'ranking', title: 'Rank', width: 80 },\n            { id: 'ngram', title: 'N-gram', width: 300 },\n            { id: 'x', title: 'User Count', width: 120 },\n            { \n                id: 'y', \n                title: currentTab === 'total_repetition' ? 'Total Reps' : 'Amplification',\n                width: 120 \n            }\n        ];\n    }, [selectedNgram, detailData, currentTab]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedNgram(''); // Clear selection when searching\n    }, []);\n\n    const handleSearchClear = useCallback(() =&gt; {\n        setSearchTerm('');\n        setSelectedNgram('');\n    }, []);\n\n    const handleChartClick = useCallback((info: PickingInfo&lt;DataPoint&gt;) =&gt; {\n        if (info.object) {\n            setSelectedNgram(info.object.ngram);\n        }\n    }, []);\n\n    const handleTableSelect = useCallback((item: DataPoint | null, selection?: GridSelection) =&gt; {\n        if (item) {\n            setSelectedNgram(item.ngram);\n        }\n        if (selection) {\n            setSelectedRows(selection.rows);\n        }\n    }, []);\n\n    const handleTabChange = useCallback((tab: string) =&gt; {\n        setCurrentTab(tab as typeof currentTab);\n    }, []);\n\n    // Tooltip formatters\n    const createTooltipFormatter = useCallback((type: string) =&gt; (params: DataPoint) =&gt; `\n        &lt;div class=\"space-y-2\"&gt;\n            &lt;div class=\"font-bold\"&gt;${params.ngram}&lt;/div&gt;\n            &lt;div&gt;Ranking: ${params.ranking}&lt;/div&gt;\n            &lt;div&gt;User Count: ${params.x}&lt;/div&gt;\n            &lt;div&gt;${type === 'total_repetition' ? 'Total Reps' : 'Amplification'}: ${params.y}&lt;/div&gt;\n        &lt;/div&gt;\n    `, []);\n\n    // Loading state\n    if (isLoading) {\n        return (\n            &lt;Card&gt;\n                &lt;CardContent className=\"flex items-center justify-center h-96\"&gt;\n                    &lt;div className=\"text-center\"&gt;\n                        &lt;div className=\"animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4\"&gt;&lt;/div&gt;\n                        &lt;p&gt;Loading dashboard...&lt;/p&gt;\n                    &lt;/div&gt;\n                &lt;/CardContent&gt;\n            &lt;/Card&gt;\n        );\n    }\n\n    return (\n        &lt;Card&gt;\n            &lt;CardContent className=\"space-y-6\"&gt;\n                &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                    &lt;div className=\"flex items-center justify-between\"&gt;\n                        &lt;TabsList&gt;\n                            &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                            &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification Factor&lt;/TabsTrigger&gt;\n                        &lt;/TabsList&gt;\n\n                        &lt;Tooltip&gt;\n                            &lt;TooltipTrigger&gt;\n                                &lt;Info className=\"h-5 w-5 text-zinc-500\" /&gt;\n                            &lt;/TooltipTrigger&gt;\n                            &lt;TooltipContent&gt;\n                                &lt;p className=\"max-w-xs\"&gt;\n                                    {presenter.explanation[currentTab]}\n                                &lt;/p&gt;\n                            &lt;/TooltipContent&gt;\n                        &lt;/Tooltip&gt;\n                    &lt;/div&gt;\n\n                    &lt;TabsContent value=\"total_repetition\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('total_repetition')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n\n                    &lt;TabsContent value=\"amplification_factor\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('amplification_factor')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n                &lt;/Tabs&gt;\n            &lt;/CardContent&gt;\n        &lt;/Card&gt;\n    );\n}\n</code></pre> <p>This comprehensive guide covers all aspects of building React dashboards for the analyzer platform. The combination of TypeScript, modern React patterns, rich UI components, and seamless API integration creates powerful, user-friendly data analysis interfaces that complement the Python-based analyzer pipeline.</p>"},{"location":"guides/contributing/dashboards/react/#next-steps","title":"Next Steps","text":"<p>After this section it would be a good idea to review the sections that discuss implementing  Shiny dashboards. Although once you finish reading this it would also be a good idea to review the sections for each domain.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/","title":"Shiny","text":""},{"location":"guides/contributing/dashboards/shiny/#shiny-dashboards-guide","title":"Shiny Dashboards Guide","text":"<p>Web presenters can create interactive dashboards using Python Shiny for rich server-side interactivity. Shiny dashboards provide immediate reactivity, complex data processing capabilities, and seamless integration with the analyzer pipeline. Which in turn allows developers and data scientists the ability to quickly prototype new analyses.</p>"},{"location":"guides/contributing/dashboards/shiny/#overview","title":"Overview","text":"<p>Shiny dashboards are server-rendered applications that provide:</p> <ul> <li>Real-time interactivity: Components update automatically when inputs change</li> <li>Server-side processing: Complex calculations run on the server with full Python ecosystem access</li> <li>Widgets: Built-in components for inputs, outputs, and visualizations</li> <li>Session management: Automatic handling of user sessions and state</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/#basic-structure","title":"Basic Structure","text":"<p>Every Shiny web presenter follows this pattern:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\nimport polars as pl\nimport plotly.express as px\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"your_output\").parquet_path)\n\n    # Define UI layout\n    dashboard_ui = ui.card(\n        ui.card_header(\"Your Dashboard Title\"),\n        ui.row(\n            ui.column(4, \n                # Input controls\n                ui.input_selectize(\"category\", \"Select Category\", \n                                 choices=df[\"category\"].unique().to_list()),\n                ui.input_slider(\"threshold\", \"Threshold\", 0, 100, 50)\n            ),\n            ui.column(8,\n                # Output displays\n                output_widget(\"main_plot\", height=\"400px\"),\n                ui.output_text(\"summary_stats\")\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            # Reactive data filtering\n            return df.filter(\n                (pl.col(\"category\") == input.category()) &amp;\n                (pl.col(\"value\") &gt;= input.threshold())\n            )\n\n        @render_widget\n        def main_plot():\n            # Create interactive plot\n            plot_df = filtered_data().to_pandas()\n            fig = px.scatter(plot_df, x=\"x\", y=\"y\", color=\"category\")\n            return fig\n\n        @render.text\n        def summary_stats():\n            data = filtered_data()\n            return f\"Showing {len(data)} items, avg value: {data['value'].mean():.2f}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", dashboard_ui)\n        )\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#user-interface-components","title":"User Interface Components","text":""},{"location":"guides/contributing/dashboards/shiny/#layout-components","title":"Layout Components","text":"<p>Organize your dashboard with these layout elements:</p> <pre><code># Cards for grouped content\nui.card(\n    ui.card_header(\"Section Title\"),\n    ui.card_body(\"Content goes here\")\n)\n\n# Grid layouts\nui.row(\n    ui.column(6, \"Left column\"),\n    ui.column(6, \"Right column\")\n)\n\n# Navigation\nui.navset_tab(\n    ui.nav_panel(\"Tab 1\", \"Content 1\"),\n    ui.nav_panel(\"Tab 2\", \"Content 2\")\n)\n\n# Sidebars\nui.sidebar(\n    \"Sidebar content\",\n    open=\"open\"  # or \"closed\"\n)\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#input-controls","title":"Input Controls","text":"<p>Collect user input with various widgets:</p> <pre><code># Text inputs\nui.input_text(\"text_id\", \"Label\", value=\"default\")\nui.input_text_area(\"textarea_id\", \"Description\", rows=3)\n\n# Numeric inputs\nui.input_numeric(\"number_id\", \"Number\", value=10, min=0, max=100)\nui.input_slider(\"slider_id\", \"Range\", 0, 100, value=[20, 80])\n\n# Selection inputs\nui.input_select(\"select_id\", \"Choose one\", choices=[\"A\", \"B\", \"C\"])\nui.input_selectize(\"selectize_id\", \"Type to search\", \n                   choices=data[\"column\"].unique().to_list(),\n                   multiple=True)\n\n# Boolean inputs\nui.input_checkbox(\"check_id\", \"Enable feature\", value=True)\nui.input_switch(\"switch_id\", \"Toggle mode\")\n\n# File uploads\nui.input_file(\"file_id\", \"Upload CSV\", accept=\".csv\")\n\n# Date/time inputs\nui.input_date(\"date_id\", \"Select date\")\nui.input_date_range(\"daterange_id\", \"Date range\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#output-components","title":"Output Components","text":"<p>Display results with these output components:</p> <pre><code># Text outputs\nui.output_text(\"text_id\")        # Plain text\nui.output_text_verbatim(\"code_id\")  # Monospace text\nui.output_ui(\"dynamic_ui\")       # Dynamic UI elements\n\n# Tables\nui.output_table(\"table_id\")      # Basic table\nui.output_data_frame(\"df_id\")    # Interactive data frame\n\n# Plots\noutput_widget(\"plot_id\")         # For plotly/bokeh widgets\nui.output_plot(\"matplotlib_id\")  # For matplotlib plots\n\n# Downloads\nui.download_button(\"download_id\", \"Download Data\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#reactive-programming","title":"Reactive Programming","text":"<p>Shiny's reactive system automatically updates outputs when inputs change:</p>"},{"location":"guides/contributing/dashboards/shiny/#reactive-calculations","title":"Reactive Calculations","text":"<p>Use <code>@reactive.Calc</code> for expensive computations that multiple outputs depend on:</p> <pre><code>@reactive.Calc\ndef processed_data():\n    # This only runs when dependencies change\n    raw_data = load_data()\n    return raw_data.filter(pl.col(\"active\") == input.show_active())\n\n@render_widget\ndef plot1():\n    data = processed_data()  # Uses cached result\n    return create_plot(data)\n\n@render.text  \ndef summary():\n    data = processed_data()  # Uses same cached result\n    return f\"Records: {len(data)}\"\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#reactive-effects","title":"Reactive Effects","text":"<p>Use <code>@reactive.Effect</code> for side effects like updating other inputs:</p> <pre><code>@reactive.Effect\ndef update_choices():\n    # Update selectize choices when category changes\n    category = input.category()\n    new_choices = df.filter(pl.col(\"category\") == category)[\"subcategory\"].unique()\n    ui.update_selectize(\"subcategory\", choices=new_choices.to_list())\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#event-handling","title":"Event Handling","text":"<p>Respond to button clicks and other events:</p> <pre><code>@reactive.Effect\n@reactive.event(input.reset_button)\ndef reset_filters():\n    ui.update_slider(\"threshold\", value=50)\n    ui.update_select(\"category\", selected=\"All\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#data-visualization","title":"Data Visualization","text":""},{"location":"guides/contributing/dashboards/shiny/#plotly-integration","title":"Plotly Integration","text":"<p>Create interactive plots with plotly:</p> <pre><code>from shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n@render_widget\ndef scatter_plot():\n    df_plot = filtered_data().to_pandas()\n\n    fig = px.scatter(\n        df_plot, \n        x=\"x_value\", \n        y=\"y_value\",\n        color=\"category\",\n        size=\"size_value\",\n        hover_data=[\"additional_info\"],\n        title=\"Interactive Scatter Plot\"\n    )\n\n    # Customize layout\n    fig.update_layout(\n        height=500,\n        showlegend=True,\n        hovermode=\"closest\"\n    )\n\n    return fig\n\n@render_widget  \ndef time_series():\n    df_ts = time_series_data().to_pandas()\n\n    fig = go.Figure()\n\n    for category in df_ts[\"category\"].unique():\n        category_data = df_ts[df_ts[\"category\"] == category]\n        fig.add_trace(go.Scatter(\n            x=category_data[\"date\"],\n            y=category_data[\"value\"],\n            name=category,\n            mode=\"lines+markers\"\n        ))\n\n    fig.update_layout(\n        title=\"Time Series Analysis\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Value\"\n    )\n\n    return fig\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#custom-plots","title":"Custom Plots","text":"<p>Create custom visualizations with matplotlib or other libraries:</p> <pre><code>from shiny import render\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n@render.plot\ndef correlation_heatmap():\n    df_corr = correlation_data().to_pandas()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        df_corr.corr(),\n        annot=True,\n        cmap=\"coolwarm\",\n        center=0,\n        square=True\n    )\n    plt.title(\"Correlation Matrix\")\n    plt.tight_layout()\n    return plt.gcf()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#data-tables","title":"Data Tables","text":"<p>Display and interact with tabular data:</p>"},{"location":"guides/contributing/dashboards/shiny/#basic-tables","title":"Basic Tables","text":"<pre><code>@render.table\ndef simple_table():\n    return filtered_data().to_pandas()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#interactive-data-frames","title":"Interactive Data Frames","text":"<pre><code>from shiny.render import DataGrid, DataTable\n\n@render.data_frame\ndef interactive_grid():\n    df_display = filtered_data().to_pandas()\n\n    return DataGrid(\n        df_display,\n        selection_mode=\"rows\",  # or \"none\", \"row\", \"rows\", \"col\", \"cols\"\n        filters=True,\n        width=\"100%\",\n        height=\"400px\"\n    )\n\n# Access selected rows\n@reactive.Effect\ndef handle_selection():\n    selected = interactive_grid.data_view(selected=True)\n    if len(selected) &gt; 0:\n        # Process selected data\n        pass\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#custom-table-styling","title":"Custom Table Styling","text":"<pre><code>@render.table\ndef styled_table():\n    df = summary_stats().to_pandas()\n\n    # Format numeric columns\n    df[\"percentage\"] = df[\"percentage\"].map(\"{:.1%}\".format)\n    df[\"amount\"] = df[\"amount\"].map(\"${:,.0f}\".format)\n\n    return df\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/contributing/dashboards/shiny/#dynamic-ui","title":"Dynamic UI","text":"<p>Create UI elements that change based on user input:</p> <pre><code>@render.ui\ndef dynamic_controls():\n    analysis_type = input.analysis_type()\n\n    if analysis_type == \"correlation\":\n        return ui.div(\n            ui.input_selectize(\"x_var\", \"X Variable\", choices=numeric_columns),\n            ui.input_selectize(\"y_var\", \"Y Variable\", choices=numeric_columns)\n        )\n\n    if analysis_type == \"distribution\":\n        return ui.div(\n            ui.input_select(\"dist_var\", \"Variable\", choices=all_columns),\n            ui.input_numeric(\"bins\", \"Number of bins\", value=30)\n        )\n\n    eturn ui.div(\"Select an analysis type\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#progress-indicators","title":"Progress Indicators","text":"<p>Show progress for long-running operations:</p> <pre><code>from shiny import ui\n\n@reactive.Effect\n@reactive.event(input.run_analysis)\ndef run_long_analysis():\n    with ui.Progress(min=0, max=100) as progress:\n        progress.set(message=\"Loading data\", value=0)\n        data = load_large_dataset()\n\n        progress.set(message=\"Processing\", value=50)\n        results = process_data(data)\n\n        progress.set(message=\"Finalizing\", value=90)\n        save_results(results)\n\n        progress.set(value=100)\n\n    ui.notification_show(\"Analysis complete!\", type=\"success\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#integration-with-analyzers","title":"Integration with Analyzers","text":""},{"location":"guides/contributing/dashboards/shiny/#accessing-analyzer-data","title":"Accessing Analyzer Data","text":"<pre><code>def factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Access primary analyzer outputs\n    main_data = pl.read_parquet(\n        context.base.table(\"main_analysis\").parquet_path\n    )\n\n    # Access secondary analyzer outputs\n    summary_data = pl.read_parquet(\n        context.dependency(summary_analyzer).table(\"summary\").parquet_path\n    )\n\n    # Access parameters used in analysis\n    threshold = context.base_params.get(\"threshold\", 0.5)\n\n    # Build dashboard with this data\n    # ...\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#parameter-integration","title":"Parameter Integration","text":"<p>Use analyzer parameters in your dashboard:</p> <pre><code>def server(input, output, session):\n    # Get analyzer parameters\n    analyzer_threshold = context.base_params.get(\"threshold\", 0.5)\n\n    @render.text\n    def analysis_info():\n        return f\"Analysis run with threshold: {analyzer_threshold}\"\n\n    @render_widget\n    def threshold_comparison():\n        # Compare user input with analyzer parameter\n        user_threshold = input.user_threshold()\n        df_comparison = main_data.with_columns([\n            (pl.col(\"value\") &gt; analyzer_threshold).alias(\"analyzer_flag\"),\n            (pl.col(\"value\") &gt; user_threshold).alias(\"user_flag\")\n        ])\n\n        return create_comparison_plot(df_comparison)\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/contributing/dashboards/shiny/#efficient-data-processing","title":"Efficient Data Processing","text":"<pre><code>@reactive.Calc\ndef base_data():\n    # Load once and cache\n    return pl.read_parquet(data_path)\n\n@reactive.Calc  \ndef filtered_data():\n    # Efficient filtering with Polars\n    filters = []\n\n    if input.category() != \"All\":\n        filters.append(pl.col(\"category\") == input.category())\n\n    if input.date_range() is not None:\n        start, end = input.date_range()\n        filters.append(pl.col(\"date\").is_between(start, end))\n\n    if filters:\n        return base_data().filter(pl.all_horizontal(filters))\n\n    else:\n        return base_data()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code>@reactive.Calc\ndef expensive_calculation():\n    # Only runs when dependencies change\n    data = filtered_data()\n\n    # Use lazy evaluation\n    result = (\n        data\n        .group_by(\"category\")\n        .agg([\n            pl.col(\"value\").mean().alias(\"avg_value\"),\n            pl.col(\"value\").std().alias(\"std_value\"),\n            pl.col(\"value\").count().alias(\"count\")\n        ])\n        .sort(\"avg_value\", descending=True)\n    )\n\n    return result\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#testing-shiny-dashboards","title":"Testing Shiny Dashboards","text":""},{"location":"guides/contributing/dashboards/shiny/#unit-testing-components","title":"Unit Testing Components","text":"<pre><code>import pytest\nfrom shiny.testing import ShinyAppProc\nfrom your_presenter import factory\n\ndef test_dashboard_loads():\n    \"\"\"Test that dashboard loads without errors\"\"\"\n    app = factory(mock_context)\n\n    # Test UI renders\n    assert app.shiny.panel is not None\n\n    # Test server function exists\n    assert callable(app.shiny.server_handler)\n\ndef test_data_filtering():\n    \"\"\"Test reactive data filtering\"\"\"\n    with ShinyAppProc(factory(mock_context)) as proc:\n        # Set input values\n        proc.set_inputs(category=\"TypeA\", threshold=50)\n\n        # Check outputs update correctly\n        output = proc.get_output(\"summary_stats\")\n        assert \"TypeA\" in output\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#integration-testing","title":"Integration Testing","text":"<pre><code>def test_with_real_data():\n    \"\"\"Test dashboard with actual analyzer output\"\"\"\n    # Run analyzer to generate test data\n    context = create_test_context(test_data_path)\n\n    # Test dashboard with real data\n    app = factory(context)\n\n    # Verify data loads correctly\n    assert app.shiny.panel is not None\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"guides/contributing/dashboards/shiny/#resource-management","title":"Resource Management","text":"<ul> <li>Use <code>@reactive.Calc</code> for expensive operations to enable caching</li> <li>Implement pagination for large datasets</li> <li>Consider data sampling for very large visualizations</li> <li>Use lazy loading for secondary data</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/#error-handling","title":"Error Handling","text":"<pre><code>@render_widget\ndef safe_plot():\n    try:\n        data = filtered_data()\n        if len(data) == 0:\n            return empty_plot_message()\n\n        return create_plot(data)\n\n    except Exception as e:\n        ui.notification_show(f\"Plot error: {str(e)}\", type=\"error\")\n        return error_plot()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#session-management","title":"Session Management","text":"<pre><code>def server(input, output, session):\n    # Clean up resources when session ends\n    @reactive.Effect\n    def cleanup():\n        session.on_ended(lambda: cleanup_user_data())\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#example-complete-dashboard","title":"Example: Complete Dashboard","text":"<p>Here's a complete example of a Shiny dashboard for analyzing message sentiment:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport polars as pl\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df_sentiment = pl.read_parquet(\n        context.base.table(\"sentiment_analysis\").parquet_path\n    )\n\n    # Get unique values for inputs\n    date_range = (df_sentiment[\"date\"].min(), df_sentiment[\"date\"].max())\n    categories = [\"All\"] + df_sentiment[\"category\"].unique().to_list()\n\n    # UI Layout\n    dashboard = ui.page_sidebar(\n        ui.sidebar(\n            ui.h3(\"Analysis Controls\"),\n            ui.input_date_range(\n                \"date_filter\", \n                \"Date Range\",\n                start=date_range[0],\n                end=date_range[1]\n            ),\n            ui.input_selectize(\n                \"category_filter\",\n                \"Categories\", \n                choices=categories,\n                selected=\"All\",\n                multiple=True\n            ),\n            ui.input_slider(\n                \"sentiment_threshold\",\n                \"Sentiment Threshold\",\n                -1, 1, 0, step=0.1\n            ),\n            ui.hr(),\n            ui.input_action_button(\"reset\", \"Reset Filters\"),\n            ui.download_button(\"download\", \"Download Data\")\n        ),\n\n        ui.div(\n            ui.h2(\"Sentiment Analysis Dashboard\"),\n\n            ui.row(\n                ui.column(6, ui.value_box(\n                    title=\"Total Messages\",\n                    value=ui.output_text(\"total_count\"),\n                    theme=\"primary\"\n                )),\n                ui.column(6, ui.value_box(\n                    title=\"Avg Sentiment\", \n                    value=ui.output_text(\"avg_sentiment\"),\n                    theme=\"success\"\n                ))\n            ),\n\n            ui.navset_tab(\n                ui.nav_panel(\n                    \"Time Series\",\n                    output_widget(\"timeseries_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Distribution\", \n                    output_widget(\"distribution_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Data Table\",\n                    ui.output_data_frame(\"data_table\")\n                )\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            data = df_sentiment\n\n            # Date filtering\n            if input.date_filter() is not None:\n                start, end = input.date_filter()\n                data = data.filter(pl.col(\"date\").is_between(start, end))\n\n            # Category filtering\n            if \"All\" not in input.category_filter():\n                data = data.filter(pl.col(\"category\").is_in(input.category_filter()))\n\n            # Sentiment filtering\n            data = data.filter(pl.col(\"sentiment\") &gt;= input.sentiment_threshold())\n\n            return data\n\n        @render.text\n        def total_count():\n            return f\"{len(filtered_data()):,}\"\n\n        @render.text\n        def avg_sentiment():\n            avg = filtered_data()[\"sentiment\"].mean()\n            return f\"{avg:.3f}\"\n\n        @render_widget\n        def timeseries_plot():\n            df_plot = (\n                filtered_data()\n                .group_by(\"date\")\n                .agg(pl.col(\"sentiment\").mean().alias(\"avg_sentiment\"))\n                .sort(\"date\")\n                .to_pandas()\n            )\n\n            fig = px.line(\n                df_plot, \n                x=\"date\", \n                y=\"avg_sentiment\",\n                title=\"Sentiment Over Time\"\n            )\n            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n            return fig\n\n        @render_widget\n        def distribution_plot():\n            df_plot = filtered_data().to_pandas()\n\n            fig = px.histogram(\n                df_plot,\n                x=\"sentiment\", \n                color=\"category\",\n                title=\"Sentiment Distribution\",\n                nbins=50\n            )\n            return fig\n\n        @render.data_frame\n        def data_table():\n            return filtered_data().to_pandas()\n\n        @reactive.Effect\n        @reactive.event(input.reset)\n        def reset_filters():\n            ui.update_date_range(\"date_filter\", start=date_range[0], end=date_range[1])\n            ui.update_selectize(\"category_filter\", selected=\"All\")\n            ui.update_slider(\"sentiment_threshold\", value=0)\n\n        @render.download(filename=\"sentiment_data.csv\")\n        def download():\n            return filtered_data().write_csv()\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Sentiment Analysis\", dashboard)\n        )\n    )\n</code></pre> <p>This comprehensive guide covers all aspects of building Shiny dashboards for your analyzer platform. The reactive programming model, rich widget ecosystem, and seamless Python integration make Shiny an excellent choice for creating sophisticated data analysis interfaces.</p>"},{"location":"guides/contributing/dashboards/shiny/#next-steps","title":"Next Steps","text":"<p>Once you finish reading section be a good idea to review the section that discuss implementing React dashboards. Might also be a good idea to review the sections for each domain. </p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/architecture/","title":"Architecture","text":"<p>Before contributing please refer to our Contributor Workflow</p>"},{"location":"guides/design-philosophy/architecture/#application-design-overview","title":"Application Design Overview","text":"<p>The CIB \ud83e\udd6d application is a terminal-based tool for performing data analysis and visualization. It is designed to be modular and extensible, allowing developers to contribute new analysis modules and visualization components while providing a consistent user experience around data import, preprocessing, and output generation.</p> <p>This design is motivated by a common pain point when moving from a data analysis script for private use to a tool that can be shared with others: A script for private consumption carries assumptions about the desired input and output data format and structure that are convenient to its author. When such a script is made available to others, debates on these aspects often arise. For a suite of analyses that this project aims to offer, if left decentralized, this debate can lead to inconsistent UX offerings across analyses, code duplication, and even bugs.</p> <p>The architecture of the CIB \ud83e\udd6d application is designed to address this problem by providing a clear separation between the core application logic and the analysis modules, such that the analysis module does not need to be concerned with the input and output data format and structure; such responsibilities are handled by the core application, where we aim to provide a rich, consistent, and intuitive user experience.</p>"},{"location":"guides/design-philosophy/architecture/#architecture-overview","title":"Architecture Overview","text":"<p>The application has three \"domains\": - The Core domain is responsible for workspace management, user flow, and integration of analysis runs and data import/export in a generic sense. It has three parts that correspond loosely to the MVC paradigm.   - The Application defines the workspace logic and exposes generic capabilities for importing and exporting data as well as analyses and dashboards. This is the \"controller\" part.   - The Terminal Components render the terminal interface and handle user input. This is the \"view\" part.   - The Storage IO persists the workspace data and is responsible for reading and writing data. This is the \"model\" part.</p> <p>The core application provides the context necessary for the other domains to function in a way that allows them to be agnostic about the specifics of the workspace and user flow.</p> <ul> <li>The Edge domain is responsible for data import and export while being agnostic about the specific analysis being run. Currently, this consists of the Importers and the Semantic Preprocessor.</li> </ul> <p>Note that the Storage IO is currently responsible for data export, but we should consider moving this to the Edge domain to allow for more extensibility and looser coupling.</p> <ul> <li>The Content domain is responsible for the actual data analysis and visualization and is agnostic about data import/export or workspace specifics. This consists of the Analyzers (both Primary and Secondary) as well as the Web Presenters.</li> </ul> <pre><code>flowchart TD\n    terminal[\"Terminal (core)\"]\n    application[\"Application (core)\"]\n    storage[\"Storage (core)\"]\n\n    importers[\"Importers (edge)\"]\n    semantic[\"Semantic Preprocessor (edge)\"]\n\n    content[\"Analyzers/Web Presenters (content)\"]\n\n    terminal --&gt; application\n    application --&gt; storage\n\n    application --&gt; importers\n    application --&gt; semantic\n\n    application --&gt; content\n</code></pre>"},{"location":"guides/design-philosophy/architecture/#questions-comments-and-feedback","title":"Questions, Comments, and Feedback","text":"<p>Talk to us on the Civic Tech DC Slack workspace!</p>"},{"location":"guides/design-philosophy/architecture/#next-steps","title":"Next Steps","text":"<p>It would be recommended to review the sections for each domain, and the section for implementing analyzers. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Implementing Analyzers</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/content-domain/","title":"Content Domain","text":""},{"location":"guides/design-philosophy/content-domain/#content-domain","title":"Content Domain","text":"<p>The Content domain is where the analysis and visualization happen.</p> <p>An analysis is added to the application by defining a Primary Analyzer, which comes with an interface declaration and an implementation. The interface declaration defines the input data structure and the output tables, which the application depends on for user guidance. The implementation is made workspace-agnostic by means of the \"context\" object.</p> <p>The goal of the Primary Analyzer is to produce a set of output tables that can be used by other analyzers, including Secondary Analyzers and Web Presenters. Primary Analyzer outputs are ideally normalized, non-duplicated, and non-redundant. As such, they are not always suitable for direct user consumption. It is the job of the Secondary Analyzers to produce user-friendly outputs and the job of Web Presenters to produce interactive visualizations.</p> <p>Both Secondary Analyzers and Web Presenters are also defined using interface objects. Secondary Analyzers will depend on the output of Primary Analyzers, and Web Presenters will depend on the output of both Primary and Secondary Analyzers.</p>"},{"location":"guides/design-philosophy/content-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/core-domain/","title":"Core Domain","text":""},{"location":"guides/design-philosophy/core-domain/#core-domain","title":"Core Domain","text":""},{"location":"guides/design-philosophy/core-domain/#application","title":"Application","text":"<p>The Application lives inside the <code>app</code> directory in the project root. This is responsible for defining and executing all capabilities of the application's workspace. Any extension or modification of the application's workspace capabilities should be done here.</p> <p>The application code should be free of specific storage implementation and be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the application module looks like</p> <p>./app/init.py:</p> <pre><code>from .analysis_context import AnalysisContext\nfrom .analysis_output_context import AnalysisOutputContext\nfrom .analysis_webserver_context import AnalysisWebServerContext\nfrom .app import App\nfrom .app_context import AppContext\nfrom .project_context import ProjectContext\nfrom .settings_context import SettingsContext\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#terminal-components","title":"Terminal Components","text":"<p>The Terminal Components live inside the <code>terminal_tools</code> inside the project root. Their main responsibility is user flow, rendering the terminal interface, and handling user input.</p> <p>The user flow understandably depends on the set of capabilities offered by the Application, so an adjustment there may require an adjustment here.</p> <p>Here's what the entrypoint for the termnal module looks like</p> <p>./terminal_tools/init.py</p> <pre><code>from .progress import ProgressReporter\nfrom .utils import (\n    clear_printed_lines,\n    clear_terminal,\n    draw_box,\n    enable_windows_ansi_support,\n    open_directory_explorer,\n    print_ascii_table,\n    wait_for_key,\n)\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#storage-io","title":"Storage IO","text":"<p>The Storage IO lives Inside the <code>storage</code> directory inside the project root. It is responsible for interacting directly with the file system where the workspace data and data files are stored. It makes decisions on paths, intermediate file formats, and database schema and implementation. It should know as little as possible about how the data is used and should be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the storage module looks like</p> <p>./storage/init.py:</p> <pre><code>import math\nimport os\nimport re\nimport shutil\nfrom datetime import datetime\nfrom typing import Callable, Iterable, Literal, Optional\n\nimport platformdirs\nimport polars as pl\nimport pyarrow.parquet as pq\nfrom filelock import FileLock\nfrom pydantic import BaseModel\nfrom tinydb import Query, TinyDB\nfrom xlsxwriter import Workbook\n\nfrom analyzer_interface.interface import AnalyzerOutput\nfrom analyzer_interface.params import ParamValue\n\nfrom .file_selector import FileSelectorStateManager\n\n\nclass ProjectModel(BaseModel):\n    class_: Literal[\"project\"] = \"project\"\n    id: str\n    display_name: str\n\n\nclass SettingsModel(BaseModel):\n    class_: Literal[\"settings\"] = \"settings\"\n    export_chunk_size: Optional[int | Literal[False]] = None\n\n\nclass FileSelectionState(BaseModel):\n    class_: Literal[\"file_selector_state\"] = \"file_selector_state\"\n    last_path: Optional[str] = None\n\n\nclass AnalysisModel(BaseModel):\n    class_: Literal[\"analysis\"] = \"analysis\"\n    analysis_id: str\n    project_id: str\n    display_name: str\n    primary_analyzer_id: str\n    path: str\n    column_mapping: Optional[dict[str, str]] = None\n    create_timestamp: Optional[float] = None\n    param_values: dict[str, ParamValue] = dict()\n    is_draft: bool = False\n\n    def create_time(self):\n        return (\n            datetime.fromtimestamp(self.create_timestamp)\n            if self.create_timestamp\n            else None\n        )\n\n\nSupportedOutputExtension = Literal[\"parquet\", \"csv\", \"xlsx\", \"json\"]\n\n\nclass Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return pl.read_parquet(output_path)\n\n    def get_secondary_output_parquet_path(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        return os.path.join(\n            self._get_project_secondary_output_root_path(analysis, secondary_id),\n            f\"{output_id}.parquet\",\n        )\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n\n\nclass TableStats(BaseModel):\n    num_rows: int\n\n\ndef collect_dataframe_chunks(\n    input: Iterable[pl.DataFrame], size_threshold: int\n) -&gt; Iterable[pl.DataFrame]:\n    output_buffer = []\n    size = 0\n    for df in input:\n        while True:\n            available_space = size_threshold - size\n            slice = df.head(available_space)\n            output_buffer.append(slice)\n            size = size + slice.height\n            remaining_space = available_space - slice.height\n\n            if remaining_space == 0:\n                yield pl.concat(output_buffer)\n                output_buffer = []\n                size = 0\n\n            if slice.height == df.height:\n                break\n            else:\n                df = df.tail(-available_space)\n\n    if output_buffer:\n        yield pl.concat(output_buffer)\n\n\nclass AppFileSelectorStateManager(FileSelectorStateManager):\n    def __init__(self, storage: \"Storage\"):\n        self.storage = storage\n\n    def get_current_path(self):\n        return self._load_state().last_path\n\n    def set_current_path(self, path: str):\n        self._save_state(path)\n\n    def _load_state(self):\n        q = Query()\n        state = self.storage.db.search(q[\"class_\"] == \"file_selector_state\")\n        if state:\n            return FileSelectionState(**state[0])\n        return FileSelectionState()\n\n    def _save_state(self, last_path: str):\n        self.storage.db.upsert(\n            FileSelectionState(last_path=last_path).model_dump(),\n            Query()[\"class_\"] == \"file_selector_state\",\n        )\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/edge-domain/","title":"Edge Domain","text":""},{"location":"guides/design-philosophy/edge-domain/#edge-domain","title":"Edge Domain","text":"<p>The Edge domain governs data import and export.</p>"},{"location":"guides/design-philosophy/edge-domain/#importers","title":"Importers","text":"<p>The Importers live inside the <code>importing</code> directory inside the project root. Each importer offers a new way to import data into the workspace. The importers should be agnostic about the available analyzers. However, the Importers currently provide a terminal user flow so that their options can be customized by the user\u2014a necessity since each importer may expose different sets of options and may have different UX approaches for their configuration.</p> <p>The importers eventually write data to a parquet file, whose path is provisioned by the application.</p> <p>Here's what the entrypoint for the importer module looks like</p> <p>./importing/init.py:</p> <pre><code>from .csv import CSVImporter\nfrom .excel import ExcelImporter\nfrom .importer import Importer, ImporterSession\n\nimporters: list[Importer[ImporterSession]] = [CSVImporter(), ExcelImporter()]\n</code></pre>"},{"location":"guides/design-philosophy/edge-domain/#semantic-preprocessor","title":"Semantic Preprocessor","text":"<p>The Semantic Preprocessor lives inside the <code>preprocessing</code> directory inside the project root. It defines all the column data semantics\u2014a kind of type system that is used to guide the user in selecting the right columns for the right analysis. It is agnostic about the specific analyzers but does depend on them in a generic way\u2014the available semantics exist to support the needs of analyzers and will be extended as necessary.</p> <p>Here's what the entrypoint for the preprocessing module looks like</p> <p>./preprocessing/series_semantic.py:</p> <pre><code>from datetime import datetime\nfrom typing import Callable, Type, Union\n\nimport polars as pl\nfrom pydantic import BaseModel\n\nfrom analyzer_interface import DataType\n\n\nclass SeriesSemantic(BaseModel):\n    semantic_name: str\n    column_type: Union[Type[pl.DataType], Callable[[pl.DataType], bool]]\n    prevalidate: Callable[[pl.Series], bool] = lambda s: True\n    try_convert: Callable[[pl.Series], pl.Series]\n    validate_result: Callable[[pl.Series], pl.Series] = lambda s: s.is_not_null()\n    data_type: DataType\n\n    def check(self, series: pl.Series, threshold: float = 0.8, sample_size: int = 100):\n        if not self.check_type(series):\n            return False\n\n        sample = sample_series(series, sample_size)\n        try:\n            if not self.prevalidate(sample):\n                return False\n            result = self.try_convert(sample)\n        except Exception:\n            return False\n        return self.validate_result(result).sum() / sample.len() &gt; threshold\n\n    def check_type(self, series: pl.Series):\n        if isinstance(self.column_type, type):\n            return isinstance(series.dtype, self.column_type)\n        return self.column_type(series.dtype)\n\n\ndatetime_string = SeriesSemantic(\n    semantic_name=\"datetime\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strptime(pl.Datetime, strict=False),\n    data_type=\"datetime\",\n)\n\n\ntimestamp_seconds = SeriesSemantic(\n    semantic_name=\"timestamp_seconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: (s * 1_000).cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\ntimestamp_milliseconds = SeriesSemantic(\n    semantic_name=\"timestamp_milliseconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: s.cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\nurl = SeriesSemantic(\n    semantic_name=\"url\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(\"^https?://\").gt(0),\n    data_type=\"url\",\n)\n\nidentifier = SeriesSemantic(\n    semantic_name=\"identifier\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(r\"^@?[A-Za-z0-9_.:-]+$\").eq(1),\n    data_type=\"identifier\",\n)\n\ntext_catch_all = SeriesSemantic(\n    semantic_name=\"free_text\",\n    column_type=pl.String,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"text\",\n)\n\ninteger_catch_all = SeriesSemantic(\n    semantic_name=\"integer\",\n    column_type=lambda dt: dt.is_integer(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"integer\",\n)\n\nfloat_catch_all = SeriesSemantic(\n    semantic_name=\"float\",\n    column_type=lambda dt: dt.is_float(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"float\",\n)\n\nboolean_catch_all = SeriesSemantic(\n    semantic_name=\"boolean\",\n    column_type=pl.Boolean,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"boolean\",\n)\n\nall_semantics = [\n    datetime_string,\n    timestamp_seconds,\n    timestamp_milliseconds,\n    url,\n    identifier,\n    text_catch_all,\n    integer_catch_all,\n    float_catch_all,\n    boolean_catch_all,\n]\n\n\ndef infer_series_semantic(\n    series: pl.Series, *, threshold: float = 0.8, sample_size=100\n):\n    for semantic in all_semantics:\n        if semantic.check(series, threshold=threshold, sample_size=sample_size):\n            return semantic\n    return None\n\n\ndef sample_series(series: pl.Series, n: int = 100):\n    if series.len() &lt; n:\n        return series\n    return series.sample(n, seed=0)\n\n\ndef constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"guides/design-philosophy/edge-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/get-started/installation/","title":"Installation","text":""},{"location":"guides/get-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"guides/get-started/installation/#required-software","title":"Required Software","text":"<ul> <li>Python 3.12 - Required for all features to work correctly</li> <li>Node.JS (20.0.0 or above) - Required for the React dashboards   to work correctly</li> <li>Git - For version control and contributing</li> <li>Terminal/Command Line - Application runs in terminal interface</li> </ul>"},{"location":"guides/get-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Windows (PowerShell), macOS, Linux</li> <li>Memory: 4GB+ RAM (for processing large datasets)</li> <li>Storage: 1GB+ free space (for project data and virtual environment)</li> </ul>"},{"location":"guides/get-started/installation/#resources","title":"Resources","text":"<p>If you haven't installed git, node.js, and/or python yet refer to the following links for instructions on downloading and installing said packages:</p> <ul> <li>https://codefinity.com/blog/A-step-by-step-guide-to-Git-installation</li> <li>https://nodejs.org/en/download</li> <li>https://realpython.com/installing-python/</li> </ul>"},{"location":"guides/get-started/installation/#checking-dependencies","title":"Checking Dependencies","text":"<p>If you're not sure which packages you already have installed on your system, the following commands can be used to figure what packages you already installed:</p>"},{"location":"guides/get-started/installation/#linux-mac-os","title":"Linux &amp; Mac OS","text":"<pre><code>which &lt;program_name_here (node|python|git)&gt;\n</code></pre>"},{"location":"guides/get-started/installation/#windows","title":"Windows","text":"<pre><code>where.exe &lt;program_name_here (node|python|git)&gt; \n</code></pre>"},{"location":"guides/get-started/installation/#installation","title":"Installation","text":""},{"location":"guides/get-started/installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/CIB-Mango-Tree/mango-tango-cli.git\ncd mango-tango-cli\n</code></pre>"},{"location":"guides/get-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\n</code></pre> <p>Verify Python version:</p> <pre><code>python --version  # Should show Python 3.12.x\n</code></pre>"},{"location":"guides/get-started/installation/#3-bootstrap-development-environment","title":"3. Bootstrap Development Environment","text":"<p>Mac OS/Linux (Bash):</p> <pre><code>./bootstrap.sh\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>./bootstrap.ps1\n</code></pre> <p>The bootstrap script will:</p> <ul> <li>Activate the virtual environment</li> <li>Install all dependencies from <code>requirements-dev.txt</code></li> <li>Set up pre-commit hooks for code formatting</li> </ul>"},{"location":"guides/get-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python -m mangotango --noop\n</code></pre> <p>Should output: \"No-op flag detected. Exiting successfully.\"</p>"},{"location":"guides/get-started/installation/#activating-virtual-environment","title":"Activating Virtual Environment","text":"<p>After Completing the Installation the following commands can be used to activate the virtual environment in order to work with the project.</p> <p>Mac OS/Linux (Bash):</p> <pre><code>source ./venv/bin/activate\n</code></pre> <p>PowerShell (Windows):</p> <pre><code>./env/bin/Activate.ps1\n</code></pre>"},{"location":"guides/get-started/installation/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"guides/get-started/installation/#dependencies-overview","title":"Dependencies Overview","text":"<p>Production Dependencies (<code>requirements.txt</code>):</p> <ul> <li><code>polars==1.9.0</code> - Primary data processing</li> <li><code>pydantic==2.9.1</code> - Data validation and models</li> <li><code>inquirer==3.4.0</code> - Interactive terminal prompts</li> <li><code>tinydb==4.8.0</code> - Lightweight JSON database</li> <li><code>dash==2.18.1</code> - Web dashboard framework</li> <li><code>shiny==1.4.0</code> - Modern web UI framework</li> <li><code>plotly==5.24.1</code> - Data visualization</li> <li><code>XlsxWriter==3.2.0</code> - Excel export functionality</li> </ul> <p>Development Dependencies (<code>requirements-dev.txt</code>):</p> <ul> <li><code>black==24.10.0</code> - Code formatter</li> <li><code>isort==5.13.2</code> - Import organizer</li> <li><code>pytest==8.3.4</code> - Testing framework</li> <li><code>pyinstaller==6.14.1</code> - Executable building</li> </ul> <p>React Dashboard Dependencies (app/web_templates/package.json):</p> <ul> <li>typescript: 5.7.3</li> <li>vite: 6.3.5</li> <li>react: 19.0.0</li> <li>@deck.gl: 9.1.11</li> <li>@visx: 3.12.0</li> <li>@glideapps/glide-data-grid: 6.0.3</li> <li>@radix-ui: (Varies based on component being used)</li> <li>zustand: 5.0.3</li> <li>tailwindcss: 4.0.6</li> <li>lucide-react: 0.475.0</li> </ul>"},{"location":"guides/get-started/installation/#code-formatting-setup","title":"Code Formatting Setup","text":"<p>The project uses automatic code formatting:</p> <ul> <li>Black: Code style and formatting</li> <li>isort: Import organization</li> <li>Pre-commit hooks: Automatic formatting on commit</li> </ul> <p>Manual formatting:</p> <pre><code>isort .\nblack .\n</code></pre>"},{"location":"guides/get-started/installation/#project-structure-setup","title":"Project Structure Setup","text":"<p>After installation, your project structure should be:</p> <pre><code>mango-tango-cli/\n\u251c\u2500\u2500 venv/                    # Virtual environment\n\u251c\u2500\u2500 .serena/                 # Serena semantic analysis\n\u2502   \u2514\u2500\u2500 memories/           # Project knowledge base\n\u251c\u2500\u2500 docs/                    # Documentation\n\u2502   \u251c\u2500\u2500 ai-context/         # AI assistant context\n\u2502   \u2514\u2500\u2500 dev-guide.md        # Development guide\n\u251c\u2500\u2500 app/                     # Application layer\n\u251c\u2500\u2500 analyzers/              # Analysis modules\n\u251c\u2500\u2500 components/             # Terminal UI components\n\u251c\u2500\u2500 storage/                # Data persistence\n\u251c\u2500\u2500 importing/              # Data import modules\n\u251c\u2500\u2500 requirements*.txt       # Dependencies\n\u2514\u2500\u2500 mangotango.py          # Main entry point\n</code></pre>"},{"location":"guides/get-started/installation/#database-and-storage-setup","title":"Database and Storage Setup","text":""},{"location":"guides/get-started/installation/#application-data-directory","title":"Application Data Directory","text":"<p>The application automatically creates data directories:</p> <ul> <li>macOS: <code>~/Library/Application Support/MangoTango/</code></li> <li>Windows: <code>%APPDATA%/Civic Tech DC/MangoTango/</code></li> <li>Linux: <code>~/.local/share/MangoTango/</code></li> </ul>"},{"location":"guides/get-started/installation/#database-initialization","title":"Database Initialization","text":"<ul> <li>TinyDB: Automatically initialized on first run</li> <li>Project Files: Created in user data directory</li> <li>Parquet Files: Used for all analysis data storage</li> </ul> <p>No manual database setup required.</p>"},{"location":"guides/get-started/installation/#running-the-application","title":"Running the Application","text":""},{"location":"guides/get-started/installation/#basic-usage","title":"Basic Usage","text":"<pre><code># Start the application\npython -m mangotango\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode","title":"Development Mode","text":"<pre><code># Run with debugging/development flags\npython -m mangotango --noop  # Test mode, exits immediately\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode-for-the-react-dashboards","title":"Development Mode for The React Dashboards","text":"<p>The following commands can be used to start the development vite server for the react dashboards that are currently in development.</p> <p>npm:</p> <pre><code>cd ./app/web_templates\nnpm run dev\n</code></pre> <p>pnpm:</p> <pre><code>cd ./app/web_templates\npnpm dev\n</code></pre>"},{"location":"guides/get-started/installation/#testing-setup","title":"Testing Setup","text":""},{"location":"guides/get-started/installation/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest analyzers/hashtags/test_hashtags_analyzer.py\n\n# Run with verbose output\npytest -v\n\n# Run specific test function\npytest analyzers/hashtags/test_hashtags_analyzer.py::test_gini\n</code></pre>"},{"location":"guides/get-started/installation/#test-data","title":"Test Data","text":"<ul> <li>Test data is co-located with analyzers in <code>test_data/</code> directories</li> <li>Each analyzer should include its own test files</li> <li>Tests use sample data to verify functionality</li> </ul>"},{"location":"guides/get-started/installation/#build-setup-optional","title":"Build Setup (Optional)","text":""},{"location":"guides/get-started/installation/#executable-building","title":"Executable Building","text":"<pre><code># Build standalone executable\npyinstaller pyinstaller.spec\n\n# Output will be in dist/ directory\n</code></pre>"},{"location":"guides/get-started/installation/#bundle-building-for-react-dashboard","title":"Bundle Building for React Dashboard","text":"<p>npm:</p> <pre><code>npm run build\n</code></pre> <p>pnpm:</p> <pre><code>pnpm build\n</code></pre>"},{"location":"guides/get-started/installation/#build-requirements","title":"Build Requirements","text":"<ul> <li>Included in <code>requirements-dev.txt</code></li> <li>Used primarily for release distribution</li> <li>Not required for development</li> </ul>"},{"location":"guides/get-started/installation/#ide-integration","title":"IDE Integration","text":""},{"location":"guides/get-started/installation/#recommended-ide-settings","title":"Recommended IDE Settings","text":"<p>VS Code (<code>.vscode/</code> configuration):</p> <ul> <li>Python interpreter: <code>./venv/bin/python</code></li> <li>Black formatter integration</li> <li>isort integration</li> <li>pytest test discovery</li> </ul> <p>PyCharm:</p> <ul> <li>Interpreter: Project virtual environment</li> <li>Code style: Black</li> <li>Import optimizer: isort</li> </ul>"},{"location":"guides/get-started/installation/#git-configuration","title":"Git Configuration","text":"<p>Pre-commit Hooks:</p> <pre><code># Hooks are set up automatically by bootstrap script\n# Manual setup if needed:\npip install pre-commit\npre-commit install\n</code></pre> <p>Git Flow:</p> <ul> <li>Branch from <code>develop</code> (not <code>main</code>)</li> <li>Feature branches: <code>feature/name</code></li> <li>Bug fixes: <code>bugfix/name</code></li> </ul>"},{"location":"guides/get-started/installation/#version-management","title":"Version Management","text":"<p>If you already have Python and Node.JS installed but are on different versions from the versions outlined in the requirements above you can switch to the correct versions for both languages for the project using version managers. The version manager for python is pyenv. Where the version manager that is recommended for Node is nvm. Guides for installing both version managers are linked down below if you need references to go off of.</p> <ul> <li>https://www.freecodecamp.org/news/node-version-manager-nvm-install-guide/</li> <li>https://github.com/pyenv/pyenv?tab=readme-ov-file#installation</li> <li>https://github.com/pyenv-win/pyenv-win?tab=readme-ov-file#installation   (If you're on windows and want to install pyenv)</li> </ul> <p>Once you have both version managers installed the following commands can be used to switch versions.</p>"},{"location":"guides/get-started/installation/#pyenv","title":"pyenv","text":"<pre><code>pyenv install 3.12\npyenv local 3.12\n</code></pre>"},{"location":"guides/get-started/installation/#nvm","title":"nvm","text":"<pre><code>nvm install v21.0.0\nnvm use v21.0.0\n</code></pre>"},{"location":"guides/get-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/get-started/installation/#common-dependency-issues","title":"Common Dependency Issues","text":"<p>One common issue when installing the dependencies for python is the installation failing due to compatibility issues with the python package <code>pyarrow</code>. The compatibility issues are due to a version mismatch between pyarrow and python itself. To resolve this issue,you MUST be on version 3.12 for python. Refer to commands above to switch to the correct version.</p> <p>Similarly, the installation for node dependencies has been known to fail for some developers due to a version mismatch caused by the underlying dependencies for the package <code>@glideapps/glide-data-grid</code>. However, getting around this issue is more straightforward with node packages. Running the installation command for node with the flag <code>--legacy-peer-deps</code> is enough for the installation to work if you run into this issue. The commands needed to run the installation manually from the project root are as such.</p> <pre><code>cd ./app/web_templates\nnpm install --legacy-peer-deps\n</code></pre>"},{"location":"guides/get-started/installation/#other-common-issues","title":"Other Common Issues","text":"<p>Import Errors:</p> <pre><code># Ensure virtual environment is activated\nsource venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\Activate.ps1     # Windows\n\n# Reinstall dependencies\npip install -r requirements-dev.txt\n</code></pre> <p>Formatting Errors in CI:</p> <pre><code># Run formatters locally before committing\nisort .\nblack .\n</code></pre> <p>Test Failures:</p> <pre><code># Ensure test data is present\nls analyzers/*/test_data/\n\n# Check if specific analyzer tests pass\npytest analyzers/hashtags/ -v\n</code></pre>"},{"location":"guides/get-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Optional Configuration:</p> <ul> <li><code>MANGOTANGO_DATA_DIR</code> - Override default data directory</li> <li><code>MANGOTANGO_LOG_LEVEL</code> - Set logging verbosity</li> </ul>"},{"location":"guides/get-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have everything installed and running without any problems, the next step is to check out the Contributor Workflow</p>"},{"location":"guides/get-started/overview/","title":"Overview","text":""},{"location":"guides/get-started/overview/#mango-tango-cli","title":"Mango Tango CLI","text":""},{"location":"guides/get-started/overview/#repository-overview","title":"Repository Overview","text":"<p>Mango Tango CLI is a Python terminal-based tool for social media data analysis and visualization. It provides a modular, extensible architecture that separates core application logic from analysis modules, ensuring consistent UX while allowing easy contribution of new analyzers. The following documentation in this section is meant to provide a general overview of how the codebase for the project is structured, and to provide some context on patterns used throughout the project.</p>"},{"location":"guides/get-started/overview/#purpose-domain","title":"Purpose &amp; Domain","text":"<ul> <li>Social Media Analytics: Hashtag analysis, n-gram analysis, temporal   patterns, user coordination</li> <li>Modular Architecture: Clear separation between data import/export,   analysis, and presentation</li> <li>Interactive Workflows: Terminal-based UI with web dashboard capabilities</li> <li>Extensible Design: Plugin-like analyzer system for easy expansion</li> </ul>"},{"location":"guides/get-started/overview/#tech-stack","title":"Tech Stack","text":"<ul> <li>Core: Python 3.12, Inquirer (CLI), TinyDB (metadata), Starlette &amp; Uvicorn (web-server)</li> <li>Data: Polars/Pandas, PyArrow, Parquet files</li> <li>Web: Dash, Shiny for Python, Plotly, React</li> <li>Dev Tools: Black, isort, pytest, PyInstaller</li> </ul>"},{"location":"reference/analyzer_interface/","title":"Analyzer Interface","text":""},{"location":"reference/analyzer_interface/#analyzer_interface","title":"<code>analyzer_interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap","title":"<code>column_automap</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.check_name_hint","title":"<code>check_name_hint(name, hint)</code>","text":"<p>Returns true if every word in the hint (split by spaces) is present in the name, in a case insensitive manner.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def check_name_hint(name: str, hint: str):\n    \"\"\"\n    Returns true if every word in the hint (split by spaces) is present in the name,\n    in a case insensitive manner.\n    \"\"\"\n    return all(word.lower().strip() in name.lower() for word in hint.split(\" \"))\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.column_automap","title":"<code>column_automap(user_columns, input_schema_columns)</code>","text":"<p>Matches user-provided columns to the expected columns based on the name hints.</p> <p>The resulting dictionary is keyed by the expected input column name.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def column_automap(\n    user_columns: list[UserInputColumn], input_schema_columns: list[InputColumn]\n):\n    \"\"\"\n    Matches user-provided columns to the expected columns based on the name hints.\n\n    The resulting dictionary is keyed by the expected input column name.\n    \"\"\"\n    matches: dict[str, str] = {}\n    for input_column in input_schema_columns:\n        max_score = None\n        best_match_user_column = None\n        for user_column in user_columns:\n            current_score = get_data_type_compatibility_score(\n                input_column.data_type, user_column.data_type\n            )\n\n            # Don't consider type-incompatible columns\n            if current_score is None:\n                continue\n\n            # Boost the score if we have a name hint match such that\n            # - among similarly compatible matches, those with name hints are preferred\n            # - among name hint matches, those with the best data type compatibility are preferred\n            if any(\n                check_name_hint(user_column.name, hint)\n                for hint in input_column.name_hints\n            ):\n                current_score += 10\n\n            if max_score is None or current_score &gt; max_score:\n                max_score = current_score\n                best_match_user_column = user_column\n\n        if best_match_user_column is not None:\n            matches[input_column.name] = best_match_user_column.name\n\n    return matches\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context","title":"<code>context</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.context.AssetsReader","title":"<code>AssetsReader</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class AssetsReader(ABC):\n    @abstractmethod\n    def table(self, output_id: str) -&gt; \"TableReader\":\n        \"\"\"\n        Gets the table reader for the specified output.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.AssetsReader.table","title":"<code>table(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the table reader for the specified output.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef table(self, output_id: str) -&gt; \"TableReader\":\n    \"\"\"\n    Gets the table reader for the specified output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext","title":"<code>BaseDerivedModuleContext</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class BaseDerivedModuleContext(ABC, BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    \"\"\"\n    Common interface for secondary analyzers and web presenters runtime contexts.\n    \"\"\"\n\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    progress_manager: Optional[\"ProgressManager\"] = None\n    \"\"\"\n  Optional progress manager shared from primary analyzer for continuous progress reporting.\n  Secondary analyzers and web presenters can use this to continue the progress flow\n  started by the primary analyzer.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def base_params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the primary analysis parameters.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def base(self) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the base primary analyzer's context, which lets you inspect and load its\n        outputs.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dependency(\n        self, secondary_interface: SecondaryAnalyzerInterface\n    ) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the context of a secondary analyzer the current module depends on, which\n        lets you inspect and load its outputs.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.base","title":"<code>base</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the base primary analyzer's context, which lets you inspect and load its outputs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.base_params","title":"<code>base_params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the primary analysis parameters.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Common interface for secondary analyzers and web presenters runtime contexts.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.progress_manager","title":"<code>progress_manager = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional progress manager shared from primary analyzer for continuous progress reporting. Secondary analyzers and web presenters can use this to continue the progress flow started by the primary analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.temp_dir","title":"<code>temp_dir</code>  <code>instance-attribute</code>","text":"<p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.dependency","title":"<code>dependency(secondary_interface)</code>  <code>abstractmethod</code>","text":"<p>Gets the context of a secondary analyzer the current module depends on, which lets you inspect and load its outputs.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef dependency(\n    self, secondary_interface: SecondaryAnalyzerInterface\n) -&gt; \"AssetsReader\":\n    \"\"\"\n    Gets the context of a secondary analyzer the current module depends on, which\n    lets you inspect and load its outputs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext","title":"<code>FactoryOutputContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for both factory and api_facotry functions for web presenters.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class FactoryOutputContext(BaseModel):\n    \"\"\"\n    Output interface for both factory and api_facotry functions for web\n    presenters.\n    \"\"\"\n\n    shiny: Optional[ShinyContext] = None\n    \"\"\"\n    Factory oputput for shiny dashboards\n    \"\"\"\n\n    api: Optional[dict[str, Any]] = None\n    \"\"\"\n    API factory output for React dashboard REST API\n    \"\"\"\n\n    data_frames: Optional[dict[str, DataFrame]] = None\n    \"\"\"\n    API factory dataframe output for React dashboard REST API\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.api","title":"<code>api = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>API factory output for React dashboard REST API</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.data_frames","title":"<code>data_frames = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>API factory dataframe output for React dashboard REST API</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.shiny","title":"<code>shiny = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Factory oputput for shiny dashboards</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.InputTableReader","title":"<code>InputTableReader</code>","text":"<p>               Bases: <code>TableReader</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class InputTableReader(TableReader):\n    @abstractmethod\n    def preprocess[PolarsDataFrameLike](\n        self, df: PolarsDataFrameLike\n    ) -&gt; PolarsDataFrameLike:\n        \"\"\"\n        Given the manually loaded user input dataframe, apply column mapping and\n        semantic transformations to give the input dataframe that the analyzer\n        expects.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.InputTableReader.preprocess","title":"<code>preprocess(df)</code>  <code>abstractmethod</code>","text":"<p>Given the manually loaded user input dataframe, apply column mapping and semantic transformations to give the input dataframe that the analyzer expects.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef preprocess[PolarsDataFrameLike](\n    self, df: PolarsDataFrameLike\n) -&gt; PolarsDataFrameLike:\n    \"\"\"\n    Given the manually loaded user input dataframe, apply column mapping and\n    semantic transformations to give the input dataframe that the analyzer\n    expects.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext","title":"<code>PrimaryAnalyzerContext</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class PrimaryAnalyzerContext(ABC, BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    progress_manager: Optional[ProgressManager] = None\n    \"\"\"\n  Optional progress manager for hierarchical progress reporting.\n  When provided, analyzers can use this to report progress with\n  visual feedback and memory monitoring capabilities.\n  \"\"\"\n\n    @abstractmethod\n    def input(self) -&gt; \"InputTableReader\":\n        \"\"\"\n        Gets the input reader context.\n\n        **Note that this is in function form** even though one input is expected,\n        in anticipation that we may want to support multiple inputs in the future.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the analysis parameters.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context for the specified output ID.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.params","title":"<code>params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the analysis parameters.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.progress_manager","title":"<code>progress_manager = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional progress manager for hierarchical progress reporting. When provided, analyzers can use this to report progress with visual feedback and memory monitoring capabilities.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.temp_dir","title":"<code>temp_dir</code>  <code>instance-attribute</code>","text":"<p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.input","title":"<code>input()</code>  <code>abstractmethod</code>","text":"<p>Gets the input reader context.</p> <p>Note that this is in function form even though one input is expected, in anticipation that we may want to support multiple inputs in the future.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef input(self) -&gt; \"InputTableReader\":\n    \"\"\"\n    Gets the input reader context.\n\n    **Note that this is in function form** even though one input is expected,\n    in anticipation that we may want to support multiple inputs in the future.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.output","title":"<code>output(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the output writer context for the specified output ID.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context for the specified output ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.SecondaryAnalyzerContext","title":"<code>SecondaryAnalyzerContext</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class SecondaryAnalyzerContext(BaseDerivedModuleContext):\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.SecondaryAnalyzerContext.output","title":"<code>output(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the output writer context</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext","title":"<code>ShinyContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for Shiny dashboards</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class ShinyContext(BaseModel):\n    \"\"\"\n    Output interface for Shiny dashboards\n    \"\"\"\n\n    panel: NavPanel = None\n    \"\"\"\n    UI navigation panel to be added to shiny dashboard\n    \"\"\"\n\n    server_handler: Optional[ServerCallback] = None\n    \"\"\"\n    Server handler callback to be called by the shiny application instance\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext.panel","title":"<code>panel = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>UI navigation panel to be added to shiny dashboard</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext.server_handler","title":"<code>server_handler = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Server handler callback to be called by the shiny application instance</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableReader","title":"<code>TableReader</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableReader(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should expect a parquet\n        file here.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableReader.parquet_path","title":"<code>parquet_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the path to the table's parquet file. The module should expect a parquet file here.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableWriter(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should write a parquet\n        file to it.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableWriter.parquet_path","title":"<code>parquet_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the path to the table's parquet file. The module should write a parquet file to it.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext","title":"<code>WebPresenterContext</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class WebPresenterContext(BaseDerivedModuleContext):\n    dash_app: Dash\n    \"\"\"\n  The Dash app that is being built.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def state_dir(self) -&gt; str:\n        \"\"\"\n        Gets the directory where the web presenter can store state that persists\n        between runs. This state space is unique for each\n        project/primary analyzer/web presenter combination.\n        \"\"\"\n        pass\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext.dash_app","title":"<code>dash_app</code>  <code>instance-attribute</code>","text":"<p>The Dash app that is being built.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext.state_dir","title":"<code>state_dir</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the directory where the web presenter can store state that persists between runs. This state space is unique for each project/primary analyzer/web presenter combination.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility","title":"<code>data_type_compatibility</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.data_type_mapping_preference","title":"<code>data_type_mapping_preference = {'text': [['text'], ['identifier', 'url']], 'integer': [['integer']], 'float': [['float', 'integer']], 'boolean': [['boolean']], 'datetime': [['datetime']], 'time': [['time'], ['datetime']], 'identifier': [['identifier'], ['url', 'datetime'], ['integer'], ['text']], 'url': [['url']]}</code>  <code>module-attribute</code>","text":"<p>For each data type, a list of lists of data types that are considered compatible with it. The first list is the most preferred, the last list is the least. The items in each list are considered equally compatible.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.get_data_type_compatibility_score","title":"<code>get_data_type_compatibility_score(expected_data_type, actual_data_type)</code>","text":"<p>Returns a score for the compatibility of the actual data type with the expected data type. Higher (less negative) scores are better. <code>None</code> means the data types are not compatible.</p> Source code in <code>analyzer_interface/data_type_compatibility.py</code> <pre><code>def get_data_type_compatibility_score(\n    expected_data_type: DataType, actual_data_type: DataType\n):\n    \"\"\"\n    Returns a score for the compatibility of the actual data type with the\n    expected data type. Higher (less negative) scores are better.\n    `None` means the data types are not compatible.\n    \"\"\"\n    if expected_data_type == actual_data_type:\n        return 0\n\n    for i, preference_list in enumerate(\n        data_type_mapping_preference[expected_data_type]\n    ):\n        if actual_data_type in preference_list:\n            return -(i + 1)\n\n    return None\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration","title":"<code>declaration</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration","title":"<code>AnalyzerDeclaration</code>","text":"<p>               Bases: <code>AnalyzerInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class AnalyzerDeclaration(AnalyzerInterface):\n    entry_point: Callable[[PrimaryAnalyzerContext], None]\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]]\n    is_distributed: bool\n\n    def __init__(\n        self,\n        interface: AnalyzerInterface,\n        main: Callable,\n        *,\n        is_distributed: bool = False,\n        default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n            lambda _: dict()\n        )\n    ):\n        \"\"\"Creates a primary analyzer declaration\n\n        Args:\n          interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n          main (Callable):\n            The entry point function for the primary analyzer. This function should\n            take a single argument of type `PrimaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n\n          is_distributed (bool):\n            Set this explicitly to `True` once the analyzer is ready to be shipped\n            to end users; it will make the analyzer available in the distributed\n            executable.\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(),\n            entry_point=main,\n            default_params=default_params,\n            is_distributed=is_distributed\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__","title":"<code>__init__(interface, main, *, is_distributed=False, default_params=lambda _: dict())</code>","text":"<p>Creates a primary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The metadata interface for the primary analyzer.</p> required <code>Callable</code> <p>The entry point function for the primary analyzer. This function should take a single argument of type <code>PrimaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required <code>bool</code> <p>Set this explicitly to <code>True</code> once the analyzer is ready to be shipped to end users; it will make the analyzer available in the distributed executable.</p> <code>False</code> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: AnalyzerInterface,\n    main: Callable,\n    *,\n    is_distributed: bool = False,\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n        lambda _: dict()\n    )\n):\n    \"\"\"Creates a primary analyzer declaration\n\n    Args:\n      interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n      main (Callable):\n        The entry point function for the primary analyzer. This function should\n        take a single argument of type `PrimaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n\n      is_distributed (bool):\n        Set this explicitly to `True` once the analyzer is ready to be shipped\n        to end users; it will make the analyzer available in the distributed\n        executable.\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(),\n        entry_point=main,\n        default_params=default_params,\n        is_distributed=is_distributed\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(main)","title":"<code>main</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(is_distributed)","title":"<code>is_distributed</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration","title":"<code>SecondaryAnalyzerDeclaration</code>","text":"<p>               Bases: <code>SecondaryAnalyzerInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class SecondaryAnalyzerDeclaration(SecondaryAnalyzerInterface):\n    entry_point: Callable[[\"SecondaryAnalyzerContext\"], None]\n\n    def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n        \"\"\"Creates a secondary analyzer declaration\n\n        Args:\n          interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n          main (Callable):\n            The entry point function for the secondary analyzer. This function should\n            take a single argument of type `SecondaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n        \"\"\"\n        super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__","title":"<code>__init__(interface, main)</code>","text":"<p>Creates a secondary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>SecondaryAnalyzerInterface</code> <p>The metadata interface for the secondary analyzer.</p> required <code>Callable</code> <p>The entry point function for the secondary analyzer. This function should take a single argument of type <code>SecondaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n    \"\"\"Creates a secondary analyzer declaration\n\n    Args:\n      interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n      main (Callable):\n        The entry point function for the secondary analyzer. This function should\n        take a single argument of type `SecondaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n    \"\"\"\n    super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__(main)","title":"<code>main</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration","title":"<code>WebPresenterDeclaration</code>","text":"<p>               Bases: <code>WebPresenterInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class WebPresenterDeclaration(WebPresenterInterface):\n    factory: Callable[[\"WebPresenterContext\"], Union[FactoryOutputContext, None]]\n    shiny: bool\n    server_name: str\n\n    def __init__(\n        self,\n        interface: WebPresenterInterface,\n        factory: Callable,\n        name: str,\n        shiny: bool,\n    ):\n        \"\"\"Creates a web presenter declaration\n\n        Args:\n          interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n          factory (Callable):\n            The factory function that creates a Dash app for the web presenter. It should\n            modify the Dash app in the context to add whatever plotting interface\n            the web presenter needs.\n\n          server_name (str):\n            The server name for the Dash app. Typically, you will use the global\n            variable `__name__` here.\n\n            If your web presenter has assets like images, CSS or JavaScript files,\n            you can put them in a folder named `assets` in the same directory\n            as the file where `__name__` is used. The Dash app will serve these\n            files at the `/assets/` URL, using the python module name in `__name__`\n            to determine the absolute path to the assets folder.\n\n            See Dash documentation for more details: https://dash.plotly.com\n            See also Python documentation for the `__name__` variable:\n            https://docs.python.org/3/tutorial/modules.html\n\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__","title":"<code>__init__(interface, factory, name, shiny)</code>","text":"<p>Creates a web presenter declaration</p> <p>Parameters:</p> Name Type Description Default <code>WebPresenterInterface</code> <p>The metadata interface for the web presenter.</p> required <code>Callable</code> <p>The factory function that creates a Dash app for the web presenter. It should modify the Dash app in the context to add whatever plotting interface the web presenter needs.</p> required <code>str</code> <p>The server name for the Dash app. Typically, you will use the global variable <code>__name__</code> here.</p> <p>If your web presenter has assets like images, CSS or JavaScript files, you can put them in a folder named <code>assets</code> in the same directory as the file where <code>__name__</code> is used. The Dash app will serve these files at the <code>/assets/</code> URL, using the python module name in <code>__name__</code> to determine the absolute path to the assets folder.</p> <p>See Dash documentation for more details: https://dash.plotly.com See also Python documentation for the <code>__name__</code> variable: https://docs.python.org/3/tutorial/modules.html</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: WebPresenterInterface,\n    factory: Callable,\n    name: str,\n    shiny: bool,\n):\n    \"\"\"Creates a web presenter declaration\n\n    Args:\n      interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n      factory (Callable):\n        The factory function that creates a Dash app for the web presenter. It should\n        modify the Dash app in the context to add whatever plotting interface\n        the web presenter needs.\n\n      server_name (str):\n        The server name for the Dash app. Typically, you will use the global\n        variable `__name__` here.\n\n        If your web presenter has assets like images, CSS or JavaScript files,\n        you can put them in a folder named `assets` in the same directory\n        as the file where `__name__` is used. The Dash app will serve these\n        files at the `/assets/` URL, using the python module name in `__name__`\n        to determine the absolute path to the assets folder.\n\n        See Dash documentation for more details: https://dash.plotly.com\n        See also Python documentation for the `__name__` variable:\n        https://docs.python.org/3/tutorial/modules.html\n\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(factory)","title":"<code>factory</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(server_name)","title":"<code>server_name</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DataType","title":"<code>DataType = Literal['text', 'integer', 'float', 'boolean', 'datetime', 'identifier', 'url', 'time']</code>  <code>module-attribute</code>","text":"<p>The semantic data type for a data column. This is not quite the same as structural data types like polars or pandas or even arrow types, but they represent how the data is intended to be interpreted.</p> <ul> <li><code>text</code> is expected to be a free-form human-readable text content.</li> <li><code>integer</code> and <code>float</code> are meant to be manipulated arithmetically.</li> <li><code>boolean</code> is a binary value.</li> <li><code>datetime</code> represents time and are meant to be manipulated as time values.</li> <li><code>time</code> represents time within a day, not including the date information.</li> <li><code>identifier</code> is a unique identifier for a record. It is not expected to be manipulated in any way.</li> <li><code>url</code> is a string that represents a URL.</li> </ul>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface","title":"<code>AnalyzerInterface</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerInterface(BaseAnalyzerInterface):\n    input: AnalyzerInput\n    \"\"\"\n  Specifies the input data schema for the analyzer.\n  \"\"\"\n\n    params: list[AnalyzerParam] = []\n    \"\"\"\n  A list of parameters that the analyzer accepts.\n  \"\"\"\n\n    outputs: list[\"AnalyzerOutput\"]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"primary\"] = \"primary\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>Specifies the input data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.outputs","title":"<code>outputs</code>  <code>instance-attribute</code>","text":"<p>Specifies the output data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.params","title":"<code>params = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of parameters that the analyzer accepts.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput","title":"<code>AnalyzerOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerOutput(BaseModel):\n    id: str\n    \"\"\"\n  Uniquely identifies the output data schema for the analyzer. The analyzer\n  must include this key in the output dictionary.\n  \"\"\"\n\n    name: str\n    \"\"\"The human-friendly for the output.\"\"\"\n\n    description: Optional[str] = None\n\n    columns: list[\"OutputColumn\"]\n\n    internal: bool = False\n\n    uses_multi_file_dataset: bool = False\n    \"\"\"\n    When True, this output will be stored as a multi-file dataset (directory with \n    multiple parquet files) instead of a single parquet file. This enables better \n    performance for large datasets by avoiding memory-intensive concatenation operations.\n    Defaults to False for backward compatibility.\n    \"\"\"\n\n    def get_column_by_name(self, name: str):\n        for column in self.columns:\n            if column.name == name:\n                return column\n        return None\n\n    def transform_output(self, output_df: pl.LazyFrame | pl.DataFrame):\n        output_columns = output_df.lazy().collect_schema().names()\n        return output_df.select(\n            [\n                pl.col(col_name).alias(\n                    output_spec.human_readable_name_or_fallback()\n                    if output_spec\n                    else col_name\n                )\n                for col_name in output_columns\n                if (output_spec := self.get_column_by_name(col_name)) or True\n            ]\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>Uniquely identifies the output data schema for the analyzer. The analyzer must include this key in the output dictionary.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The human-friendly for the output.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput.uses_multi_file_dataset","title":"<code>uses_multi_file_dataset = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When True, this output will be stored as a multi-file dataset (directory with  multiple parquet files) instead of a single parquet file. This enables better  performance for large datasets by avoiding memory-intensive concatenation operations. Defaults to False for backward compatibility.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam","title":"<code>AnalyzerParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerParam(BaseModel):\n    id: str\n    \"\"\"\n    The name of the parameter. This becomes the key in the parameters dictionary\n    that is passed to the analyzer.\n    \"\"\"\n\n    human_readable_name: Optional[str] = None\n    \"\"\"\n    The human-friendly name for the parameter. This is used in the UI to\n    represent the parameter.\n    \"\"\"\n\n    description: Optional[str] = None\n    \"\"\"\n    A short description of the parameter. This is used in the UI to represent\n    the parameter.\n    \"\"\"\n\n    type: ParamType\n    \"\"\"\n    The type of the parameter. This is used for validation and for customizing\n    the UX for parameter input.\n    \"\"\"\n\n    default: Optional[ParamValue] = None\n    \"\"\"\n    Optional: define a static default value for this parameter. A parameter\n    without a default will need to be chosen explicitly by the user.\n    \"\"\"\n\n    backfill_value: Optional[ParamValue] = None\n    \"\"\"\n    Recommended if this is a parameter that is newly introduced in a previously\n    released analyzer. The backfill is show what this parameter was before it\n    became customizable.\n    \"\"\"\n\n    @property\n    def print_name(self):\n        return self.human_readable_name or self.id\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.backfill_value","title":"<code>backfill_value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Recommended if this is a parameter that is newly introduced in a previously released analyzer. The backfill is show what this parameter was before it became customizable.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.default","title":"<code>default = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional: define a static default value for this parameter. A parameter without a default will need to be chosen explicitly by the user.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A short description of the parameter. This is used in the UI to represent the parameter.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.human_readable_name","title":"<code>human_readable_name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The human-friendly name for the parameter. This is used in the UI to represent the parameter.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>The name of the parameter. This becomes the key in the parameters dictionary that is passed to the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>The type of the parameter. This is used for validation and for customizing the UX for parameter input.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface","title":"<code>BaseAnalyzerInterface</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class BaseAnalyzerInterface(BaseModel):\n    id: str\n    \"\"\"\n  The static ID for the analyzer that, with the version, uniquely identifies the\n  analyzer and will be stored as metadata as part of the output data.\n  \"\"\"\n\n    version: str\n    \"\"\"\n  The version ID for the analyzer. In future, we may choose to support output\n  migration between versions of the same analyzer.\n  \"\"\"\n\n    name: str\n    \"\"\"\n  The short human-readable name of the analyzer.\n  \"\"\"\n\n    short_description: str\n    \"\"\"\n  A short, one-liner description of what the analyzer does.\n  \"\"\"\n\n    long_description: Optional[str] = None\n    \"\"\"\n  A longer description of what the analyzer does that will be shown separately.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>The static ID for the analyzer that, with the version, uniquely identifies the analyzer and will be stored as metadata as part of the output data.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.long_description","title":"<code>long_description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A longer description of what the analyzer does that will be shown separately.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The short human-readable name of the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.short_description","title":"<code>short_description</code>  <code>instance-attribute</code>","text":"<p>A short, one-liner description of what the analyzer does.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.version","title":"<code>version</code>  <code>instance-attribute</code>","text":"<p>The version ID for the analyzer. In future, we may choose to support output migration between versions of the same analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface","title":"<code>DerivedAnalyzerInterface</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class DerivedAnalyzerInterface(BaseAnalyzerInterface):\n    base_analyzer: AnalyzerInterface\n    \"\"\"\n  The base analyzer that this secondary analyzer extends. This is always a primary\n  analyzer. If your module depends on other secondary analyzers (which must have\n  the same base analyzer), you can specify them in the `depends_on` field.\n  \"\"\"\n\n    depends_on: list[\"SecondaryAnalyzerInterface\"] = []\n    \"\"\"\n  A dictionary of secondary analyzers that must be run before the current analyzer\n  secondary analyzer is run. These secondary analyzers must have the same\n  primary base.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface.base_analyzer","title":"<code>base_analyzer</code>  <code>instance-attribute</code>","text":"<p>The base analyzer that this secondary analyzer extends. This is always a primary analyzer. If your module depends on other secondary analyzers (which must have the same base analyzer), you can specify them in the <code>depends_on</code> field.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface.depends_on","title":"<code>depends_on = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of secondary analyzers that must be run before the current analyzer secondary analyzer is run. These secondary analyzers must have the same primary base.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.InputColumn","title":"<code>InputColumn</code>","text":"<p>               Bases: <code>Column</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class InputColumn(Column):\n    name_hints: list[str] = []\n    \"\"\"\n  Specifies a list of space-separated words that are likely to be found in the\n  column name of the user-provided data. This is used to help the user map the\n  input columns to the expected columns.\n\n  Any individual hint matching is sufficient for a match to be called. The hint\n  in turn is matched if every word matches some part of the column name.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.InputColumn.name_hints","title":"<code>name_hints = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specifies a list of space-separated words that are likely to be found in the column name of the user-provided data. This is used to help the user map the input columns to the expected columns.</p> <p>Any individual hint matching is sufficient for a match to be called. The hint in turn is matched if every word matches some part of the column name.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.SecondaryAnalyzerInterface","title":"<code>SecondaryAnalyzerInterface</code>","text":"<p>               Bases: <code>DerivedAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class SecondaryAnalyzerInterface(DerivedAnalyzerInterface):\n    outputs: list[AnalyzerOutput]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"secondary\"] = \"secondary\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.SecondaryAnalyzerInterface.outputs","title":"<code>outputs</code>  <code>instance-attribute</code>","text":"<p>Specifies the output data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.params","title":"<code>params</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.params.BooleanParam","title":"<code>BooleanParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a boolean value</p> <p>The corresponding value will be of type <code>bool</code>.</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class BooleanParam(BaseModel):\n    \"\"\"\n    Represents a boolean value\n\n    The corresponding value will be of type `bool`.\n    \"\"\"\n\n    type: Literal[\"boolean\"] = \"boolean\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.IntegerParam","title":"<code>IntegerParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an integer value</p> <p>The corresponding value will be of type <code>int</code>.</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class IntegerParam(BaseModel):\n    \"\"\"\n    Represents an integer value\n\n    The corresponding value will be of type `int`.\n    \"\"\"\n\n    type: Literal[\"integer\"] = \"integer\"\n    min: int\n    max: int\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningParam","title":"<code>TimeBinningParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a time bin.</p> <p>The corresponding value will be of type <code>TimeBinningValue</code>.</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningParam(BaseModel):\n    \"\"\"\n    Represents a time bin.\n\n    The corresponding value will be of type `TimeBinningValue`.\n    \"\"\"\n\n    type: Literal[\"time_binning\"] = \"time_binning\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningValue","title":"<code>TimeBinningValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningValue(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    unit: TimeBinningUnit\n    amount: int\n\n    def to_polars_truncate_spec(self) -&gt; str:\n        \"\"\"\n        Converts the value to a string that can be used in Polars truncate spec.\n        See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n        \"\"\"\n        amount = self.amount\n        unit = self.unit\n        if unit == \"year\":\n            return f\"{amount}y\"\n        if unit == \"month\":\n            return f\"{amount}mo\"\n        if unit == \"week\":\n            return f\"{amount}w\"\n        if unit == \"day\":\n            return f\"{amount}d\"\n        if unit == \"hour\":\n            return f\"{amount}h\"\n        if unit == \"minute\":\n            return f\"{amount}m\"\n        if unit == \"second\":\n            return f\"{amount}s\"\n\n        raise ValueError(\"Invalid time binning value\")\n\n    def to_human_readable_text(self) -&gt; str:\n        amount = self.amount\n        unit = self.unit\n\n        if unit == \"year\":\n            return f\"{amount} year{'s' if amount &gt; 1 else ''}\"\n        if unit == \"month\":\n            return f\"{amount} month{'s' if amount &gt; 1 else ''}\"\n        if unit == \"week\":\n            return f\"{amount} week{'s' if amount &gt; 1 else ''}\"\n        if unit == \"day\":\n            return f\"{amount} day{'s' if amount &gt; 1 else ''}\"\n        if unit == \"hour\":\n            return f\"{amount} hour{'s' if amount &gt; 1 else ''}\"\n        if unit == \"minute\":\n            return f\"{amount} minute{'s' if amount &gt; 1 else ''}\"\n        if unit == \"second\":\n            return f\"{amount} second{'s' if amount &gt; 1 else ''}\"\n\n        raise ValueError(\"Invalid time binning value\")\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningValue.to_polars_truncate_spec","title":"<code>to_polars_truncate_spec()</code>","text":"<p>Converts the value to a string that can be used in Polars truncate spec. See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>def to_polars_truncate_spec(self) -&gt; str:\n    \"\"\"\n    Converts the value to a string that can be used in Polars truncate spec.\n    See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n    \"\"\"\n    amount = self.amount\n    unit = self.unit\n    if unit == \"year\":\n        return f\"{amount}y\"\n    if unit == \"month\":\n        return f\"{amount}mo\"\n    if unit == \"week\":\n        return f\"{amount}w\"\n    if unit == \"day\":\n        return f\"{amount}d\"\n    if unit == \"hour\":\n        return f\"{amount}h\"\n    if unit == \"minute\":\n        return f\"{amount}m\"\n    if unit == \"second\":\n        return f\"{amount}s\"\n\n    raise ValueError(\"Invalid time binning value\")\n</code></pre>"},{"location":"reference/app/","title":"App","text":""},{"location":"reference/app/#app","title":"<code>app</code>","text":""},{"location":"reference/app/#app.logger","title":"<code>logger</code>","text":"<p>Application-wide logging system for Mango Tango CLI.</p> <p>Provides structured JSON logging with: - Console output (ERROR and CRITICAL levels only) to stderr - File output (INFO and above) with automatic rotation - Configurable log levels via CLI flag</p>"},{"location":"reference/app/#app.logger.ContextEnrichmentFilter","title":"<code>ContextEnrichmentFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter that enriches log records with contextual information.</p> <p>Adds: - process_id: Current process ID - thread_id: Current thread ID - app_version: Application version (if available)</p> Source code in <code>app/logger.py</code> <pre><code>class ContextEnrichmentFilter(logging.Filter):\n    \"\"\"\n    Filter that enriches log records with contextual information.\n\n    Adds:\n    - process_id: Current process ID\n    - thread_id: Current thread ID\n    - app_version: Application version (if available)\n    \"\"\"\n\n    def __init__(self, app_version: str = \"unknown\"):\n        super().__init__()\n        self.app_version = app_version\n        self.process_id = os.getpid()\n\n    def filter(self, record: logging.LogRecord) -&gt; bool:\n        # Add contextual information to the log record\n        record.process_id = self.process_id\n        record.thread_id = threading.get_ident()\n        record.app_version = self.app_version\n        return True\n</code></pre>"},{"location":"reference/app/#app.logger.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Get a logger instance for the specified module.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>app/logger.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger instance for the specified module.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n</code></pre>"},{"location":"reference/app/#app.logger.get_logger(name)","title":"<code>name</code>","text":""},{"location":"reference/app/#app.logger.setup_logging","title":"<code>setup_logging(log_file_path, level=logging.INFO, app_version='unknown')</code>","text":"<p>Configure application-wide logging with structured JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>Path to the log file</p> required <code>int</code> <p>Minimum logging level (default: logging.INFO)</p> <code>INFO</code> <code>str</code> <p>Application version to include in logs</p> <code>'unknown'</code> Source code in <code>app/logger.py</code> <pre><code>def setup_logging(\n    log_file_path: Path, level: int = logging.INFO, app_version: str = \"unknown\"\n) -&gt; None:\n    \"\"\"\n    Configure application-wide logging with structured JSON output.\n\n    Args:\n        log_file_path: Path to the log file\n        level: Minimum logging level (default: logging.INFO)\n        app_version: Application version to include in logs\n    \"\"\"\n    # Ensure the log directory exists\n    log_file_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Logging configuration dictionary\n    config: Dict[str, Any] = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\n            \"json\": {\n                \"()\": \"pythonjsonlogger.json.JsonFormatter\",\n                \"format\": \"%(asctime)s %(name)s %(levelname)s %(message)s %(process_id)s %(thread_id)s %(app_version)s\",\n                \"rename_fields\": {\"levelname\": \"level\", \"asctime\": \"timestamp\"},\n            }\n        },\n        \"filters\": {\n            \"context_enrichment\": {\n                \"()\": ContextEnrichmentFilter,\n                \"app_version\": app_version,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"level\": \"ERROR\",\n                \"formatter\": \"json\",\n                \"filters\": [\"context_enrichment\"],\n                \"stream\": sys.stderr,\n            },\n            \"file\": {\n                \"class\": \"logging.handlers.RotatingFileHandler\",\n                \"level\": level,\n                \"formatter\": \"json\",\n                \"filters\": [\"context_enrichment\"],\n                \"filename\": str(log_file_path),\n                \"maxBytes\": 10485760,  # 10MB\n                \"backupCount\": 5,\n                \"encoding\": \"utf-8\",\n            },\n        },\n        \"root\": {\"level\": level, \"handlers\": [\"console\", \"file\"]},\n        \"loggers\": {\n            # Third-party library loggers - keep them quieter by default\n            \"urllib3\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"requests\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"dash\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"plotly\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"shiny\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"uvicorn\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"starlette\": {\"level\": \"WARNING\", \"propagate\": True},\n            # Application loggers - inherit from root level\n            \"mangotango\": {\"level\": level, \"propagate\": True},\n            \"app\": {\"level\": level, \"propagate\": True},\n            \"analyzers\": {\"level\": level, \"propagate\": True},\n            \"components\": {\"level\": level, \"propagate\": True},\n            \"storage\": {\"level\": level, \"propagate\": True},\n            \"importing\": {\"level\": level, \"propagate\": True},\n        },\n    }\n\n    # Apply the configuration\n    logging.config.dictConfig(config)\n\n    # Set up global exception handler\n    def handle_exception(exc_type, exc_value, exc_traceback):\n        \"\"\"Handle uncaught exceptions by logging them.\"\"\"\n        if issubclass(exc_type, KeyboardInterrupt):\n            # Let KeyboardInterrupt be handled normally\n            sys.__excepthook__(exc_type, exc_value, exc_traceback)\n            return\n\n        logger = logging.getLogger(\"uncaught_exception\")\n        logger.critical(\n            \"Uncaught exception\",\n            exc_info=(exc_type, exc_value, exc_traceback),\n            extra={\n                \"exception_type\": exc_type.__name__,\n                \"exception_message\": str(exc_value),\n            },\n        )\n\n    # Install the global exception handler\n    sys.excepthook = handle_exception\n</code></pre>"},{"location":"reference/app/#app.logger.setup_logging(log_file_path)","title":"<code>log_file_path</code>","text":""},{"location":"reference/app/#app.logger.setup_logging(level)","title":"<code>level</code>","text":""},{"location":"reference/app/#app.logger.setup_logging(app_version)","title":"<code>app_version</code>","text":""},{"location":"reference/app/#app.test_logger","title":"<code>test_logger</code>","text":"<p>Unit tests for the logging configuration module.</p>"},{"location":"reference/app/#app.test_logger.TestContextEnrichment","title":"<code>TestContextEnrichment</code>","text":"<p>Test cases for context enrichment features.</p> Source code in <code>app/test_logger.py</code> <pre><code>class TestContextEnrichment:\n    \"\"\"Test cases for context enrichment features.\"\"\"\n\n    def test_context_filter_adds_metadata(self):\n        \"\"\"Test that context filter adds process_id, thread_id, and app_version.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            setup_logging(log_file_path, logging.INFO, \"context_test_version\")\n            logger = get_logger(\"context_test\")\n            logger.info(\"Test context enrichment\")\n\n            # Force flush\n            for handler in logging.getLogger().handlers:\n                handler.flush()\n\n            # Read and verify log contains enriched context\n            if log_file_path.exists():\n                log_content = log_file_path.read_text().strip()\n                if log_content:\n                    log_entry = json.loads(log_content)\n                    assert \"process_id\" in log_entry\n                    assert \"thread_id\" in log_entry\n                    assert log_entry[\"app_version\"] == \"context_test_version\"\n                    assert isinstance(log_entry[\"process_id\"], int)\n                    assert isinstance(log_entry[\"thread_id\"], int)\n\n    def test_third_party_logger_levels(self):\n        \"\"\"Test that third-party loggers are set to WARNING level.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            setup_logging(log_file_path, logging.DEBUG, \"hierarchy_test\")\n\n            # Test third-party loggers\n            urllib3_logger = logging.getLogger(\"urllib3\")\n            requests_logger = logging.getLogger(\"requests\")\n            dash_logger = logging.getLogger(\"dash\")\n\n            # They should be set to WARNING level\n            assert urllib3_logger.level == logging.WARNING\n            assert requests_logger.level == logging.WARNING\n            assert dash_logger.level == logging.WARNING\n\n            # Application loggers should inherit root level (DEBUG)\n            app_logger = logging.getLogger(\"app\")\n            assert app_logger.level == logging.DEBUG\n\n    def test_cli_level_controls_file_handler(self):\n        \"\"\"Test that CLI log level properly controls file handler level.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            # Set up with DEBUG level\n            setup_logging(log_file_path, logging.DEBUG, \"cli_test\")\n\n            root_logger = logging.getLogger()\n            file_handler = None\n\n            # Find the file handler\n            for handler in root_logger.handlers:\n                if hasattr(handler, \"baseFilename\"):\n                    file_handler = handler\n                    break\n\n            assert file_handler is not None\n            # File handler level should match the CLI level\n            assert file_handler.level == logging.DEBUG\n\n    def test_global_exception_handler_setup(self):\n        \"\"\"Test that global exception handler is installed.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            # Store original exception handler\n            original_excepthook = sys.excepthook\n\n            try:\n                setup_logging(log_file_path, logging.INFO, \"exception_test\")\n\n                # Exception handler should be modified\n                assert sys.excepthook != original_excepthook\n\n            finally:\n                # Restore original handler\n                sys.excepthook = original_excepthook\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestContextEnrichment.test_cli_level_controls_file_handler","title":"<code>test_cli_level_controls_file_handler()</code>","text":"<p>Test that CLI log level properly controls file handler level.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_cli_level_controls_file_handler(self):\n    \"\"\"Test that CLI log level properly controls file handler level.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        # Set up with DEBUG level\n        setup_logging(log_file_path, logging.DEBUG, \"cli_test\")\n\n        root_logger = logging.getLogger()\n        file_handler = None\n\n        # Find the file handler\n        for handler in root_logger.handlers:\n            if hasattr(handler, \"baseFilename\"):\n                file_handler = handler\n                break\n\n        assert file_handler is not None\n        # File handler level should match the CLI level\n        assert file_handler.level == logging.DEBUG\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestContextEnrichment.test_context_filter_adds_metadata","title":"<code>test_context_filter_adds_metadata()</code>","text":"<p>Test that context filter adds process_id, thread_id, and app_version.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_context_filter_adds_metadata(self):\n    \"\"\"Test that context filter adds process_id, thread_id, and app_version.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        setup_logging(log_file_path, logging.INFO, \"context_test_version\")\n        logger = get_logger(\"context_test\")\n        logger.info(\"Test context enrichment\")\n\n        # Force flush\n        for handler in logging.getLogger().handlers:\n            handler.flush()\n\n        # Read and verify log contains enriched context\n        if log_file_path.exists():\n            log_content = log_file_path.read_text().strip()\n            if log_content:\n                log_entry = json.loads(log_content)\n                assert \"process_id\" in log_entry\n                assert \"thread_id\" in log_entry\n                assert log_entry[\"app_version\"] == \"context_test_version\"\n                assert isinstance(log_entry[\"process_id\"], int)\n                assert isinstance(log_entry[\"thread_id\"], int)\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestContextEnrichment.test_global_exception_handler_setup","title":"<code>test_global_exception_handler_setup()</code>","text":"<p>Test that global exception handler is installed.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_global_exception_handler_setup(self):\n    \"\"\"Test that global exception handler is installed.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        # Store original exception handler\n        original_excepthook = sys.excepthook\n\n        try:\n            setup_logging(log_file_path, logging.INFO, \"exception_test\")\n\n            # Exception handler should be modified\n            assert sys.excepthook != original_excepthook\n\n        finally:\n            # Restore original handler\n            sys.excepthook = original_excepthook\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestContextEnrichment.test_third_party_logger_levels","title":"<code>test_third_party_logger_levels()</code>","text":"<p>Test that third-party loggers are set to WARNING level.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_third_party_logger_levels(self):\n    \"\"\"Test that third-party loggers are set to WARNING level.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        setup_logging(log_file_path, logging.DEBUG, \"hierarchy_test\")\n\n        # Test third-party loggers\n        urllib3_logger = logging.getLogger(\"urllib3\")\n        requests_logger = logging.getLogger(\"requests\")\n        dash_logger = logging.getLogger(\"dash\")\n\n        # They should be set to WARNING level\n        assert urllib3_logger.level == logging.WARNING\n        assert requests_logger.level == logging.WARNING\n        assert dash_logger.level == logging.WARNING\n\n        # Application loggers should inherit root level (DEBUG)\n        app_logger = logging.getLogger(\"app\")\n        assert app_logger.level == logging.DEBUG\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestGetLogger","title":"<code>TestGetLogger</code>","text":"<p>Test cases for the get_logger function.</p> Source code in <code>app/test_logger.py</code> <pre><code>class TestGetLogger:\n    \"\"\"Test cases for the get_logger function.\"\"\"\n\n    def test_get_logger_returns_logger_instance(self):\n        \"\"\"Test that get_logger returns a logging.Logger instance.\"\"\"\n        logger = get_logger(\"test\")\n        assert isinstance(logger, logging.Logger)\n\n    def test_get_logger_with_different_names(self):\n        \"\"\"Test that get_logger returns different loggers for different names.\"\"\"\n        logger1 = get_logger(\"test1\")\n        logger2 = get_logger(\"test2\")\n\n        assert logger1.name == \"test1\"\n        assert logger2.name == \"test2\"\n        assert logger1 is not logger2\n\n    def test_get_logger_with_same_name_returns_same_instance(self):\n        \"\"\"Test that get_logger returns the same instance for the same name.\"\"\"\n        logger1 = get_logger(\"test\")\n        logger2 = get_logger(\"test\")\n\n        assert logger1 is logger2\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestGetLogger.test_get_logger_returns_logger_instance","title":"<code>test_get_logger_returns_logger_instance()</code>","text":"<p>Test that get_logger returns a logging.Logger instance.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_get_logger_returns_logger_instance(self):\n    \"\"\"Test that get_logger returns a logging.Logger instance.\"\"\"\n    logger = get_logger(\"test\")\n    assert isinstance(logger, logging.Logger)\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestGetLogger.test_get_logger_with_different_names","title":"<code>test_get_logger_with_different_names()</code>","text":"<p>Test that get_logger returns different loggers for different names.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_get_logger_with_different_names(self):\n    \"\"\"Test that get_logger returns different loggers for different names.\"\"\"\n    logger1 = get_logger(\"test1\")\n    logger2 = get_logger(\"test2\")\n\n    assert logger1.name == \"test1\"\n    assert logger2.name == \"test2\"\n    assert logger1 is not logger2\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestGetLogger.test_get_logger_with_same_name_returns_same_instance","title":"<code>test_get_logger_with_same_name_returns_same_instance()</code>","text":"<p>Test that get_logger returns the same instance for the same name.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_get_logger_with_same_name_returns_same_instance(self):\n    \"\"\"Test that get_logger returns the same instance for the same name.\"\"\"\n    logger1 = get_logger(\"test\")\n    logger2 = get_logger(\"test\")\n\n    assert logger1 is logger2\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestIntegration","title":"<code>TestIntegration</code>","text":"<p>Integration tests for the logging system.</p> Source code in <code>app/test_logger.py</code> <pre><code>class TestIntegration:\n    \"\"\"Integration tests for the logging system.\"\"\"\n\n    def test_full_logging_workflow(self):\n        \"\"\"Test the complete logging workflow.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"integration_test.log\"\n\n            # Setup logging\n            setup_logging(log_file_path, logging.INFO, \"integration_test_version\")\n\n            # Get logger and log messages\n            logger = get_logger(\"integration_test\")\n            logger.info(\"Integration test started\")\n            logger.warning(\"This is a warning\")\n            logger.error(\"This is an error\")\n\n            # Force flush\n            for handler in logging.getLogger().handlers:\n                handler.flush()\n\n            # Verify log file exists and contains expected content\n            assert log_file_path.exists()\n            log_content = log_file_path.read_text()\n            assert \"Integration test started\" in log_content\n            assert \"This is a warning\" in log_content\n            assert \"This is an error\" in log_content\n\n            # Verify JSON format\n            for line in log_content.strip().split(\"\\n\"):\n                if line.strip():\n                    log_entry = json.loads(line)\n                    assert log_entry[\"name\"] == \"integration_test\"\n                    assert log_entry[\"level\"] in [\n                        \"INFO\",\n                        \"WARNING\",\n                        \"ERROR\",\n                    ]  # renamed field\n                    assert log_entry[\"app_version\"] == \"integration_test_version\"\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestIntegration.test_full_logging_workflow","title":"<code>test_full_logging_workflow()</code>","text":"<p>Test the complete logging workflow.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_full_logging_workflow(self):\n    \"\"\"Test the complete logging workflow.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"integration_test.log\"\n\n        # Setup logging\n        setup_logging(log_file_path, logging.INFO, \"integration_test_version\")\n\n        # Get logger and log messages\n        logger = get_logger(\"integration_test\")\n        logger.info(\"Integration test started\")\n        logger.warning(\"This is a warning\")\n        logger.error(\"This is an error\")\n\n        # Force flush\n        for handler in logging.getLogger().handlers:\n            handler.flush()\n\n        # Verify log file exists and contains expected content\n        assert log_file_path.exists()\n        log_content = log_file_path.read_text()\n        assert \"Integration test started\" in log_content\n        assert \"This is a warning\" in log_content\n        assert \"This is an error\" in log_content\n\n        # Verify JSON format\n        for line in log_content.strip().split(\"\\n\"):\n            if line.strip():\n                log_entry = json.loads(line)\n                assert log_entry[\"name\"] == \"integration_test\"\n                assert log_entry[\"level\"] in [\n                    \"INFO\",\n                    \"WARNING\",\n                    \"ERROR\",\n                ]  # renamed field\n                assert log_entry[\"app_version\"] == \"integration_test_version\"\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging","title":"<code>TestSetupLogging</code>","text":"<p>Test cases for the setup_logging function.</p> Source code in <code>app/test_logger.py</code> <pre><code>class TestSetupLogging:\n    \"\"\"Test cases for the setup_logging function.\"\"\"\n\n    def test_setup_logging_creates_log_directory(self):\n        \"\"\"Test that setup_logging creates the log directory if it doesn't exist.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"logs\" / \"test.log\"\n\n            # Directory shouldn't exist initially\n            assert not log_file_path.parent.exists()\n\n            setup_logging(log_file_path)\n\n            # Directory should be created\n            assert log_file_path.parent.exists()\n\n    def test_setup_logging_configures_root_logger(self):\n        \"\"\"Test that setup_logging configures the root logger with correct level.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            setup_logging(log_file_path, logging.DEBUG, \"test_version\")\n\n            root_logger = logging.getLogger()\n            assert root_logger.level == logging.DEBUG\n\n    def test_setup_logging_configures_handlers(self):\n        \"\"\"Test that setup_logging configures both console and file handlers.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n            root_logger = logging.getLogger()\n\n            # Should have 2 handlers (console and file)\n            assert len(root_logger.handlers) &gt;= 2\n\n            # Find console and file handlers\n            console_handler = None\n            file_handler = None\n\n            for handler in root_logger.handlers:\n                if (\n                    isinstance(handler, logging.StreamHandler)\n                    and handler.stream == sys.stderr\n                ):\n                    console_handler = handler\n                elif hasattr(handler, \"baseFilename\"):\n                    file_handler = handler\n\n            # Console handler should exist and be set to ERROR level\n            assert console_handler is not None\n            assert console_handler.level == logging.ERROR\n\n            # File handler should exist and be set to INFO level\n            assert file_handler is not None\n            assert file_handler.level == logging.INFO\n\n    def test_console_handler_only_shows_errors(self):\n        \"\"\"Test that console handler only shows ERROR and CRITICAL messages.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            # Use StringIO to capture stderr output\n            from io import StringIO\n\n            captured_stderr = StringIO()\n\n            # Temporarily replace sys.stderr before setting up logging\n            original_stderr = sys.stderr\n            sys.stderr = captured_stderr\n\n            try:\n                setup_logging(log_file_path, logging.DEBUG, \"test_version\")\n                logger = logging.getLogger(\"test\")\n\n                # Log messages at different levels\n                logger.debug(\"Debug message\")\n                logger.info(\"Info message\")\n                logger.warning(\"Warning message\")\n                logger.error(\"Error message\")\n                logger.critical(\"Critical message\")\n\n                # Force handlers to flush\n                for handler in logging.getLogger().handlers:\n                    handler.flush()\n\n                # Get captured output\n                stderr_output = captured_stderr.getvalue()\n\n                # Only ERROR and CRITICAL should appear on console\n                assert \"Error message\" in stderr_output\n                assert \"Critical message\" in stderr_output\n                assert \"Debug message\" not in stderr_output\n                assert \"Info message\" not in stderr_output\n                assert \"Warning message\" not in stderr_output\n\n            finally:\n                # Restore original stderr\n                sys.stderr = original_stderr\n\n    def test_file_handler_logs_info_and_above(self):\n        \"\"\"Test that file handler logs INFO and above messages when set to INFO level.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            # Set logging to INFO level (not DEBUG) to test INFO+ filtering\n            setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n            logger = logging.getLogger(\"test\")\n\n            # Log messages at different levels\n            logger.debug(\"Debug message\")\n            logger.info(\"Info message\")\n            logger.warning(\"Warning message\")\n            logger.error(\"Error message\")\n            logger.critical(\"Critical message\")\n\n            # Force handlers to flush\n            for handler in logging.getLogger().handlers:\n                handler.flush()\n\n            # Read log file content\n            if log_file_path.exists():\n                log_content = log_file_path.read_text()\n\n                # Should contain INFO, WARNING, ERROR, CRITICAL but not DEBUG\n                assert \"Info message\" in log_content\n                assert \"Warning message\" in log_content\n                assert \"Error message\" in log_content\n                assert \"Critical message\" in log_content\n                assert \"Debug message\" not in log_content\n\n    def test_log_format_is_json(self):\n        \"\"\"Test that log messages are formatted as JSON.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            log_file_path = Path(temp_dir) / \"test.log\"\n\n            setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n            logger = logging.getLogger(\"test\")\n            logger.info(\"Test JSON format\")\n\n            # Force handlers to flush\n            for handler in logging.getLogger().handlers:\n                handler.flush()\n\n            # Read log file and verify JSON format\n            if log_file_path.exists():\n                log_content = log_file_path.read_text().strip()\n                if log_content:\n                    # Each line should be valid JSON\n                    for line in log_content.split(\"\\n\"):\n                        if line.strip():\n                            try:\n                                log_entry = json.loads(line)\n                                assert \"timestamp\" in log_entry  # renamed from asctime\n                                assert \"name\" in log_entry\n                                assert \"level\" in log_entry  # renamed from levelname\n                                assert \"message\" in log_entry\n                                assert \"process_id\" in log_entry\n                                assert \"thread_id\" in log_entry\n                                assert \"app_version\" in log_entry\n                            except json.JSONDecodeError:\n                                pytest.fail(f\"Log line is not valid JSON: {line}\")\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_console_handler_only_shows_errors","title":"<code>test_console_handler_only_shows_errors()</code>","text":"<p>Test that console handler only shows ERROR and CRITICAL messages.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_console_handler_only_shows_errors(self):\n    \"\"\"Test that console handler only shows ERROR and CRITICAL messages.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        # Use StringIO to capture stderr output\n        from io import StringIO\n\n        captured_stderr = StringIO()\n\n        # Temporarily replace sys.stderr before setting up logging\n        original_stderr = sys.stderr\n        sys.stderr = captured_stderr\n\n        try:\n            setup_logging(log_file_path, logging.DEBUG, \"test_version\")\n            logger = logging.getLogger(\"test\")\n\n            # Log messages at different levels\n            logger.debug(\"Debug message\")\n            logger.info(\"Info message\")\n            logger.warning(\"Warning message\")\n            logger.error(\"Error message\")\n            logger.critical(\"Critical message\")\n\n            # Force handlers to flush\n            for handler in logging.getLogger().handlers:\n                handler.flush()\n\n            # Get captured output\n            stderr_output = captured_stderr.getvalue()\n\n            # Only ERROR and CRITICAL should appear on console\n            assert \"Error message\" in stderr_output\n            assert \"Critical message\" in stderr_output\n            assert \"Debug message\" not in stderr_output\n            assert \"Info message\" not in stderr_output\n            assert \"Warning message\" not in stderr_output\n\n        finally:\n            # Restore original stderr\n            sys.stderr = original_stderr\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_file_handler_logs_info_and_above","title":"<code>test_file_handler_logs_info_and_above()</code>","text":"<p>Test that file handler logs INFO and above messages when set to INFO level.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_file_handler_logs_info_and_above(self):\n    \"\"\"Test that file handler logs INFO and above messages when set to INFO level.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        # Set logging to INFO level (not DEBUG) to test INFO+ filtering\n        setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n        logger = logging.getLogger(\"test\")\n\n        # Log messages at different levels\n        logger.debug(\"Debug message\")\n        logger.info(\"Info message\")\n        logger.warning(\"Warning message\")\n        logger.error(\"Error message\")\n        logger.critical(\"Critical message\")\n\n        # Force handlers to flush\n        for handler in logging.getLogger().handlers:\n            handler.flush()\n\n        # Read log file content\n        if log_file_path.exists():\n            log_content = log_file_path.read_text()\n\n            # Should contain INFO, WARNING, ERROR, CRITICAL but not DEBUG\n            assert \"Info message\" in log_content\n            assert \"Warning message\" in log_content\n            assert \"Error message\" in log_content\n            assert \"Critical message\" in log_content\n            assert \"Debug message\" not in log_content\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_log_format_is_json","title":"<code>test_log_format_is_json()</code>","text":"<p>Test that log messages are formatted as JSON.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_log_format_is_json(self):\n    \"\"\"Test that log messages are formatted as JSON.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n        logger = logging.getLogger(\"test\")\n        logger.info(\"Test JSON format\")\n\n        # Force handlers to flush\n        for handler in logging.getLogger().handlers:\n            handler.flush()\n\n        # Read log file and verify JSON format\n        if log_file_path.exists():\n            log_content = log_file_path.read_text().strip()\n            if log_content:\n                # Each line should be valid JSON\n                for line in log_content.split(\"\\n\"):\n                    if line.strip():\n                        try:\n                            log_entry = json.loads(line)\n                            assert \"timestamp\" in log_entry  # renamed from asctime\n                            assert \"name\" in log_entry\n                            assert \"level\" in log_entry  # renamed from levelname\n                            assert \"message\" in log_entry\n                            assert \"process_id\" in log_entry\n                            assert \"thread_id\" in log_entry\n                            assert \"app_version\" in log_entry\n                        except json.JSONDecodeError:\n                            pytest.fail(f\"Log line is not valid JSON: {line}\")\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_setup_logging_configures_handlers","title":"<code>test_setup_logging_configures_handlers()</code>","text":"<p>Test that setup_logging configures both console and file handlers.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_setup_logging_configures_handlers(self):\n    \"\"\"Test that setup_logging configures both console and file handlers.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        setup_logging(log_file_path, logging.INFO, \"test_version\")\n\n        root_logger = logging.getLogger()\n\n        # Should have 2 handlers (console and file)\n        assert len(root_logger.handlers) &gt;= 2\n\n        # Find console and file handlers\n        console_handler = None\n        file_handler = None\n\n        for handler in root_logger.handlers:\n            if (\n                isinstance(handler, logging.StreamHandler)\n                and handler.stream == sys.stderr\n            ):\n                console_handler = handler\n            elif hasattr(handler, \"baseFilename\"):\n                file_handler = handler\n\n        # Console handler should exist and be set to ERROR level\n        assert console_handler is not None\n        assert console_handler.level == logging.ERROR\n\n        # File handler should exist and be set to INFO level\n        assert file_handler is not None\n        assert file_handler.level == logging.INFO\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_setup_logging_configures_root_logger","title":"<code>test_setup_logging_configures_root_logger()</code>","text":"<p>Test that setup_logging configures the root logger with correct level.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_setup_logging_configures_root_logger(self):\n    \"\"\"Test that setup_logging configures the root logger with correct level.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"test.log\"\n\n        setup_logging(log_file_path, logging.DEBUG, \"test_version\")\n\n        root_logger = logging.getLogger()\n        assert root_logger.level == logging.DEBUG\n</code></pre>"},{"location":"reference/app/#app.test_logger.TestSetupLogging.test_setup_logging_creates_log_directory","title":"<code>test_setup_logging_creates_log_directory()</code>","text":"<p>Test that setup_logging creates the log directory if it doesn't exist.</p> Source code in <code>app/test_logger.py</code> <pre><code>def test_setup_logging_creates_log_directory(self):\n    \"\"\"Test that setup_logging creates the log directory if it doesn't exist.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        log_file_path = Path(temp_dir) / \"logs\" / \"test.log\"\n\n        # Directory shouldn't exist initially\n        assert not log_file_path.parent.exists()\n\n        setup_logging(log_file_path)\n\n        # Directory should be created\n        assert log_file_path.parent.exists()\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress","title":"<code>test_memory_aware_progress</code>","text":"<p>Tests for ProgressManager memory monitoring functionality.</p>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures","title":"<code>TestProgressManagerMemoryFeatures</code>","text":"<p>Test ProgressManager memory monitoring functionality.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>class TestProgressManagerMemoryFeatures:\n    \"\"\"Test ProgressManager memory monitoring functionality.\"\"\"\n\n    def test_initialization_with_memory_manager(self):\n        \"\"\"Test ProgressManager initializes correctly with memory manager.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        progress_manager = ProgressManager(\n            \"Test Analysis\", memory_manager=memory_manager\n        )\n\n        assert progress_manager.memory_manager == memory_manager\n        assert progress_manager.last_memory_warning == 0\n        assert \"Test Analysis\" in progress_manager.title\n\n    def test_initialization_without_memory_manager(self):\n        \"\"\"Test ProgressManager initializes correctly without memory manager.\"\"\"\n        progress_manager = ProgressManager(\"Test Analysis\")\n\n        assert progress_manager.memory_manager is None\n        assert progress_manager.last_memory_warning is None\n        assert \"Test Analysis\" in progress_manager.title\n\n    def test_update_step_with_memory_low_pressure(self):\n        \"\"\"Test memory-aware step updates with low memory pressure.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\n            \"rss_mb\": 500.0,\n            \"process_memory_percent\": 12.5,\n            \"pressure_level\": \"low\",\n        }\n        memory_manager.should_trigger_gc.return_value = False\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        # Should update normally without warnings\n        progress_manager.update_step_with_memory(\"test_step\", 50, \"testing\")\n\n        # Verify memory stats were retrieved\n        memory_manager.get_current_memory_usage.assert_called_once()\n        memory_manager.should_trigger_gc.assert_called_once()\n\n        memory_manager.enhanced_gc_cleanup.assert_not_called()\n\n    def test_update_step_with_memory_high_pressure(self):\n        \"\"\"Test memory-aware step updates with high memory pressure.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\n            \"rss_mb\": 3000.0,\n            \"process_memory_percent\": 75.0,\n            \"pressure_level\": \"high\",\n        }\n        memory_manager.should_trigger_gc.return_value = True\n        memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 100.0}\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        with patch(\"rich.console.Console.print\"):\n            progress_manager.update_step_with_memory(\n                \"test_step\", 75, \"high pressure test\"\n            )\n\n        memory_manager.enhanced_gc_cleanup.assert_called_once()\n\n    def test_update_step_with_memory_critical_pressure(self):\n        \"\"\"Test memory-aware step updates with critical memory pressure.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\n            \"rss_mb\": 3500.0,\n            \"process_memory_percent\": 87.5,\n            \"pressure_level\": \"critical\",\n        }\n        memory_manager.should_trigger_gc.return_value = True\n        memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 200.0}\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        with patch.object(progress_manager, \"_display_memory_warning\") as mock_warning:\n            progress_manager.update_step_with_memory(\"test_step\", 90, \"critical test\")\n\n            mock_warning.assert_called_once()\n            call_args = mock_warning.call_args[0]\n            assert call_args[0] == MemoryPressureLevel.CRITICAL\n\n    def test_memory_warning_throttling(self):\n        \"\"\"Test that memory warnings are throttled to avoid spam.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\n            \"rss_mb\": 3000.0,\n            \"process_memory_percent\": 75.0,\n            \"pressure_level\": \"high\",\n        }\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            progress_manager._display_memory_warning(\n                MemoryPressureLevel.HIGH,\n                {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n                \"test context\",\n            )\n            first_call_count = mock_console_print.call_count\n\n            progress_manager._display_memory_warning(\n                MemoryPressureLevel.HIGH,\n                {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n                \"test context\",\n            )\n            second_call_count = mock_console_print.call_count\n\n            assert second_call_count == first_call_count\n\n    def test_memory_warning_throttling_timeout(self):\n        \"\"\"Test that memory warnings can be displayed again after timeout.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n        progress_manager.last_memory_warning = time.time() - 31\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            progress_manager._display_memory_warning(\n                MemoryPressureLevel.HIGH,\n                {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n                \"test context\",\n            )\n\n            mock_console_print.assert_called()\n\n    def test_display_memory_warning_content(self):\n        \"\"\"Test the content and formatting of memory warnings.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            # Test HIGH pressure warning\n            progress_manager._display_memory_warning(\n                MemoryPressureLevel.HIGH,\n                {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n                \"n-gram generation\",\n            )\n\n            mock_console_print.assert_called()\n            call_args = mock_console_print.call_args\n            assert (\n                call_args is not None\n            ), \"mock_console.print was not called with arguments\"\n            call_args = call_args[0]\n            panel = call_args[0]\n\n            assert panel.border_style == \"yellow\"\n            assert \"Current usage: 3000.0MB\" in str(panel.renderable)\n            assert \"n-gram generation\" in str(panel.renderable)\n            assert \"Memory Pressure: HIGH\" in str(panel.renderable)\n\n            mock_console_print.reset_mock()\n            progress_manager.last_memory_warning = None\n            progress_manager._display_memory_warning(\n                MemoryPressureLevel.CRITICAL,\n                {\"rss_mb\": 3500.0, \"process_memory_percent\": 87.5},\n                \"unique extraction\",\n            )\n\n            call_args = mock_console_print.call_args\n            assert (\n                call_args is not None\n            ), \"mock_console.print was not called with arguments\"\n            call_args = call_args[0]\n            panel = call_args[0]\n\n            assert panel.border_style == \"red\"\n            assert \"Memory Pressure: CRITICAL\" in str(panel.renderable)\n            assert \"unique extraction\" in str(panel.renderable)\n\n    def test_display_memory_summary(self):\n        \"\"\"Test memory summary display.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\n            \"rss_mb\": 2500.0,\n            \"peak_rss_mb\": 3000.0,\n            \"available_mb\": 1500.0,\n            \"pressure_level\": \"medium\",\n        }\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            progress_manager.display_memory_summary()\n\n            mock_console_print.assert_called()\n            call_args = mock_console_print.call_args\n            assert (\n                call_args is not None\n            ), \"mock_console.print was not called with arguments\"\n            call_args = call_args[0]\n            panel = call_args[0]\n\n            assert panel.border_style == \"green\"\n            assert \"Peak memory usage: 3000.0MB\" in str(panel.renderable)\n            assert \"Final memory usage: 2500.0MB\" in str(panel.renderable)\n            assert \"Available memory: 1500.0MB\" in str(panel.renderable)\n            assert \"Final pressure level: medium\" in str(panel.renderable)\n\n    def test_garbage_collection_reporting(self):\n        \"\"\"Test garbage collection effectiveness reporting.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\"pressure_level\": \"low\"}\n        memory_manager.should_trigger_gc.return_value = True\n        memory_manager.enhanced_gc_cleanup.return_value = {\n            \"memory_freed_mb\": 150.0  # Significant cleanup\n        }\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            progress_manager.update_step_with_memory(\"test_step\", 50, \"gc test\")\n\n            print_calls = [str(call) for call in mock_console_print.call_args_list]\n            assert any(\"Freed 150.0MB memory\" in call for call in print_calls)\n\n    def test_no_gc_reporting_for_small_cleanup(self):\n        \"\"\"Test that small GC cleanups are not reported to avoid noise.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n        memory_manager.get_current_memory_usage.return_value = {\"pressure_level\": \"low\"}\n        memory_manager.should_trigger_gc.return_value = True\n        memory_manager.enhanced_gc_cleanup.return_value = {\n            \"memory_freed_mb\": 10.0  # Small cleanup\n        }\n\n        progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n        progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n        with patch(\"rich.console.Console.print\") as mock_console_print:\n            progress_manager.update_step_with_memory(\"test_step\", 50, \"small gc test\")\n\n            print_calls = [str(call) for call in mock_console_print.call_args_list]\n            assert not any(\n                \"Freed\" in call and \"MB memory\" in call for call in print_calls\n            )\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_display_memory_summary","title":"<code>test_display_memory_summary()</code>","text":"<p>Test memory summary display.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_display_memory_summary(self):\n    \"\"\"Test memory summary display.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\n        \"rss_mb\": 2500.0,\n        \"peak_rss_mb\": 3000.0,\n        \"available_mb\": 1500.0,\n        \"pressure_level\": \"medium\",\n    }\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        progress_manager.display_memory_summary()\n\n        mock_console_print.assert_called()\n        call_args = mock_console_print.call_args\n        assert (\n            call_args is not None\n        ), \"mock_console.print was not called with arguments\"\n        call_args = call_args[0]\n        panel = call_args[0]\n\n        assert panel.border_style == \"green\"\n        assert \"Peak memory usage: 3000.0MB\" in str(panel.renderable)\n        assert \"Final memory usage: 2500.0MB\" in str(panel.renderable)\n        assert \"Available memory: 1500.0MB\" in str(panel.renderable)\n        assert \"Final pressure level: medium\" in str(panel.renderable)\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_display_memory_warning_content","title":"<code>test_display_memory_warning_content()</code>","text":"<p>Test the content and formatting of memory warnings.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_display_memory_warning_content(self):\n    \"\"\"Test the content and formatting of memory warnings.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        # Test HIGH pressure warning\n        progress_manager._display_memory_warning(\n            MemoryPressureLevel.HIGH,\n            {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n            \"n-gram generation\",\n        )\n\n        mock_console_print.assert_called()\n        call_args = mock_console_print.call_args\n        assert (\n            call_args is not None\n        ), \"mock_console.print was not called with arguments\"\n        call_args = call_args[0]\n        panel = call_args[0]\n\n        assert panel.border_style == \"yellow\"\n        assert \"Current usage: 3000.0MB\" in str(panel.renderable)\n        assert \"n-gram generation\" in str(panel.renderable)\n        assert \"Memory Pressure: HIGH\" in str(panel.renderable)\n\n        mock_console_print.reset_mock()\n        progress_manager.last_memory_warning = None\n        progress_manager._display_memory_warning(\n            MemoryPressureLevel.CRITICAL,\n            {\"rss_mb\": 3500.0, \"process_memory_percent\": 87.5},\n            \"unique extraction\",\n        )\n\n        call_args = mock_console_print.call_args\n        assert (\n            call_args is not None\n        ), \"mock_console.print was not called with arguments\"\n        call_args = call_args[0]\n        panel = call_args[0]\n\n        assert panel.border_style == \"red\"\n        assert \"Memory Pressure: CRITICAL\" in str(panel.renderable)\n        assert \"unique extraction\" in str(panel.renderable)\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_garbage_collection_reporting","title":"<code>test_garbage_collection_reporting()</code>","text":"<p>Test garbage collection effectiveness reporting.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_garbage_collection_reporting(self):\n    \"\"\"Test garbage collection effectiveness reporting.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\"pressure_level\": \"low\"}\n    memory_manager.should_trigger_gc.return_value = True\n    memory_manager.enhanced_gc_cleanup.return_value = {\n        \"memory_freed_mb\": 150.0  # Significant cleanup\n    }\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        progress_manager.update_step_with_memory(\"test_step\", 50, \"gc test\")\n\n        print_calls = [str(call) for call in mock_console_print.call_args_list]\n        assert any(\"Freed 150.0MB memory\" in call for call in print_calls)\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_initialization_with_memory_manager","title":"<code>test_initialization_with_memory_manager()</code>","text":"<p>Test ProgressManager initializes correctly with memory manager.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_initialization_with_memory_manager(self):\n    \"\"\"Test ProgressManager initializes correctly with memory manager.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    progress_manager = ProgressManager(\n        \"Test Analysis\", memory_manager=memory_manager\n    )\n\n    assert progress_manager.memory_manager == memory_manager\n    assert progress_manager.last_memory_warning == 0\n    assert \"Test Analysis\" in progress_manager.title\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_initialization_without_memory_manager","title":"<code>test_initialization_without_memory_manager()</code>","text":"<p>Test ProgressManager initializes correctly without memory manager.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_initialization_without_memory_manager(self):\n    \"\"\"Test ProgressManager initializes correctly without memory manager.\"\"\"\n    progress_manager = ProgressManager(\"Test Analysis\")\n\n    assert progress_manager.memory_manager is None\n    assert progress_manager.last_memory_warning is None\n    assert \"Test Analysis\" in progress_manager.title\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_memory_warning_throttling","title":"<code>test_memory_warning_throttling()</code>","text":"<p>Test that memory warnings are throttled to avoid spam.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_memory_warning_throttling(self):\n    \"\"\"Test that memory warnings are throttled to avoid spam.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\n        \"rss_mb\": 3000.0,\n        \"process_memory_percent\": 75.0,\n        \"pressure_level\": \"high\",\n    }\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        progress_manager._display_memory_warning(\n            MemoryPressureLevel.HIGH,\n            {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n            \"test context\",\n        )\n        first_call_count = mock_console_print.call_count\n\n        progress_manager._display_memory_warning(\n            MemoryPressureLevel.HIGH,\n            {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n            \"test context\",\n        )\n        second_call_count = mock_console_print.call_count\n\n        assert second_call_count == first_call_count\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_memory_warning_throttling_timeout","title":"<code>test_memory_warning_throttling_timeout()</code>","text":"<p>Test that memory warnings can be displayed again after timeout.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_memory_warning_throttling_timeout(self):\n    \"\"\"Test that memory warnings can be displayed again after timeout.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n\n    progress_manager.last_memory_warning = time.time() - 31\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        progress_manager._display_memory_warning(\n            MemoryPressureLevel.HIGH,\n            {\"rss_mb\": 3000.0, \"process_memory_percent\": 75.0},\n            \"test context\",\n        )\n\n        mock_console_print.assert_called()\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_no_gc_reporting_for_small_cleanup","title":"<code>test_no_gc_reporting_for_small_cleanup()</code>","text":"<p>Test that small GC cleanups are not reported to avoid noise.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_no_gc_reporting_for_small_cleanup(self):\n    \"\"\"Test that small GC cleanups are not reported to avoid noise.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\"pressure_level\": \"low\"}\n    memory_manager.should_trigger_gc.return_value = True\n    memory_manager.enhanced_gc_cleanup.return_value = {\n        \"memory_freed_mb\": 10.0  # Small cleanup\n    }\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    with patch(\"rich.console.Console.print\") as mock_console_print:\n        progress_manager.update_step_with_memory(\"test_step\", 50, \"small gc test\")\n\n        print_calls = [str(call) for call in mock_console_print.call_args_list]\n        assert not any(\n            \"Freed\" in call and \"MB memory\" in call for call in print_calls\n        )\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_update_step_with_memory_critical_pressure","title":"<code>test_update_step_with_memory_critical_pressure()</code>","text":"<p>Test memory-aware step updates with critical memory pressure.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_update_step_with_memory_critical_pressure(self):\n    \"\"\"Test memory-aware step updates with critical memory pressure.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\n        \"rss_mb\": 3500.0,\n        \"process_memory_percent\": 87.5,\n        \"pressure_level\": \"critical\",\n    }\n    memory_manager.should_trigger_gc.return_value = True\n    memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 200.0}\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    with patch.object(progress_manager, \"_display_memory_warning\") as mock_warning:\n        progress_manager.update_step_with_memory(\"test_step\", 90, \"critical test\")\n\n        mock_warning.assert_called_once()\n        call_args = mock_warning.call_args[0]\n        assert call_args[0] == MemoryPressureLevel.CRITICAL\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_update_step_with_memory_high_pressure","title":"<code>test_update_step_with_memory_high_pressure()</code>","text":"<p>Test memory-aware step updates with high memory pressure.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_update_step_with_memory_high_pressure(self):\n    \"\"\"Test memory-aware step updates with high memory pressure.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\n        \"rss_mb\": 3000.0,\n        \"process_memory_percent\": 75.0,\n        \"pressure_level\": \"high\",\n    }\n    memory_manager.should_trigger_gc.return_value = True\n    memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 100.0}\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    with patch(\"rich.console.Console.print\"):\n        progress_manager.update_step_with_memory(\n            \"test_step\", 75, \"high pressure test\"\n        )\n\n    memory_manager.enhanced_gc_cleanup.assert_called_once()\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryFeatures.test_update_step_with_memory_low_pressure","title":"<code>test_update_step_with_memory_low_pressure()</code>","text":"<p>Test memory-aware step updates with low memory pressure.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_update_step_with_memory_low_pressure(self):\n    \"\"\"Test memory-aware step updates with low memory pressure.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n    memory_manager.get_current_memory_usage.return_value = {\n        \"rss_mb\": 500.0,\n        \"process_memory_percent\": 12.5,\n        \"pressure_level\": \"low\",\n    }\n    memory_manager.should_trigger_gc.return_value = False\n\n    progress_manager = ProgressManager(\"Test\", memory_manager=memory_manager)\n    progress_manager.add_step(\"test_step\", \"Testing\", 100)\n\n    # Should update normally without warnings\n    progress_manager.update_step_with_memory(\"test_step\", 50, \"testing\")\n\n    # Verify memory stats were retrieved\n    memory_manager.get_current_memory_usage.assert_called_once()\n    memory_manager.should_trigger_gc.assert_called_once()\n\n    memory_manager.enhanced_gc_cleanup.assert_not_called()\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryIntegration","title":"<code>TestProgressManagerMemoryIntegration</code>","text":"<p>Integration tests for ProgressManager memory features.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>class TestProgressManagerMemoryIntegration:\n    \"\"\"Integration tests for ProgressManager memory features.\"\"\"\n\n    def test_full_analysis_simulation(self):\n        \"\"\"Simulate a full analysis workflow with memory monitoring.\"\"\"\n        memory_manager = MagicMock(spec=MemoryManager)\n\n        # Simulate increasing memory pressure during analysis\n        memory_states = [\n            {\"rss_mb\": 500.0, \"process_memory_percent\": 12.5, \"pressure_level\": \"low\"},\n            {\"rss_mb\": 1500.0, \"process_memory_percent\": 37.5, \"pressure_level\": \"low\"},\n            {\n                \"rss_mb\": 2500.0,\n                \"process_memory_percent\": 62.5,\n                \"pressure_level\": \"medium\",\n            },\n            {\n                \"rss_mb\": 3200.0,\n                \"process_memory_percent\": 80.0,\n                \"pressure_level\": \"high\",\n            },\n            {\n                \"rss_mb\": 2800.0,\n                \"process_memory_percent\": 70.0,\n                \"pressure_level\": \"medium\",\n            },  # After cleanup\n        ]\n\n        memory_manager.get_current_memory_usage.side_effect = memory_states + [\n            {\n                \"rss_mb\": 2800.0,\n                \"process_memory_percent\": 70.0,\n                \"pressure_level\": \"medium\",\n            }\n        ]\n        memory_manager.should_trigger_gc.side_effect = [\n            False,\n            False,\n            False,\n            True,\n            False,\n        ]\n        memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 400.0}\n\n        progress_manager = ProgressManager(\n            \"Simulated Analysis\", memory_manager=memory_manager\n        )\n\n        steps = [\"preprocess\", \"tokenize\", \"ngrams\", \"extract_unique\", \"write_output\"]\n        for step in steps:\n            progress_manager.add_step(step, f\"Processing {step}\", 100)\n\n        with patch(\"rich.console.Console.print\"):\n            for i, step in enumerate(steps):\n                progress_manager.start_step(step)\n                progress_manager.update_step_with_memory(step, 50, f\"{step} processing\")\n                progress_manager.complete_step(step)\n\n            progress_manager.display_memory_summary()\n\n        assert memory_manager.get_current_memory_usage.call_count == len(steps) + 1\n        assert memory_manager.should_trigger_gc.call_count == len(steps)\n        assert memory_manager.enhanced_gc_cleanup.call_count == 1\n</code></pre>"},{"location":"reference/app/#app.test_memory_aware_progress.TestProgressManagerMemoryIntegration.test_full_analysis_simulation","title":"<code>test_full_analysis_simulation()</code>","text":"<p>Simulate a full analysis workflow with memory monitoring.</p> Source code in <code>app/test_memory_aware_progress.py</code> <pre><code>def test_full_analysis_simulation(self):\n    \"\"\"Simulate a full analysis workflow with memory monitoring.\"\"\"\n    memory_manager = MagicMock(spec=MemoryManager)\n\n    # Simulate increasing memory pressure during analysis\n    memory_states = [\n        {\"rss_mb\": 500.0, \"process_memory_percent\": 12.5, \"pressure_level\": \"low\"},\n        {\"rss_mb\": 1500.0, \"process_memory_percent\": 37.5, \"pressure_level\": \"low\"},\n        {\n            \"rss_mb\": 2500.0,\n            \"process_memory_percent\": 62.5,\n            \"pressure_level\": \"medium\",\n        },\n        {\n            \"rss_mb\": 3200.0,\n            \"process_memory_percent\": 80.0,\n            \"pressure_level\": \"high\",\n        },\n        {\n            \"rss_mb\": 2800.0,\n            \"process_memory_percent\": 70.0,\n            \"pressure_level\": \"medium\",\n        },  # After cleanup\n    ]\n\n    memory_manager.get_current_memory_usage.side_effect = memory_states + [\n        {\n            \"rss_mb\": 2800.0,\n            \"process_memory_percent\": 70.0,\n            \"pressure_level\": \"medium\",\n        }\n    ]\n    memory_manager.should_trigger_gc.side_effect = [\n        False,\n        False,\n        False,\n        True,\n        False,\n    ]\n    memory_manager.enhanced_gc_cleanup.return_value = {\"memory_freed_mb\": 400.0}\n\n    progress_manager = ProgressManager(\n        \"Simulated Analysis\", memory_manager=memory_manager\n    )\n\n    steps = [\"preprocess\", \"tokenize\", \"ngrams\", \"extract_unique\", \"write_output\"]\n    for step in steps:\n        progress_manager.add_step(step, f\"Processing {step}\", 100)\n\n    with patch(\"rich.console.Console.print\"):\n        for i, step in enumerate(steps):\n            progress_manager.start_step(step)\n            progress_manager.update_step_with_memory(step, 50, f\"{step} processing\")\n            progress_manager.complete_step(step)\n\n        progress_manager.display_memory_summary()\n\n    assert memory_manager.get_current_memory_usage.call_count == len(steps) + 1\n    assert memory_manager.should_trigger_gc.call_count == len(steps)\n    assert memory_manager.enhanced_gc_cleanup.call_count == 1\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager","title":"<code>test_memory_manager</code>","text":"<p>Comprehensive tests for the MemoryManager class and memory-aware processing.</p>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager","title":"<code>TestMemoryManager</code>","text":"<p>Test core MemoryManager functionality.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>class TestMemoryManager:\n    \"\"\"Test core MemoryManager functionality.\"\"\"\n\n    def test_memory_manager_initialization(self):\n        \"\"\"Test MemoryManager initializes correctly.\"\"\"\n        manager = MemoryManager(max_memory_gb=2.0, process_name=\"test\")\n\n        assert manager.max_memory_bytes == 2.0 * 1024**3\n        assert manager.process_name == \"test\"\n        assert len(manager.thresholds) == 3\n        assert len(manager.chunk_size_factors) == 4\n        assert manager.memory_history == []\n\n    def test_get_current_memory_usage(self):\n        \"\"\"Test memory usage statistics collection.\"\"\"\n        manager = MemoryManager()\n        stats = manager.get_current_memory_usage()\n\n        # Check all required fields are present\n        required_fields = [\n            \"rss_bytes\",\n            \"vms_bytes\",\n            \"rss_mb\",\n            \"vms_mb\",\n            \"rss_gb\",\n            \"system_available_gb\",\n            \"system_used_percent\",\n            \"process_memory_percent\",\n            \"pressure_level\",\n        ]\n\n        for field in required_fields:\n            assert field in stats\n            assert isinstance(stats[field], (int, float, str))\n\n        # Check memory history is updated\n        assert len(manager.memory_history) == 1\n        assert \"timestamp\" in manager.memory_history[0]\n        assert \"rss_bytes\" in manager.memory_history[0]\n\n    def test_memory_pressure_levels(self):\n        \"\"\"Test memory pressure level detection.\"\"\"\n        manager = MemoryManager(max_memory_gb=1.0)  # Small limit for testing\n\n        # Mock different memory usage levels\n        with patch.object(manager.process, \"memory_info\") as mock_memory:\n            # Test LOW pressure (40% usage)\n            mock_memory.return_value.rss = int(manager.max_memory_bytes * 40 // 100)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.LOW\n\n            # Test MEDIUM pressure (75% usage - safely above 70% threshold)\n            mock_memory.return_value.rss = int(manager.max_memory_bytes * 75 // 100)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.MEDIUM\n\n            # Test HIGH pressure (85% usage - safely above 80% threshold)\n            mock_memory.return_value.rss = int(manager.max_memory_bytes * 85 // 100)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.HIGH\n\n            # Test CRITICAL pressure (95% usage - safely above 90% threshold)\n            mock_memory.return_value.rss = int(manager.max_memory_bytes * 95 // 100)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.CRITICAL\n\n    def test_adaptive_chunk_sizing(self):\n        \"\"\"Test adaptive chunk size calculation based on memory pressure.\"\"\"\n        manager = MemoryManager()\n        base_size = 10000\n\n        with patch(\n            \"app.utils.MemoryManager.get_memory_pressure_level\"\n        ) as mock_pressure:\n            # Test LOW pressure - no reduction\n            mock_pressure.return_value = MemoryPressureLevel.LOW\n            size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n            assert size == base_size\n\n            # Test MEDIUM pressure - 30% reduction\n            mock_pressure.return_value = MemoryPressureLevel.MEDIUM\n            size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n            assert size == int(base_size * 0.8)\n\n            # Test HIGH pressure - 60% reduction\n            mock_pressure.return_value = MemoryPressureLevel.HIGH\n            size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n            assert size == int(base_size * 0.6)\n\n            # Test CRITICAL pressure - 80% reduction\n            mock_pressure.return_value = MemoryPressureLevel.CRITICAL\n            size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n            assert size == int(base_size * 0.4)\n\n    def test_operation_specific_chunk_sizing(self):\n        \"\"\"Test operation-specific chunk size adjustments.\"\"\"\n        manager = MemoryManager()\n        base_size = 10000\n\n        with patch(\n            \"app.utils.MemoryManager.get_memory_pressure_level\"\n        ) as mock_pressure:\n            mock_pressure.return_value = MemoryPressureLevel.LOW\n\n            # Test different operation types\n            tokenization_size = manager.calculate_adaptive_chunk_size(\n                base_size, \"tokenization\"\n            )\n            ngram_size = manager.calculate_adaptive_chunk_size(\n                base_size, \"ngram_generation\"\n            )\n            unique_size = manager.calculate_adaptive_chunk_size(\n                base_size, \"unique_extraction\"\n            )\n\n            # N-gram generation should be smaller (more memory intensive)\n            assert ngram_size &lt; tokenization_size\n            # Unique extraction should be larger (less memory intensive)\n            assert unique_size &gt; tokenization_size\n\n    def test_minimum_chunk_size_enforcement(self):\n        \"\"\"Test that minimum chunk size is enforced.\"\"\"\n        manager = MemoryManager()\n        small_base = 5000\n\n        with patch(\n            \"app.utils.MemoryManager.get_memory_pressure_level\"\n        ) as mock_pressure:\n            mock_pressure.return_value = MemoryPressureLevel.CRITICAL\n\n            size = manager.calculate_adaptive_chunk_size(small_base, \"ngram_generation\")\n\n            # Should not go below minimum (max of 1000 or base_size // 10)\n            expected_min = max(1000, small_base // 10)\n            assert size &gt;= expected_min\n\n    def test_gc_trigger_threshold(self):\n        \"\"\"Test garbage collection trigger logic.\"\"\"\n        manager = MemoryManager(max_memory_gb=1.0)\n\n        with patch.object(manager.process, \"memory_info\") as mock_memory:\n            # Below threshold - should not trigger\n            mock_memory.return_value.rss = int(0.6 * manager.max_memory_bytes)\n            assert not manager.should_trigger_gc()\n\n            # Above threshold - should trigger\n            mock_memory.return_value.rss = int(0.8 * manager.max_memory_bytes)\n            assert manager.should_trigger_gc()\n\n    def test_enhanced_gc_cleanup(self):\n        \"\"\"Test enhanced garbage collection functionality.\"\"\"\n        manager = MemoryManager()\n\n        with patch(\"app.utils.MemoryManager.get_current_memory_usage\") as mock_usage:\n            # Mock memory before and after cleanup\n            mock_usage.side_effect = [\n                {\"rss_mb\": 1000, \"pressure_level\": \"high\"},  # Before\n                {\"rss_mb\": 800, \"pressure_level\": \"medium\"},  # After\n            ]\n\n            with patch(\"gc.collect\") as mock_gc:\n                mock_gc.return_value = 50  # Some objects collected\n\n                stats = manager.enhanced_gc_cleanup()\n\n                assert \"memory_freed_mb\" in stats\n                assert \"memory_before_mb\" in stats\n                assert \"memory_after_mb\" in stats\n                assert \"pressure_before\" in stats\n                assert \"pressure_after\" in stats\n\n                assert stats[\"memory_freed_mb\"] == 200  # 1000 - 800\n                assert mock_gc.call_count &gt;= 1\n\n    def test_memory_trend_analysis(self):\n        \"\"\"Test memory usage trend analysis.\"\"\"\n        manager = MemoryManager()\n\n        # Not enough data\n        assert manager.get_memory_trend() == \"insufficient_data\"\n\n        # Add some increasing memory usage data\n        for i in range(5):\n            manager.memory_history.append(\n                {\n                    \"timestamp\": time.time(),\n                    \"rss_bytes\": 1000 + (i * 100),  # Increasing\n                    \"pressure_level\": \"low\",\n                }\n            )\n\n        assert manager.get_memory_trend() == \"increasing\"\n\n        # Add decreasing data\n        manager.memory_history.clear()\n        for i in range(5):\n            manager.memory_history.append(\n                {\n                    \"timestamp\": time.time(),\n                    \"rss_bytes\": 1500 - (i * 100),  # Decreasing\n                    \"pressure_level\": \"low\",\n                }\n            )\n\n        assert manager.get_memory_trend() == \"decreasing\"\n\n        # Add stable data\n        manager.memory_history.clear()\n        for i in range(5):\n            manager.memory_history.append(\n                {\n                    \"timestamp\": time.time(),\n                    \"rss_bytes\": 1000 + (i % 2 * 50),  # Fluctuating\n                    \"pressure_level\": \"low\",\n                }\n            )\n\n        assert manager.get_memory_trend() == \"stable\"\n\n    def test_memory_history_size_limit(self):\n        \"\"\"Test memory history size is properly limited.\"\"\"\n        manager = MemoryManager()\n        manager.max_history_size = 5  # Small limit for testing\n\n        # Add more entries than the limit\n        for i in range(10):\n            manager.get_current_memory_usage()\n\n        # Should not exceed the limit\n        assert len(manager.memory_history) &lt;= manager.max_history_size\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_adaptive_chunk_sizing","title":"<code>test_adaptive_chunk_sizing()</code>","text":"<p>Test adaptive chunk size calculation based on memory pressure.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_adaptive_chunk_sizing(self):\n    \"\"\"Test adaptive chunk size calculation based on memory pressure.\"\"\"\n    manager = MemoryManager()\n    base_size = 10000\n\n    with patch(\n        \"app.utils.MemoryManager.get_memory_pressure_level\"\n    ) as mock_pressure:\n        # Test LOW pressure - no reduction\n        mock_pressure.return_value = MemoryPressureLevel.LOW\n        size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n        assert size == base_size\n\n        # Test MEDIUM pressure - 30% reduction\n        mock_pressure.return_value = MemoryPressureLevel.MEDIUM\n        size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n        assert size == int(base_size * 0.8)\n\n        # Test HIGH pressure - 60% reduction\n        mock_pressure.return_value = MemoryPressureLevel.HIGH\n        size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n        assert size == int(base_size * 0.6)\n\n        # Test CRITICAL pressure - 80% reduction\n        mock_pressure.return_value = MemoryPressureLevel.CRITICAL\n        size = manager.calculate_adaptive_chunk_size(base_size, \"tokenization\")\n        assert size == int(base_size * 0.4)\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_enhanced_gc_cleanup","title":"<code>test_enhanced_gc_cleanup()</code>","text":"<p>Test enhanced garbage collection functionality.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_enhanced_gc_cleanup(self):\n    \"\"\"Test enhanced garbage collection functionality.\"\"\"\n    manager = MemoryManager()\n\n    with patch(\"app.utils.MemoryManager.get_current_memory_usage\") as mock_usage:\n        # Mock memory before and after cleanup\n        mock_usage.side_effect = [\n            {\"rss_mb\": 1000, \"pressure_level\": \"high\"},  # Before\n            {\"rss_mb\": 800, \"pressure_level\": \"medium\"},  # After\n        ]\n\n        with patch(\"gc.collect\") as mock_gc:\n            mock_gc.return_value = 50  # Some objects collected\n\n            stats = manager.enhanced_gc_cleanup()\n\n            assert \"memory_freed_mb\" in stats\n            assert \"memory_before_mb\" in stats\n            assert \"memory_after_mb\" in stats\n            assert \"pressure_before\" in stats\n            assert \"pressure_after\" in stats\n\n            assert stats[\"memory_freed_mb\"] == 200  # 1000 - 800\n            assert mock_gc.call_count &gt;= 1\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_gc_trigger_threshold","title":"<code>test_gc_trigger_threshold()</code>","text":"<p>Test garbage collection trigger logic.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_gc_trigger_threshold(self):\n    \"\"\"Test garbage collection trigger logic.\"\"\"\n    manager = MemoryManager(max_memory_gb=1.0)\n\n    with patch.object(manager.process, \"memory_info\") as mock_memory:\n        # Below threshold - should not trigger\n        mock_memory.return_value.rss = int(0.6 * manager.max_memory_bytes)\n        assert not manager.should_trigger_gc()\n\n        # Above threshold - should trigger\n        mock_memory.return_value.rss = int(0.8 * manager.max_memory_bytes)\n        assert manager.should_trigger_gc()\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_get_current_memory_usage","title":"<code>test_get_current_memory_usage()</code>","text":"<p>Test memory usage statistics collection.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_get_current_memory_usage(self):\n    \"\"\"Test memory usage statistics collection.\"\"\"\n    manager = MemoryManager()\n    stats = manager.get_current_memory_usage()\n\n    # Check all required fields are present\n    required_fields = [\n        \"rss_bytes\",\n        \"vms_bytes\",\n        \"rss_mb\",\n        \"vms_mb\",\n        \"rss_gb\",\n        \"system_available_gb\",\n        \"system_used_percent\",\n        \"process_memory_percent\",\n        \"pressure_level\",\n    ]\n\n    for field in required_fields:\n        assert field in stats\n        assert isinstance(stats[field], (int, float, str))\n\n    # Check memory history is updated\n    assert len(manager.memory_history) == 1\n    assert \"timestamp\" in manager.memory_history[0]\n    assert \"rss_bytes\" in manager.memory_history[0]\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_memory_history_size_limit","title":"<code>test_memory_history_size_limit()</code>","text":"<p>Test memory history size is properly limited.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_memory_history_size_limit(self):\n    \"\"\"Test memory history size is properly limited.\"\"\"\n    manager = MemoryManager()\n    manager.max_history_size = 5  # Small limit for testing\n\n    # Add more entries than the limit\n    for i in range(10):\n        manager.get_current_memory_usage()\n\n    # Should not exceed the limit\n    assert len(manager.memory_history) &lt;= manager.max_history_size\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_memory_manager_initialization","title":"<code>test_memory_manager_initialization()</code>","text":"<p>Test MemoryManager initializes correctly.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_memory_manager_initialization(self):\n    \"\"\"Test MemoryManager initializes correctly.\"\"\"\n    manager = MemoryManager(max_memory_gb=2.0, process_name=\"test\")\n\n    assert manager.max_memory_bytes == 2.0 * 1024**3\n    assert manager.process_name == \"test\"\n    assert len(manager.thresholds) == 3\n    assert len(manager.chunk_size_factors) == 4\n    assert manager.memory_history == []\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_memory_pressure_levels","title":"<code>test_memory_pressure_levels()</code>","text":"<p>Test memory pressure level detection.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_memory_pressure_levels(self):\n    \"\"\"Test memory pressure level detection.\"\"\"\n    manager = MemoryManager(max_memory_gb=1.0)  # Small limit for testing\n\n    # Mock different memory usage levels\n    with patch.object(manager.process, \"memory_info\") as mock_memory:\n        # Test LOW pressure (40% usage)\n        mock_memory.return_value.rss = int(manager.max_memory_bytes * 40 // 100)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.LOW\n\n        # Test MEDIUM pressure (75% usage - safely above 70% threshold)\n        mock_memory.return_value.rss = int(manager.max_memory_bytes * 75 // 100)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.MEDIUM\n\n        # Test HIGH pressure (85% usage - safely above 80% threshold)\n        mock_memory.return_value.rss = int(manager.max_memory_bytes * 85 // 100)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.HIGH\n\n        # Test CRITICAL pressure (95% usage - safely above 90% threshold)\n        mock_memory.return_value.rss = int(manager.max_memory_bytes * 95 // 100)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.CRITICAL\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_memory_trend_analysis","title":"<code>test_memory_trend_analysis()</code>","text":"<p>Test memory usage trend analysis.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_memory_trend_analysis(self):\n    \"\"\"Test memory usage trend analysis.\"\"\"\n    manager = MemoryManager()\n\n    # Not enough data\n    assert manager.get_memory_trend() == \"insufficient_data\"\n\n    # Add some increasing memory usage data\n    for i in range(5):\n        manager.memory_history.append(\n            {\n                \"timestamp\": time.time(),\n                \"rss_bytes\": 1000 + (i * 100),  # Increasing\n                \"pressure_level\": \"low\",\n            }\n        )\n\n    assert manager.get_memory_trend() == \"increasing\"\n\n    # Add decreasing data\n    manager.memory_history.clear()\n    for i in range(5):\n        manager.memory_history.append(\n            {\n                \"timestamp\": time.time(),\n                \"rss_bytes\": 1500 - (i * 100),  # Decreasing\n                \"pressure_level\": \"low\",\n            }\n        )\n\n    assert manager.get_memory_trend() == \"decreasing\"\n\n    # Add stable data\n    manager.memory_history.clear()\n    for i in range(5):\n        manager.memory_history.append(\n            {\n                \"timestamp\": time.time(),\n                \"rss_bytes\": 1000 + (i % 2 * 50),  # Fluctuating\n                \"pressure_level\": \"low\",\n            }\n        )\n\n    assert manager.get_memory_trend() == \"stable\"\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_minimum_chunk_size_enforcement","title":"<code>test_minimum_chunk_size_enforcement()</code>","text":"<p>Test that minimum chunk size is enforced.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_minimum_chunk_size_enforcement(self):\n    \"\"\"Test that minimum chunk size is enforced.\"\"\"\n    manager = MemoryManager()\n    small_base = 5000\n\n    with patch(\n        \"app.utils.MemoryManager.get_memory_pressure_level\"\n    ) as mock_pressure:\n        mock_pressure.return_value = MemoryPressureLevel.CRITICAL\n\n        size = manager.calculate_adaptive_chunk_size(small_base, \"ngram_generation\")\n\n        # Should not go below minimum (max of 1000 or base_size // 10)\n        expected_min = max(1000, small_base // 10)\n        assert size &gt;= expected_min\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManager.test_operation_specific_chunk_sizing","title":"<code>test_operation_specific_chunk_sizing()</code>","text":"<p>Test operation-specific chunk size adjustments.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_operation_specific_chunk_sizing(self):\n    \"\"\"Test operation-specific chunk size adjustments.\"\"\"\n    manager = MemoryManager()\n    base_size = 10000\n\n    with patch(\n        \"app.utils.MemoryManager.get_memory_pressure_level\"\n    ) as mock_pressure:\n        mock_pressure.return_value = MemoryPressureLevel.LOW\n\n        # Test different operation types\n        tokenization_size = manager.calculate_adaptive_chunk_size(\n            base_size, \"tokenization\"\n        )\n        ngram_size = manager.calculate_adaptive_chunk_size(\n            base_size, \"ngram_generation\"\n        )\n        unique_size = manager.calculate_adaptive_chunk_size(\n            base_size, \"unique_extraction\"\n        )\n\n        # N-gram generation should be smaller (more memory intensive)\n        assert ngram_size &lt; tokenization_size\n        # Unique extraction should be larger (less memory intensive)\n        assert unique_size &gt; tokenization_size\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManagerIntegration","title":"<code>TestMemoryManagerIntegration</code>","text":"<p>Integration tests for MemoryManager with other components.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>class TestMemoryManagerIntegration:\n    \"\"\"Integration tests for MemoryManager with other components.\"\"\"\n\n    def test_memory_manager_with_real_operations(self):\n        \"\"\"Test MemoryManager with actual memory operations.\"\"\"\n        manager = MemoryManager(max_memory_gb=8.0)  # Reasonable limit\n\n        # Get baseline\n        initial_stats = manager.get_current_memory_usage()\n        assert initial_stats[\"pressure_level\"] in [\"low\", \"medium\", \"high\", \"critical\"]\n\n        # Perform some memory-intensive operations\n        large_data = [list(range(1000)) for _ in range(100)]\n\n        # Check memory increased\n        after_stats = manager.get_current_memory_usage()\n        assert after_stats[\"rss_mb\"] &gt;= initial_stats[\"rss_mb\"]\n\n        # Cleanup and verify GC works\n        del large_data\n        cleanup_stats = manager.enhanced_gc_cleanup()\n\n        # Should have freed some memory\n        assert cleanup_stats[\"memory_freed_mb\"] &gt;= 0\n\n        # Verify trend analysis works with real data\n        trend = manager.get_memory_trend()\n        assert trend in [\"insufficient_data\", \"increasing\", \"decreasing\", \"stable\"]\n\n    def test_adaptive_chunk_sizing_realistic_scenarios(self):\n        \"\"\"Test adaptive chunk sizing with realistic scenarios.\"\"\"\n        manager = MemoryManager(max_memory_gb=4.0)\n\n        # Test various operation types with different base sizes\n        operations = [\n            \"tokenization\",\n            \"ngram_generation\",\n            \"unique_extraction\",\n            \"join_operations\",\n        ]\n        base_sizes = [10000, 50000, 100000]\n\n        for operation in operations:\n            for base_size in base_sizes:\n                adaptive_size = manager.calculate_adaptive_chunk_size(\n                    base_size, operation\n                )\n\n                # Should never be zero or negative\n                assert adaptive_size &gt; 0\n\n                # Should respect minimum size\n                expected_min = max(1000, base_size // 10)\n                assert adaptive_size &gt;= expected_min\n\n                # Should not exceed original size (except for unique_extraction which can be larger)\n                if operation != \"unique_extraction\":\n                    assert adaptive_size &lt;= base_size\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManagerIntegration.test_adaptive_chunk_sizing_realistic_scenarios","title":"<code>test_adaptive_chunk_sizing_realistic_scenarios()</code>","text":"<p>Test adaptive chunk sizing with realistic scenarios.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_adaptive_chunk_sizing_realistic_scenarios(self):\n    \"\"\"Test adaptive chunk sizing with realistic scenarios.\"\"\"\n    manager = MemoryManager(max_memory_gb=4.0)\n\n    # Test various operation types with different base sizes\n    operations = [\n        \"tokenization\",\n        \"ngram_generation\",\n        \"unique_extraction\",\n        \"join_operations\",\n    ]\n    base_sizes = [10000, 50000, 100000]\n\n    for operation in operations:\n        for base_size in base_sizes:\n            adaptive_size = manager.calculate_adaptive_chunk_size(\n                base_size, operation\n            )\n\n            # Should never be zero or negative\n            assert adaptive_size &gt; 0\n\n            # Should respect minimum size\n            expected_min = max(1000, base_size // 10)\n            assert adaptive_size &gt;= expected_min\n\n            # Should not exceed original size (except for unique_extraction which can be larger)\n            if operation != \"unique_extraction\":\n                assert adaptive_size &lt;= base_size\n</code></pre>"},{"location":"reference/app/#app.test_memory_manager.TestMemoryManagerIntegration.test_memory_manager_with_real_operations","title":"<code>test_memory_manager_with_real_operations()</code>","text":"<p>Test MemoryManager with actual memory operations.</p> Source code in <code>app/test_memory_manager.py</code> <pre><code>def test_memory_manager_with_real_operations(self):\n    \"\"\"Test MemoryManager with actual memory operations.\"\"\"\n    manager = MemoryManager(max_memory_gb=8.0)  # Reasonable limit\n\n    # Get baseline\n    initial_stats = manager.get_current_memory_usage()\n    assert initial_stats[\"pressure_level\"] in [\"low\", \"medium\", \"high\", \"critical\"]\n\n    # Perform some memory-intensive operations\n    large_data = [list(range(1000)) for _ in range(100)]\n\n    # Check memory increased\n    after_stats = manager.get_current_memory_usage()\n    assert after_stats[\"rss_mb\"] &gt;= initial_stats[\"rss_mb\"]\n\n    # Cleanup and verify GC works\n    del large_data\n    cleanup_stats = manager.enhanced_gc_cleanup()\n\n    # Should have freed some memory\n    assert cleanup_stats[\"memory_freed_mb\"] &gt;= 0\n\n    # Verify trend analysis works with real data\n    trend = manager.get_memory_trend()\n    assert trend in [\"insufficient_data\", \"increasing\", \"decreasing\", \"stable\"]\n</code></pre>"},{"location":"reference/app/#app.test_utils","title":"<code>test_utils</code>","text":"<p>Tests for app/utils.py tokenization engine.</p> <p>This test suite validates: - Space-separated vs non-space-separated text detection - Social media entity preservation - Mixed script handling - Edge cases and error conditions - Performance with various text types</p>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated","title":"<code>TestIsSpaceSeparated</code>","text":"<p>Test the is_space_separated function for various script types.</p> Source code in <code>app/test_utils.py</code> <pre><code>class TestIsSpaceSeparated:\n    \"\"\"Test the is_space_separated function for various script types.\"\"\"\n\n    def test_latin_script(self):\n        \"\"\"Test Latin script text is detected as space-separated.\"\"\"\n        text = \"Hello world this is English text\"\n        assert is_space_separated(text) is True\n\n    def test_cyrillic_script(self):\n        \"\"\"Test Cyrillic script text is detected as space-separated.\"\"\"\n        text = \"\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440 \u044d\u0442\u043e \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442\"\n        assert is_space_separated(text) is True\n\n    def test_arabic_script(self):\n        \"\"\"Test Arabic script text is detected as space-separated.\"\"\"\n        text = \"\u0645\u0631\u062d\u0628\u0627 \u0628\u0627\u0644\u0639\u0627\u0644\u0645 \u0647\u0630\u0627 \u0646\u0635 \u0639\u0631\u0628\u064a\"\n        assert is_space_separated(text) is True\n\n    def test_chinese_script(self):\n        \"\"\"Test Chinese script text is detected as non-space-separated.\"\"\"\n        text = \"\u4f60\u597d\u4e16\u754c\u8fd9\u662f\u4e2d\u6587\u6587\u672c\"\n        assert is_space_separated(text) is False\n\n    def test_japanese_script(self):\n        \"\"\"Test Japanese script text is detected as non-space-separated.\"\"\"\n        text = \"\u3053\u3093\u306b\u3061\u306f\u4e16\u754c\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u3059\"\n        assert is_space_separated(text) is False\n\n    def test_thai_script(self):\n        \"\"\"Test Thai script text is detected as non-space-separated.\"\"\"\n        text = \"\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e42\u0e25\u0e01\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e44\u0e17\u0e22\"\n        assert is_space_separated(text) is False\n\n    def test_mixed_scripts_majority_latin(self):\n        \"\"\"Test mixed scripts with majority Latin characters.\"\"\"\n        text = \"Hello \u4f60\u597d world this is mostly English\"\n        assert is_space_separated(text) is True\n\n    def test_mixed_scripts_majority_chinese(self):\n        \"\"\"Test mixed scripts with majority Chinese characters.\"\"\"\n        text = \"iPhone\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u5e94\u7528\u7a0b\u5e8f\u5728\u624b\u673a\u4e0a\"\n        assert is_space_separated(text) is False\n\n    def test_empty_text(self):\n        \"\"\"Test empty text defaults to space-separated.\"\"\"\n        assert is_space_separated(\"\") is True\n        assert is_space_separated(\"   \") is True\n\n    def test_no_script_characters(self):\n        \"\"\"Test text with no specific script characters.\"\"\"\n        text = \"123 456 !@# $%^\"\n        assert is_space_separated(text) is True\n\n    def test_polars_expression(self):\n        \"\"\"Test is_space_separated works with polars expressions.\"\"\"\n        df = pl.DataFrame(\n            {\"text\": [\"Hello world\", \"\u4f60\u597d\u4e16\u754c\", \"\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440\", \"\u3053\u3093\u306b\u3061\u306f\", \"\"]}\n        )\n\n        result = df.with_columns(\n            [is_space_separated(pl.col(\"text\")).alias(\"is_space_sep\")]\n        )\n\n        expected = [True, False, True, False, True]\n        assert result[\"is_space_sep\"].to_list() == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_arabic_script","title":"<code>test_arabic_script()</code>","text":"<p>Test Arabic script text is detected as space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_arabic_script(self):\n    \"\"\"Test Arabic script text is detected as space-separated.\"\"\"\n    text = \"\u0645\u0631\u062d\u0628\u0627 \u0628\u0627\u0644\u0639\u0627\u0644\u0645 \u0647\u0630\u0627 \u0646\u0635 \u0639\u0631\u0628\u064a\"\n    assert is_space_separated(text) is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_chinese_script","title":"<code>test_chinese_script()</code>","text":"<p>Test Chinese script text is detected as non-space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_chinese_script(self):\n    \"\"\"Test Chinese script text is detected as non-space-separated.\"\"\"\n    text = \"\u4f60\u597d\u4e16\u754c\u8fd9\u662f\u4e2d\u6587\u6587\u672c\"\n    assert is_space_separated(text) is False\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_cyrillic_script","title":"<code>test_cyrillic_script()</code>","text":"<p>Test Cyrillic script text is detected as space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_cyrillic_script(self):\n    \"\"\"Test Cyrillic script text is detected as space-separated.\"\"\"\n    text = \"\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440 \u044d\u0442\u043e \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442\"\n    assert is_space_separated(text) is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_empty_text","title":"<code>test_empty_text()</code>","text":"<p>Test empty text defaults to space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_empty_text(self):\n    \"\"\"Test empty text defaults to space-separated.\"\"\"\n    assert is_space_separated(\"\") is True\n    assert is_space_separated(\"   \") is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_japanese_script","title":"<code>test_japanese_script()</code>","text":"<p>Test Japanese script text is detected as non-space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_japanese_script(self):\n    \"\"\"Test Japanese script text is detected as non-space-separated.\"\"\"\n    text = \"\u3053\u3093\u306b\u3061\u306f\u4e16\u754c\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u3059\"\n    assert is_space_separated(text) is False\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_latin_script","title":"<code>test_latin_script()</code>","text":"<p>Test Latin script text is detected as space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_latin_script(self):\n    \"\"\"Test Latin script text is detected as space-separated.\"\"\"\n    text = \"Hello world this is English text\"\n    assert is_space_separated(text) is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_mixed_scripts_majority_chinese","title":"<code>test_mixed_scripts_majority_chinese()</code>","text":"<p>Test mixed scripts with majority Chinese characters.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_mixed_scripts_majority_chinese(self):\n    \"\"\"Test mixed scripts with majority Chinese characters.\"\"\"\n    text = \"iPhone\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u5e94\u7528\u7a0b\u5e8f\u5728\u624b\u673a\u4e0a\"\n    assert is_space_separated(text) is False\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_mixed_scripts_majority_latin","title":"<code>test_mixed_scripts_majority_latin()</code>","text":"<p>Test mixed scripts with majority Latin characters.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_mixed_scripts_majority_latin(self):\n    \"\"\"Test mixed scripts with majority Latin characters.\"\"\"\n    text = \"Hello \u4f60\u597d world this is mostly English\"\n    assert is_space_separated(text) is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_no_script_characters","title":"<code>test_no_script_characters()</code>","text":"<p>Test text with no specific script characters.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_no_script_characters(self):\n    \"\"\"Test text with no specific script characters.\"\"\"\n    text = \"123 456 !@# $%^\"\n    assert is_space_separated(text) is True\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_polars_expression","title":"<code>test_polars_expression()</code>","text":"<p>Test is_space_separated works with polars expressions.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_polars_expression(self):\n    \"\"\"Test is_space_separated works with polars expressions.\"\"\"\n    df = pl.DataFrame(\n        {\"text\": [\"Hello world\", \"\u4f60\u597d\u4e16\u754c\", \"\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440\", \"\u3053\u3093\u306b\u3061\u306f\", \"\"]}\n    )\n\n    result = df.with_columns(\n        [is_space_separated(pl.col(\"text\")).alias(\"is_space_sep\")]\n    )\n\n    expected = [True, False, True, False, True]\n    assert result[\"is_space_sep\"].to_list() == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestIsSpaceSeparated.test_thai_script","title":"<code>test_thai_script()</code>","text":"<p>Test Thai script text is detected as non-space-separated.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_thai_script(self):\n    \"\"\"Test Thai script text is detected as non-space-separated.\"\"\"\n    text = \"\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e42\u0e25\u0e01\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e44\u0e17\u0e22\"\n    assert is_space_separated(text) is False\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizationIntegration","title":"<code>TestTokenizationIntegration</code>","text":"<p>Integration tests for tokenization engine with n-gram analysis.</p> Source code in <code>app/test_utils.py</code> <pre><code>class TestTokenizationIntegration:\n    \"\"\"Integration tests for tokenization engine with n-gram analysis.\"\"\"\n\n    def test_tokenization_with_ngram_pipeline(self):\n        \"\"\"Test that tokenization works well with n-gram generation.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"message_text\": [\n                    \"This is a test message\",\n                    \"Check out @user and https://example.com\",\n                    \"Mixed text with \u4e2d\u6587 content\",\n                ],\n                \"message_surrogate_id\": [1, 2, 3],\n            }\n        ).lazy()\n\n        # Apply tokenization\n        tokenized = tokenize_text(df, \"message_text\").collect()\n\n        # Verify all messages were tokenized\n        assert len(tokenized) == 3\n        assert all(isinstance(tokens.to_list(), list) for tokens in tokenized[\"tokens\"])\n        assert all(len(tokens.to_list()) &gt; 0 for tokens in tokenized[\"tokens\"])\n\n        # Verify social media entities are preserved\n        tokens_2 = tokenized[\"tokens\"][1].to_list()\n        assert any(\"@user\" in str(token) for token in tokens_2)\n        assert any(\"https://example.com\" in str(token) for token in tokens_2)\n\n    def test_empty_message_handling(self):\n        \"\"\"Test handling of datasets with empty messages.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"message_text\": [\"Valid message\", \"\", \"   \", \"Another valid message\"],\n                \"message_surrogate_id\": [1, 2, 3, 4],\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"message_text\").collect()\n\n        # Should handle empty messages gracefully\n        assert len(result) == 4\n        assert len(result[\"tokens\"][0].to_list()) &gt; 0  # Valid message\n        assert len(result[\"tokens\"][1].to_list()) == 0  # Empty message\n        assert len(result[\"tokens\"][2].to_list()) == 0  # Whitespace-only message\n        assert len(result[\"tokens\"][3].to_list()) &gt; 0  # Valid message\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizationIntegration.test_empty_message_handling","title":"<code>test_empty_message_handling()</code>","text":"<p>Test handling of datasets with empty messages.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_empty_message_handling(self):\n    \"\"\"Test handling of datasets with empty messages.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"message_text\": [\"Valid message\", \"\", \"   \", \"Another valid message\"],\n            \"message_surrogate_id\": [1, 2, 3, 4],\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"message_text\").collect()\n\n    # Should handle empty messages gracefully\n    assert len(result) == 4\n    assert len(result[\"tokens\"][0].to_list()) &gt; 0  # Valid message\n    assert len(result[\"tokens\"][1].to_list()) == 0  # Empty message\n    assert len(result[\"tokens\"][2].to_list()) == 0  # Whitespace-only message\n    assert len(result[\"tokens\"][3].to_list()) &gt; 0  # Valid message\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizationIntegration.test_tokenization_with_ngram_pipeline","title":"<code>test_tokenization_with_ngram_pipeline()</code>","text":"<p>Test that tokenization works well with n-gram generation.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_tokenization_with_ngram_pipeline(self):\n    \"\"\"Test that tokenization works well with n-gram generation.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"message_text\": [\n                \"This is a test message\",\n                \"Check out @user and https://example.com\",\n                \"Mixed text with \u4e2d\u6587 content\",\n            ],\n            \"message_surrogate_id\": [1, 2, 3],\n        }\n    ).lazy()\n\n    # Apply tokenization\n    tokenized = tokenize_text(df, \"message_text\").collect()\n\n    # Verify all messages were tokenized\n    assert len(tokenized) == 3\n    assert all(isinstance(tokens.to_list(), list) for tokens in tokenized[\"tokens\"])\n    assert all(len(tokens.to_list()) &gt; 0 for tokens in tokenized[\"tokens\"])\n\n    # Verify social media entities are preserved\n    tokens_2 = tokenized[\"tokens\"][1].to_list()\n    assert any(\"@user\" in str(token) for token in tokens_2)\n    assert any(\"https://example.com\" in str(token) for token in tokens_2)\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText","title":"<code>TestTokenizeText</code>","text":"<p>Test the tokenize_text function for various text types and edge cases.</p> Source code in <code>app/test_utils.py</code> <pre><code>class TestTokenizeText:\n    \"\"\"Test the tokenize_text function for various text types and edge cases.\"\"\"\n\n    def test_simple_english_text(self):\n        \"\"\"Test basic English text tokenization.\"\"\"\n        df = pl.DataFrame({\"text\": [\"Hello world this is a test\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        expected = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n        assert tokens == expected\n\n    def test_social_media_entities(self):\n        \"\"\"Test preservation of social media entities.\"\"\"\n        df = pl.DataFrame(\n            {\"text\": [\"Check out https://example.com and @username #hashtag\"]}\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # URLs, mentions, and hashtags should be preserved as-is\n        assert \"https://example.com\" in tokens\n        assert \"@username\" in tokens\n        assert \"#hashtag\" in tokens\n        assert \"check\" in tokens\n        assert \"out\" in tokens\n        assert \"and\" in tokens\n\n    def test_chinese_text(self):\n        \"\"\"Test Chinese text character-level tokenization.\"\"\"\n        df = pl.DataFrame({\"text\": [\"\u8fd9\u662f\u4e2d\u6587\u6d4b\u8bd5\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Chinese text should be split into individual characters\n        expected = [\"\u8fd9\", \"\u662f\", \"\u4e2d\", \"\u6587\", \"\u6d4b\", \"\u8bd5\"]\n        assert tokens == expected\n\n    def test_chinese_text_with_spaces(self):\n        \"\"\"Test Chinese text with spaces (should still split into characters).\"\"\"\n        df = pl.DataFrame({\"text\": [\"\u4f60\u597d \u4e16\u754c \u8fd9\u662f \u4e2d\u6587\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Should split into individual characters, not space-separated words\n        expected = [\"\u4f60\", \"\u597d\", \"\u4e16\", \"\u754c\", \"\u8fd9\", \"\u662f\", \"\u4e2d\", \"\u6587\"]\n        assert tokens == expected\n\n    def test_url_with_cjk_text(self):\n        \"\"\"Test URL preservation with surrounding CJK characters.\"\"\"\n        df = pl.DataFrame({\"text\": [\"\u8bbf\u95eehttps://example.com\u7f51\u7ad9\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # URL should be preserved, CJK characters should be split individually\n        expected = [\"\u8bbf\", \"\u95ee\", \"https://example.com\", \"\u7f51\", \"\u7ad9\"]\n        assert tokens == expected\n\n    def test_mixed_script_text(self):\n        \"\"\"Test mixed script text handling.\"\"\"\n        df = pl.DataFrame({\"text\": [\"iPhone\u7528\u6237 can use this app\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Should contain both the mixed token and separate words\n        assert \"iphone\u7528\u6237\" in tokens  # Mixed script token (lowercased)\n        assert \"can\" in tokens\n        assert \"use\" in tokens\n        assert \"this\" in tokens\n        assert \"app\" in tokens\n\n    def test_whitespace_normalization(self):\n        \"\"\"Test that multiple whitespace is normalized.\"\"\"\n        df = pl.DataFrame({\"text\": [\"hello    world\\t\\ttest\\n\\nmore   spaces\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        expected = [\"hello\", \"world\", \"test\", \"more\", \"spaces\"]\n        assert tokens == expected\n\n    def test_empty_text(self):\n        \"\"\"Test handling of empty text.\"\"\"\n        df = pl.DataFrame({\"text\": [\"\", \"   \", \"\\t\\n\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # All should result in empty token lists\n        assert result[\"tokens\"][0].to_list() == []\n        assert result[\"tokens\"][1].to_list() == []\n        assert result[\"tokens\"][2].to_list() == []\n\n    def test_punctuation_handling(self):\n        \"\"\"Test handling of punctuation.\"\"\"\n        df = pl.DataFrame({\"text\": [\"Hello, world! How are you?\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Punctuation should be included with words (except for social media entities)\n        expected = [\"hello,\", \"world!\", \"how\", \"are\", \"you?\"]\n        assert tokens == expected\n\n    def test_case_preservation_for_urls(self):\n        \"\"\"Test that URLs preserve their case.\"\"\"\n        df = pl.DataFrame({\"text\": [\"Visit HTTPS://Example.COM/Path today\"]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        assert \"HTTPS://Example.COM/Path\" in tokens\n        assert \"visit\" in tokens\n        assert \"today\" in tokens\n\n    def test_multiple_messages(self):\n        \"\"\"Test tokenization of multiple messages.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"First message here\",\n                    \"Second message with @mention\",\n                    \"Third message \u4f60\u597d\u4e16\u754c\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        assert len(result) == 3\n        assert result[\"tokens\"][0].to_list() == [\"first\", \"message\", \"here\"]\n        assert \"@mention\" in result[\"tokens\"][1].to_list()\n        # CJK characters should be split individually for consistency\n        tokens_2 = result[\"tokens\"][2].to_list()\n        assert \"\u4f60\" in tokens_2\n        assert \"\u597d\" in tokens_2\n        assert \"\u4e16\" in tokens_2\n        assert \"\u754c\" in tokens_2\n\n    def test_invalid_input_types(self):\n        \"\"\"Test error handling for invalid input types.\"\"\"\n        # Non-LazyFrame input\n        with pytest.raises(TypeError, match=\"Expected polars LazyFrame\"):\n            tokenize_text(\"not a dataframe\", \"text\")\n\n        # Non-string column name\n        df = pl.DataFrame({\"text\": [\"test\"]}).lazy()\n        with pytest.raises(TypeError, match=\"text_column must be a string\"):\n            tokenize_text(df, 123)\n\n    def test_nonexistent_column(self):\n        \"\"\"Test error handling for nonexistent column.\"\"\"\n        df = pl.DataFrame({\"other_col\": [\"test\"]}).lazy()\n\n        # This should raise an error when the lazy frame is executed\n        with pytest.raises(Exception):  # Will be a polars error about missing column\n            tokenize_text(df, \"nonexistent_column\").collect()\n\n    def test_special_characters(self):\n        \"\"\"Test handling of various special characters.\"\"\"\n        df = pl.DataFrame(\n            {\"text\": [\"Text with \u00e9mojis \ud83d\ude00 and \u00e0cc\u00e9nts caf\u00e9 na\u00efve\"]}\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Should handle accented characters properly\n        assert \"\u00e9mojis\" in tokens\n        assert \"\ud83d\ude00\" in tokens\n        assert \"\u00e0cc\u00e9nts\" in tokens\n        assert \"caf\u00e9\" in tokens\n        assert \"na\u00efve\" in tokens\n\n    def test_performance_with_large_text(self):\n        \"\"\"Test tokenization performance with larger text.\"\"\"\n        large_text = \" \".join([\"word\"] * 1000)\n        df = pl.DataFrame({\"text\": [large_text]}).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        assert len(tokens) == 1000\n        assert all(token == \"word\" for token in tokens)\n\n    def test_social_media_entity_variations(self):\n        \"\"\"Test various social media entity formats.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Check http://short.ly and https://secure.example.com/path?query=123 plus @user_name and #CamelCaseTag\"\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # All URL formats should be preserved\n        assert \"http://short.ly\" in tokens\n        assert \"https://secure.example.com/path?query=123\" in tokens\n        assert \"@user_name\" in tokens\n        assert \"#CamelCaseTag\" in tokens\n\n    def test_pure_punctuation_filtering(self):\n        \"\"\"Test that pure punctuation tokens are filtered out.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"!!! ... ,,, ??? ::: ;;;\",  # Pure punctuation only\n                    \"Hello!!! World... Test,,,\",  # Mixed content\n                    \"\u3002\u3002\u3002 \uff01\uff01\uff01 \uff1f\uff1f\uff1f\",  # CJK punctuation\n                    \"((())) [[[]]] {{{}}}\",  # Brackets and braces\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # First row: pure punctuation should be filtered to empty list\n        tokens_0 = result[\"tokens\"][0].to_list()\n        assert (\n            tokens_0 == []\n        ), f\"Expected empty tokens for pure punctuation, got: {tokens_0}\"\n\n        # Second row: mixed content should preserve words but filter pure punctuation\n        tokens_1 = result[\"tokens\"][1].to_list()\n        # Should contain words but not pure punctuation sequences\n        word_tokens = [token for token in tokens_1 if any(c.isalnum() for c in token)]\n        assert len(word_tokens) &gt;= 2, f\"Expected words to be preserved, got: {tokens_1}\"\n\n        # Third row: CJK punctuation should also be filtered\n        tokens_2 = result[\"tokens\"][2].to_list()\n        assert (\n            tokens_2 == []\n        ), f\"Expected CJK punctuation to be filtered, got: {tokens_2}\"\n\n        # Fourth row: brackets and braces should be filtered\n        tokens_3 = result[\"tokens\"][3].to_list()\n        assert (\n            tokens_3 == []\n        ), f\"Expected brackets/braces to be filtered, got: {tokens_3}\"\n\n    def test_punctuation_edge_cases_preserved(self):\n        \"\"\"Test that legitimate tokens with punctuation are preserved.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Visit https://example.com/path?query=test&amp;param=1 today\",\n                    \"Contact @user123 and check #hashtag!\",\n                    \"Words like don't, can't, won't should work\",\n                    \"Email test@example.com or visit sub.domain.com\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # URLs with punctuation should be preserved\n        tokens_0 = result[\"tokens\"][0].to_list()\n        assert \"https://example.com/path?query=test&amp;param=1\" in tokens_0\n\n        # Social media entities should be preserved\n        tokens_1 = result[\"tokens\"][1].to_list()\n        assert \"@user123\" in tokens_1\n        assert \"#hashtag\" in tokens_1\n\n        # Contractions should be preserved\n        tokens_2 = result[\"tokens\"][2].to_list()\n        contraction_found = any(\"'\" in token for token in tokens_2)\n        assert (\n            contraction_found\n        ), f\"Expected contractions to be preserved, got: {tokens_2}\"\n\n        # Email-like patterns should work (even if not in URL pattern)\n        tokens_3 = result[\"tokens\"][3].to_list()\n        email_or_domain_found = any(\n            \".\" in token and len(token) &gt; 1 for token in tokens_3\n        )\n        assert email_or_domain_found, f\"Expected domain/email patterns, got: {tokens_3}\"\n\n    def test_punctuation_with_multilingual_text(self):\n        \"\"\"Test punctuation filtering with various languages.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"English... \u4e2d\u6587\u3002\u3002\u3002 \ud55c\uad6d\uc5b4!!! \u0440\u0443\u0441\u0441\u043a\u0438\u0439???\",\n                    \"Mixed iPhone\u7528\u6237!!! can use this.\",\n                    \"URL https://\u4f8b\u3048.com/\u30d1\u30b9 works fine.\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # Should preserve language text but filter pure punctuation\n        tokens_0 = result[\"tokens\"][0].to_list()\n        has_text = any(\n            any(c.isalnum() or ord(c) &gt; 127 for c in token) for token in tokens_0\n        )\n        assert has_text, f\"Expected multilingual text to be preserved, got: {tokens_0}\"\n\n        # Mixed script tokens should be preserved\n        tokens_1 = result[\"tokens\"][1].to_list()\n        assert any(\n            \"iphone\" in token.lower() for token in tokens_1\n        ), f\"Mixed script not found: {tokens_1}\"\n\n        # International domain names: protocol should be preserved, but non-ASCII parts will be tokenized separately\n        tokens_2 = result[\"tokens\"][2].to_list()\n        https_found = any(\"https:\" in token for token in tokens_2)\n        japanese_chars_found = any(\n            ord(c) &gt; 127 for token in tokens_2 for c in token if c.isalpha()\n        )\n        assert https_found, f\"HTTPS protocol not preserved: {tokens_2}\"\n        assert japanese_chars_found, f\"Japanese characters not preserved: {tokens_2}\"\n\n    def test_ngram_punctuation_regression(self):\n        \"\"\"Test that n-gram analysis won't generate pure punctuation n-grams.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Normal text with... excessive punctuation!!! And more???\",\n                    \"!!! ... ,,, !!! ... ,,,\",  # Pattern that previously generated bad n-grams\n                    \"Good content. Bad punctuation!!!\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # Collect all tokens and ensure no pure punctuation tokens exist\n        all_tokens = []\n        for token_list in result[\"tokens\"].to_list():\n            all_tokens.extend(token_list)\n\n        # No token should be pure punctuation\n        pure_punctuation_tokens = [\n            token\n            for token in all_tokens\n            if token\n            and all(not c.isalnum() and ord(c) &lt; 256 for c in token)\n            and not token.startswith((\"http\", \"@\", \"#\"))  # Exclude legitimate patterns\n        ]\n\n        assert (\n            pure_punctuation_tokens == []\n        ), f\"Found pure punctuation tokens: {pure_punctuation_tokens}\"\n\n        # Should still have legitimate content\n        content_tokens = [\n            token for token in all_tokens if any(c.isalnum() for c in token)\n        ]\n        assert (\n            len(content_tokens) &gt; 0\n        ), \"No content tokens found - over-filtering occurred\"\n\n    def test_complex_urls_with_punctuation(self):\n        \"\"\"Test complex URLs with various punctuation marks are preserved.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Check https://example.com/path?query=1&amp;param=test#anchor and http://sub.domain.co.uk/\"\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Complex URLs should be preserved exactly\n        assert \"https://example.com/path?query=1&amp;param=test#anchor\" in tokens\n        assert \"http://sub.domain.co.uk/\" in tokens\n        assert \"check\" in tokens\n        assert \"and\" in tokens\n\n    def test_symbol_filtering_specificity(self):\n        \"\"\"Test that only problematic symbols are filtered, not meaningful ones.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Math symbols === +++ --- should be filtered\",\n                    \"But emojis \ud83d\ude00\ud83d\ude0e\ud83c\udf89 should be preserved\",\n                    \"Currency symbols $100 \u20ac50 should be filtered\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # Math symbols should be filtered\n        tokens_0 = result[\"tokens\"][0].to_list()\n        math_symbols_found = any(token in [\"===\", \"+++\", \"---\"] for token in tokens_0)\n        assert not math_symbols_found, f\"Math symbols not filtered: {tokens_0}\"\n        assert \"math\" in tokens_0\n        assert \"symbols\" in tokens_0\n\n        # Emojis should be preserved\n        tokens_1 = result[\"tokens\"][1].to_list()\n        emoji_found = any(\n            ord(c) &gt; 127 and not c.isalpha() for token in tokens_1 for c in token\n        )\n        assert emoji_found, f\"Emojis not preserved: {tokens_1}\"\n\n        # Currency symbols should be filtered, numbers preserved as individual digits\n        tokens_2 = result[\"tokens\"][2].to_list()\n        currency_symbols_found = any(token in [\"$\", \"\u20ac\"] for token in tokens_2)\n        assert not currency_symbols_found, f\"Currency symbols not filtered: {tokens_2}\"\n        # Numbers may be tokenized as individual digits or groups\n        has_numbers = any(c.isdigit() for token in tokens_2 for c in token)\n        assert has_numbers, f\"Numbers not preserved: {tokens_2}\"\n\n    def test_real_world_social_media_example(self):\n        \"\"\"Test realistic social media content with mixed punctuation.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"OMG!!! Check this out: https://tinyurl.com/demo @everyone #viral #trending... So cool!!!\"\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n        tokens = result[\"tokens\"][0].to_list()\n\n        # Should preserve content but filter pure punctuation\n        assert \"https://tinyurl.com/demo\" in tokens\n        assert \"@everyone\" in tokens\n        assert \"#viral\" in tokens\n        assert \"#trending\" in tokens\n        assert any(\"omg\" in token.lower() for token in tokens)\n        assert any(\"check\" in token.lower() for token in tokens)\n        assert any(\"cool\" in token.lower() for token in tokens)\n\n    def test_comprehensive_punctuation_categories(self):\n        \"\"\"Test various Unicode punctuation categories are properly filtered.\"\"\"\n        df = pl.DataFrame(\n            {\n                \"text\": [\n                    \"Brackets: ()[]{}  Quotes: \\\"'`  Dashes: -\u2013\u2014  Math: +=*\u00f7\",\n                    \"CJK punct: \u3002\uff01\uff1f\uff0c\uff1a\uff1b  Symbols: @#$%^&amp;*  Mixed: word!!! ...word\",\n                ]\n            }\n        ).lazy()\n\n        result = tokenize_text(df, \"text\").collect()\n\n        # First row: various punctuation types\n        tokens_0 = result[\"tokens\"][0].to_list()\n        content_words = [token for token in tokens_0 if any(c.isalpha() for c in token)]\n        # Words may include attached punctuation (like \"brackets:\")\n        word_stems = [w.rstrip(\":\").lower() for w in content_words]\n        assert \"brackets\" in word_stems\n        assert \"quotes\" in word_stems\n\n        # Second row: mixed content\n        tokens_1 = result[\"tokens\"][1].to_list()\n        # Should preserve mixed punctuation with letters but filter pure punctuation\n        mixed_tokens = [token for token in tokens_1 if any(c.isalpha() for c in token)]\n        assert len(mixed_tokens) &gt;= 2, f\"Expected mixed alpha tokens: {tokens_1}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_case_preservation_for_urls","title":"<code>test_case_preservation_for_urls()</code>","text":"<p>Test that URLs preserve their case.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_case_preservation_for_urls(self):\n    \"\"\"Test that URLs preserve their case.\"\"\"\n    df = pl.DataFrame({\"text\": [\"Visit HTTPS://Example.COM/Path today\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    assert \"HTTPS://Example.COM/Path\" in tokens\n    assert \"visit\" in tokens\n    assert \"today\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_chinese_text","title":"<code>test_chinese_text()</code>","text":"<p>Test Chinese text character-level tokenization.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_chinese_text(self):\n    \"\"\"Test Chinese text character-level tokenization.\"\"\"\n    df = pl.DataFrame({\"text\": [\"\u8fd9\u662f\u4e2d\u6587\u6d4b\u8bd5\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Chinese text should be split into individual characters\n    expected = [\"\u8fd9\", \"\u662f\", \"\u4e2d\", \"\u6587\", \"\u6d4b\", \"\u8bd5\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_chinese_text_with_spaces","title":"<code>test_chinese_text_with_spaces()</code>","text":"<p>Test Chinese text with spaces (should still split into characters).</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_chinese_text_with_spaces(self):\n    \"\"\"Test Chinese text with spaces (should still split into characters).\"\"\"\n    df = pl.DataFrame({\"text\": [\"\u4f60\u597d \u4e16\u754c \u8fd9\u662f \u4e2d\u6587\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Should split into individual characters, not space-separated words\n    expected = [\"\u4f60\", \"\u597d\", \"\u4e16\", \"\u754c\", \"\u8fd9\", \"\u662f\", \"\u4e2d\", \"\u6587\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_complex_urls_with_punctuation","title":"<code>test_complex_urls_with_punctuation()</code>","text":"<p>Test complex URLs with various punctuation marks are preserved.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_complex_urls_with_punctuation(self):\n    \"\"\"Test complex URLs with various punctuation marks are preserved.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Check https://example.com/path?query=1&amp;param=test#anchor and http://sub.domain.co.uk/\"\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Complex URLs should be preserved exactly\n    assert \"https://example.com/path?query=1&amp;param=test#anchor\" in tokens\n    assert \"http://sub.domain.co.uk/\" in tokens\n    assert \"check\" in tokens\n    assert \"and\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_comprehensive_punctuation_categories","title":"<code>test_comprehensive_punctuation_categories()</code>","text":"<p>Test various Unicode punctuation categories are properly filtered.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_comprehensive_punctuation_categories(self):\n    \"\"\"Test various Unicode punctuation categories are properly filtered.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Brackets: ()[]{}  Quotes: \\\"'`  Dashes: -\u2013\u2014  Math: +=*\u00f7\",\n                \"CJK punct: \u3002\uff01\uff1f\uff0c\uff1a\uff1b  Symbols: @#$%^&amp;*  Mixed: word!!! ...word\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # First row: various punctuation types\n    tokens_0 = result[\"tokens\"][0].to_list()\n    content_words = [token for token in tokens_0 if any(c.isalpha() for c in token)]\n    # Words may include attached punctuation (like \"brackets:\")\n    word_stems = [w.rstrip(\":\").lower() for w in content_words]\n    assert \"brackets\" in word_stems\n    assert \"quotes\" in word_stems\n\n    # Second row: mixed content\n    tokens_1 = result[\"tokens\"][1].to_list()\n    # Should preserve mixed punctuation with letters but filter pure punctuation\n    mixed_tokens = [token for token in tokens_1 if any(c.isalpha() for c in token)]\n    assert len(mixed_tokens) &gt;= 2, f\"Expected mixed alpha tokens: {tokens_1}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_empty_text","title":"<code>test_empty_text()</code>","text":"<p>Test handling of empty text.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_empty_text(self):\n    \"\"\"Test handling of empty text.\"\"\"\n    df = pl.DataFrame({\"text\": [\"\", \"   \", \"\\t\\n\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # All should result in empty token lists\n    assert result[\"tokens\"][0].to_list() == []\n    assert result[\"tokens\"][1].to_list() == []\n    assert result[\"tokens\"][2].to_list() == []\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_invalid_input_types","title":"<code>test_invalid_input_types()</code>","text":"<p>Test error handling for invalid input types.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_invalid_input_types(self):\n    \"\"\"Test error handling for invalid input types.\"\"\"\n    # Non-LazyFrame input\n    with pytest.raises(TypeError, match=\"Expected polars LazyFrame\"):\n        tokenize_text(\"not a dataframe\", \"text\")\n\n    # Non-string column name\n    df = pl.DataFrame({\"text\": [\"test\"]}).lazy()\n    with pytest.raises(TypeError, match=\"text_column must be a string\"):\n        tokenize_text(df, 123)\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_mixed_script_text","title":"<code>test_mixed_script_text()</code>","text":"<p>Test mixed script text handling.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_mixed_script_text(self):\n    \"\"\"Test mixed script text handling.\"\"\"\n    df = pl.DataFrame({\"text\": [\"iPhone\u7528\u6237 can use this app\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Should contain both the mixed token and separate words\n    assert \"iphone\u7528\u6237\" in tokens  # Mixed script token (lowercased)\n    assert \"can\" in tokens\n    assert \"use\" in tokens\n    assert \"this\" in tokens\n    assert \"app\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_multiple_messages","title":"<code>test_multiple_messages()</code>","text":"<p>Test tokenization of multiple messages.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_multiple_messages(self):\n    \"\"\"Test tokenization of multiple messages.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"First message here\",\n                \"Second message with @mention\",\n                \"Third message \u4f60\u597d\u4e16\u754c\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    assert len(result) == 3\n    assert result[\"tokens\"][0].to_list() == [\"first\", \"message\", \"here\"]\n    assert \"@mention\" in result[\"tokens\"][1].to_list()\n    # CJK characters should be split individually for consistency\n    tokens_2 = result[\"tokens\"][2].to_list()\n    assert \"\u4f60\" in tokens_2\n    assert \"\u597d\" in tokens_2\n    assert \"\u4e16\" in tokens_2\n    assert \"\u754c\" in tokens_2\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_ngram_punctuation_regression","title":"<code>test_ngram_punctuation_regression()</code>","text":"<p>Test that n-gram analysis won't generate pure punctuation n-grams.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_ngram_punctuation_regression(self):\n    \"\"\"Test that n-gram analysis won't generate pure punctuation n-grams.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Normal text with... excessive punctuation!!! And more???\",\n                \"!!! ... ,,, !!! ... ,,,\",  # Pattern that previously generated bad n-grams\n                \"Good content. Bad punctuation!!!\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # Collect all tokens and ensure no pure punctuation tokens exist\n    all_tokens = []\n    for token_list in result[\"tokens\"].to_list():\n        all_tokens.extend(token_list)\n\n    # No token should be pure punctuation\n    pure_punctuation_tokens = [\n        token\n        for token in all_tokens\n        if token\n        and all(not c.isalnum() and ord(c) &lt; 256 for c in token)\n        and not token.startswith((\"http\", \"@\", \"#\"))  # Exclude legitimate patterns\n    ]\n\n    assert (\n        pure_punctuation_tokens == []\n    ), f\"Found pure punctuation tokens: {pure_punctuation_tokens}\"\n\n    # Should still have legitimate content\n    content_tokens = [\n        token for token in all_tokens if any(c.isalnum() for c in token)\n    ]\n    assert (\n        len(content_tokens) &gt; 0\n    ), \"No content tokens found - over-filtering occurred\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_nonexistent_column","title":"<code>test_nonexistent_column()</code>","text":"<p>Test error handling for nonexistent column.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_nonexistent_column(self):\n    \"\"\"Test error handling for nonexistent column.\"\"\"\n    df = pl.DataFrame({\"other_col\": [\"test\"]}).lazy()\n\n    # This should raise an error when the lazy frame is executed\n    with pytest.raises(Exception):  # Will be a polars error about missing column\n        tokenize_text(df, \"nonexistent_column\").collect()\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_performance_with_large_text","title":"<code>test_performance_with_large_text()</code>","text":"<p>Test tokenization performance with larger text.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_performance_with_large_text(self):\n    \"\"\"Test tokenization performance with larger text.\"\"\"\n    large_text = \" \".join([\"word\"] * 1000)\n    df = pl.DataFrame({\"text\": [large_text]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    assert len(tokens) == 1000\n    assert all(token == \"word\" for token in tokens)\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_punctuation_edge_cases_preserved","title":"<code>test_punctuation_edge_cases_preserved()</code>","text":"<p>Test that legitimate tokens with punctuation are preserved.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_punctuation_edge_cases_preserved(self):\n    \"\"\"Test that legitimate tokens with punctuation are preserved.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Visit https://example.com/path?query=test&amp;param=1 today\",\n                \"Contact @user123 and check #hashtag!\",\n                \"Words like don't, can't, won't should work\",\n                \"Email test@example.com or visit sub.domain.com\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # URLs with punctuation should be preserved\n    tokens_0 = result[\"tokens\"][0].to_list()\n    assert \"https://example.com/path?query=test&amp;param=1\" in tokens_0\n\n    # Social media entities should be preserved\n    tokens_1 = result[\"tokens\"][1].to_list()\n    assert \"@user123\" in tokens_1\n    assert \"#hashtag\" in tokens_1\n\n    # Contractions should be preserved\n    tokens_2 = result[\"tokens\"][2].to_list()\n    contraction_found = any(\"'\" in token for token in tokens_2)\n    assert (\n        contraction_found\n    ), f\"Expected contractions to be preserved, got: {tokens_2}\"\n\n    # Email-like patterns should work (even if not in URL pattern)\n    tokens_3 = result[\"tokens\"][3].to_list()\n    email_or_domain_found = any(\n        \".\" in token and len(token) &gt; 1 for token in tokens_3\n    )\n    assert email_or_domain_found, f\"Expected domain/email patterns, got: {tokens_3}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_punctuation_handling","title":"<code>test_punctuation_handling()</code>","text":"<p>Test handling of punctuation.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_punctuation_handling(self):\n    \"\"\"Test handling of punctuation.\"\"\"\n    df = pl.DataFrame({\"text\": [\"Hello, world! How are you?\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Punctuation should be included with words (except for social media entities)\n    expected = [\"hello,\", \"world!\", \"how\", \"are\", \"you?\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_punctuation_with_multilingual_text","title":"<code>test_punctuation_with_multilingual_text()</code>","text":"<p>Test punctuation filtering with various languages.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_punctuation_with_multilingual_text(self):\n    \"\"\"Test punctuation filtering with various languages.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"English... \u4e2d\u6587\u3002\u3002\u3002 \ud55c\uad6d\uc5b4!!! \u0440\u0443\u0441\u0441\u043a\u0438\u0439???\",\n                \"Mixed iPhone\u7528\u6237!!! can use this.\",\n                \"URL https://\u4f8b\u3048.com/\u30d1\u30b9 works fine.\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # Should preserve language text but filter pure punctuation\n    tokens_0 = result[\"tokens\"][0].to_list()\n    has_text = any(\n        any(c.isalnum() or ord(c) &gt; 127 for c in token) for token in tokens_0\n    )\n    assert has_text, f\"Expected multilingual text to be preserved, got: {tokens_0}\"\n\n    # Mixed script tokens should be preserved\n    tokens_1 = result[\"tokens\"][1].to_list()\n    assert any(\n        \"iphone\" in token.lower() for token in tokens_1\n    ), f\"Mixed script not found: {tokens_1}\"\n\n    # International domain names: protocol should be preserved, but non-ASCII parts will be tokenized separately\n    tokens_2 = result[\"tokens\"][2].to_list()\n    https_found = any(\"https:\" in token for token in tokens_2)\n    japanese_chars_found = any(\n        ord(c) &gt; 127 for token in tokens_2 for c in token if c.isalpha()\n    )\n    assert https_found, f\"HTTPS protocol not preserved: {tokens_2}\"\n    assert japanese_chars_found, f\"Japanese characters not preserved: {tokens_2}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_pure_punctuation_filtering","title":"<code>test_pure_punctuation_filtering()</code>","text":"<p>Test that pure punctuation tokens are filtered out.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_pure_punctuation_filtering(self):\n    \"\"\"Test that pure punctuation tokens are filtered out.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"!!! ... ,,, ??? ::: ;;;\",  # Pure punctuation only\n                \"Hello!!! World... Test,,,\",  # Mixed content\n                \"\u3002\u3002\u3002 \uff01\uff01\uff01 \uff1f\uff1f\uff1f\",  # CJK punctuation\n                \"((())) [[[]]] {{{}}}\",  # Brackets and braces\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # First row: pure punctuation should be filtered to empty list\n    tokens_0 = result[\"tokens\"][0].to_list()\n    assert (\n        tokens_0 == []\n    ), f\"Expected empty tokens for pure punctuation, got: {tokens_0}\"\n\n    # Second row: mixed content should preserve words but filter pure punctuation\n    tokens_1 = result[\"tokens\"][1].to_list()\n    # Should contain words but not pure punctuation sequences\n    word_tokens = [token for token in tokens_1 if any(c.isalnum() for c in token)]\n    assert len(word_tokens) &gt;= 2, f\"Expected words to be preserved, got: {tokens_1}\"\n\n    # Third row: CJK punctuation should also be filtered\n    tokens_2 = result[\"tokens\"][2].to_list()\n    assert (\n        tokens_2 == []\n    ), f\"Expected CJK punctuation to be filtered, got: {tokens_2}\"\n\n    # Fourth row: brackets and braces should be filtered\n    tokens_3 = result[\"tokens\"][3].to_list()\n    assert (\n        tokens_3 == []\n    ), f\"Expected brackets/braces to be filtered, got: {tokens_3}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_real_world_social_media_example","title":"<code>test_real_world_social_media_example()</code>","text":"<p>Test realistic social media content with mixed punctuation.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_real_world_social_media_example(self):\n    \"\"\"Test realistic social media content with mixed punctuation.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"OMG!!! Check this out: https://tinyurl.com/demo @everyone #viral #trending... So cool!!!\"\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Should preserve content but filter pure punctuation\n    assert \"https://tinyurl.com/demo\" in tokens\n    assert \"@everyone\" in tokens\n    assert \"#viral\" in tokens\n    assert \"#trending\" in tokens\n    assert any(\"omg\" in token.lower() for token in tokens)\n    assert any(\"check\" in token.lower() for token in tokens)\n    assert any(\"cool\" in token.lower() for token in tokens)\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_simple_english_text","title":"<code>test_simple_english_text()</code>","text":"<p>Test basic English text tokenization.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_simple_english_text(self):\n    \"\"\"Test basic English text tokenization.\"\"\"\n    df = pl.DataFrame({\"text\": [\"Hello world this is a test\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    expected = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_social_media_entities","title":"<code>test_social_media_entities()</code>","text":"<p>Test preservation of social media entities.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_social_media_entities(self):\n    \"\"\"Test preservation of social media entities.\"\"\"\n    df = pl.DataFrame(\n        {\"text\": [\"Check out https://example.com and @username #hashtag\"]}\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # URLs, mentions, and hashtags should be preserved as-is\n    assert \"https://example.com\" in tokens\n    assert \"@username\" in tokens\n    assert \"#hashtag\" in tokens\n    assert \"check\" in tokens\n    assert \"out\" in tokens\n    assert \"and\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_social_media_entity_variations","title":"<code>test_social_media_entity_variations()</code>","text":"<p>Test various social media entity formats.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_social_media_entity_variations(self):\n    \"\"\"Test various social media entity formats.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Check http://short.ly and https://secure.example.com/path?query=123 plus @user_name and #CamelCaseTag\"\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # All URL formats should be preserved\n    assert \"http://short.ly\" in tokens\n    assert \"https://secure.example.com/path?query=123\" in tokens\n    assert \"@user_name\" in tokens\n    assert \"#CamelCaseTag\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_special_characters","title":"<code>test_special_characters()</code>","text":"<p>Test handling of various special characters.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_special_characters(self):\n    \"\"\"Test handling of various special characters.\"\"\"\n    df = pl.DataFrame(\n        {\"text\": [\"Text with \u00e9mojis \ud83d\ude00 and \u00e0cc\u00e9nts caf\u00e9 na\u00efve\"]}\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # Should handle accented characters properly\n    assert \"\u00e9mojis\" in tokens\n    assert \"\ud83d\ude00\" in tokens\n    assert \"\u00e0cc\u00e9nts\" in tokens\n    assert \"caf\u00e9\" in tokens\n    assert \"na\u00efve\" in tokens\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_symbol_filtering_specificity","title":"<code>test_symbol_filtering_specificity()</code>","text":"<p>Test that only problematic symbols are filtered, not meaningful ones.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_symbol_filtering_specificity(self):\n    \"\"\"Test that only problematic symbols are filtered, not meaningful ones.\"\"\"\n    df = pl.DataFrame(\n        {\n            \"text\": [\n                \"Math symbols === +++ --- should be filtered\",\n                \"But emojis \ud83d\ude00\ud83d\ude0e\ud83c\udf89 should be preserved\",\n                \"Currency symbols $100 \u20ac50 should be filtered\",\n            ]\n        }\n    ).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n\n    # Math symbols should be filtered\n    tokens_0 = result[\"tokens\"][0].to_list()\n    math_symbols_found = any(token in [\"===\", \"+++\", \"---\"] for token in tokens_0)\n    assert not math_symbols_found, f\"Math symbols not filtered: {tokens_0}\"\n    assert \"math\" in tokens_0\n    assert \"symbols\" in tokens_0\n\n    # Emojis should be preserved\n    tokens_1 = result[\"tokens\"][1].to_list()\n    emoji_found = any(\n        ord(c) &gt; 127 and not c.isalpha() for token in tokens_1 for c in token\n    )\n    assert emoji_found, f\"Emojis not preserved: {tokens_1}\"\n\n    # Currency symbols should be filtered, numbers preserved as individual digits\n    tokens_2 = result[\"tokens\"][2].to_list()\n    currency_symbols_found = any(token in [\"$\", \"\u20ac\"] for token in tokens_2)\n    assert not currency_symbols_found, f\"Currency symbols not filtered: {tokens_2}\"\n    # Numbers may be tokenized as individual digits or groups\n    has_numbers = any(c.isdigit() for token in tokens_2 for c in token)\n    assert has_numbers, f\"Numbers not preserved: {tokens_2}\"\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_url_with_cjk_text","title":"<code>test_url_with_cjk_text()</code>","text":"<p>Test URL preservation with surrounding CJK characters.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_url_with_cjk_text(self):\n    \"\"\"Test URL preservation with surrounding CJK characters.\"\"\"\n    df = pl.DataFrame({\"text\": [\"\u8bbf\u95eehttps://example.com\u7f51\u7ad9\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    # URL should be preserved, CJK characters should be split individually\n    expected = [\"\u8bbf\", \"\u95ee\", \"https://example.com\", \"\u7f51\", \"\u7ad9\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.test_utils.TestTokenizeText.test_whitespace_normalization","title":"<code>test_whitespace_normalization()</code>","text":"<p>Test that multiple whitespace is normalized.</p> Source code in <code>app/test_utils.py</code> <pre><code>def test_whitespace_normalization(self):\n    \"\"\"Test that multiple whitespace is normalized.\"\"\"\n    df = pl.DataFrame({\"text\": [\"hello    world\\t\\ttest\\n\\nmore   spaces\"]}).lazy()\n\n    result = tokenize_text(df, \"text\").collect()\n    tokens = result[\"tokens\"][0].to_list()\n\n    expected = [\"hello\", \"world\", \"test\", \"more\", \"spaces\"]\n    assert tokens == expected\n</code></pre>"},{"location":"reference/app/#app.utils","title":"<code>utils</code>","text":""},{"location":"reference/app/#app.utils.MemoryManager","title":"<code>MemoryManager</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Real-time memory monitoring and adaptive processing control.</p> <p>Provides memory usage tracking, adaptive chunk sizing, early warning system, and automatic garbage collection triggering for memory pressure scenarios.</p> Source code in <code>app/utils.py</code> <pre><code>class MemoryManager(BaseModel):\n    \"\"\"\n    Real-time memory monitoring and adaptive processing control.\n\n    Provides memory usage tracking, adaptive chunk sizing, early warning system,\n    and automatic garbage collection triggering for memory pressure scenarios.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    max_memory_gb: float = 4.0\n    process_name: str = \"memory_manager\"\n    max_memory_bytes: float = 0\n    process: Optional[psutil.Process] = None\n    # More lenient thresholds for higher-memory systems\n    thresholds: Dict[MemoryPressureLevel, float] = {\n        MemoryPressureLevel.MEDIUM: 0.70,  # Increased from 0.60\n        MemoryPressureLevel.HIGH: 0.80,  # Increased from 0.75\n        MemoryPressureLevel.CRITICAL: 0.90,  # Increased from 0.85\n    }\n    # Less aggressive chunk size reduction\n    chunk_size_factors: Dict[MemoryPressureLevel, float] = {\n        MemoryPressureLevel.LOW: 1.0,\n        MemoryPressureLevel.MEDIUM: 0.8,  # Increased from 0.7\n        MemoryPressureLevel.HIGH: 0.6,  # Increased from 0.4\n        MemoryPressureLevel.CRITICAL: 0.4,  # Increased from 0.2\n    }\n    memory_history: list = []\n    max_history_size: int = 100\n    logger: Optional[logging.Logger] = None\n\n    def __init__(self, max_memory_gb: Optional[float] = None, **data):\n        # Auto-detect memory limit if not provided\n        was_auto_detected = max_memory_gb is None\n        if max_memory_gb is None:\n            max_memory_gb = self._auto_detect_memory_limit()\n\n        # Update data with detected/provided memory limit\n        data[\"max_memory_gb\"] = max_memory_gb\n\n        super().__init__(**data)\n        self.max_memory_bytes = self.max_memory_gb * 1024**3\n        self.process = psutil.Process()\n        self.logger = get_logger(f\"{__name__}.{self.process_name}_memory\")\n\n        # Log detected configuration for transparency\n        system_memory = psutil.virtual_memory()\n        total_gb = system_memory.total / 1024**3\n        self.logger.info(\n            \"Memory manager initialized with intelligent detection\",\n            extra={\n                \"system_total_gb\": round(total_gb, 1),\n                \"detected_limit_gb\": round(self.max_memory_gb, 1),\n                \"allocation_percent\": round((self.max_memory_gb / total_gb) * 100, 1),\n                \"detection_method\": \"auto\" if was_auto_detected else \"manual_override\",\n            },\n        )\n\n    @classmethod\n    def _auto_detect_memory_limit(cls) -&gt; float:\n        \"\"\"\n        Auto-detect appropriate memory limit based on system RAM.\n\n        Uses tiered allocation strategy:\n        - \u226532GB systems: 40% of total RAM (12-16GB)\n        - \u226516GB systems: 30% of total RAM (5-8GB)\n        - \u22658GB systems: 25% of total RAM (2-4GB)\n        - &lt;8GB systems: 20% of total RAM (conservative)\n\n        Returns:\n            float: Recommended memory limit in GB\n        \"\"\"\n        system_memory = psutil.virtual_memory()\n        total_gb = system_memory.total / 1024**3\n\n        if total_gb &gt;= 32:  # High-memory system\n            return total_gb * 0.4\n        elif total_gb &gt;= 16:  # Standard system\n            return total_gb * 0.3\n        elif total_gb &gt;= 8:  # Lower-memory system\n            return total_gb * 0.25\n        else:  # Very constrained\n            return total_gb * 0.2\n\n    def get_current_memory_usage(self) -&gt; Dict:\n        \"\"\"Get comprehensive current memory statistics.\"\"\"\n        memory_info = self.process.memory_info()\n        system_memory = psutil.virtual_memory()\n\n        current_rss = memory_info.rss\n        current_vms = memory_info.vms\n\n        usage_stats = {\n            \"rss_bytes\": current_rss,\n            \"vms_bytes\": current_vms,\n            \"rss_mb\": current_rss / 1024**2,\n            \"vms_mb\": current_vms / 1024**2,\n            \"rss_gb\": current_rss / 1024**3,\n            \"system_available_gb\": system_memory.available / 1024**3,\n            \"system_used_percent\": system_memory.percent,\n            \"process_memory_percent\": (current_rss / self.max_memory_bytes) * 100,\n            \"pressure_level\": self.get_memory_pressure_level().value,\n        }\n\n        # Add to history for trend analysis\n        self.memory_history.append(\n            {\n                \"timestamp\": time.time(),\n                \"rss_bytes\": current_rss,\n                \"pressure_level\": usage_stats[\"pressure_level\"],\n            }\n        )\n\n        # Maintain history size\n        if len(self.memory_history) &gt; self.max_history_size:\n            self.memory_history.pop(0)\n\n        return usage_stats\n\n    def get_memory_pressure_level(self) -&gt; MemoryPressureLevel:\n        \"\"\"Determine current memory pressure level.\"\"\"\n        current_usage = self.process.memory_info().rss\n        usage_ratio = current_usage / self.max_memory_bytes\n\n        if usage_ratio &gt;= self.thresholds[MemoryPressureLevel.CRITICAL]:\n            return MemoryPressureLevel.CRITICAL\n        elif usage_ratio &gt;= self.thresholds[MemoryPressureLevel.HIGH]:\n            return MemoryPressureLevel.HIGH\n        elif usage_ratio &gt;= self.thresholds[MemoryPressureLevel.MEDIUM]:\n            return MemoryPressureLevel.MEDIUM\n        else:\n            return MemoryPressureLevel.LOW\n\n    def calculate_adaptive_chunk_size(\n        self, base_chunk_size: int, operation_type: str\n    ) -&gt; int:\n        \"\"\"Calculate optimal chunk size based on current memory pressure.\"\"\"\n        pressure_level = self.get_memory_pressure_level()\n        adjustment_factor = self.chunk_size_factors[pressure_level]\n\n        # Operation-specific base adjustments\n        operation_factors = {\n            \"tokenization\": 1.0,\n            \"ngram_generation\": 0.6,  # More memory intensive\n            \"unique_extraction\": 1.2,\n            \"join_operations\": 0.8,\n        }\n\n        operation_factor = operation_factors.get(operation_type, 1.0)\n        adjusted_size = int(base_chunk_size * adjustment_factor * operation_factor)\n\n        # Ensure minimum viable chunk size\n        min_chunk_size = max(1000, base_chunk_size // 10)\n        return max(adjusted_size, min_chunk_size)\n\n    def should_trigger_gc(self, force_threshold: float = 0.7) -&gt; bool:\n        \"\"\"Determine if garbage collection should be triggered.\"\"\"\n        current_usage = self.process.memory_info().rss\n        usage_ratio = current_usage / self.max_memory_bytes\n\n        return usage_ratio &gt;= force_threshold\n\n    def enhanced_gc_cleanup(self) -&gt; Dict:\n        \"\"\"Perform comprehensive garbage collection with metrics.\"\"\"\n        memory_before = self.get_current_memory_usage()\n\n        # Multiple GC passes for thorough cleanup\n        for i in range(3):\n            collected = gc.collect()\n            if collected == 0:\n                break\n\n        memory_after = self.get_current_memory_usage()\n\n        cleanup_stats = {\n            \"memory_freed_mb\": (memory_before[\"rss_mb\"] - memory_after[\"rss_mb\"]),\n            \"memory_before_mb\": memory_before[\"rss_mb\"],\n            \"memory_after_mb\": memory_after[\"rss_mb\"],\n            \"pressure_before\": memory_before[\"pressure_level\"],\n            \"pressure_after\": memory_after[\"pressure_level\"],\n        }\n\n        self.logger.debug(\n            \"Memory cleanup completed\",\n            extra={\n                \"memory_freed_mb\": cleanup_stats[\"memory_freed_mb\"],\n                \"memory_before_mb\": cleanup_stats[\"memory_before_mb\"],\n                \"memory_after_mb\": cleanup_stats[\"memory_after_mb\"],\n                \"pressure_before\": cleanup_stats[\"pressure_before\"],\n                \"pressure_after\": cleanup_stats[\"pressure_after\"],\n            },\n        )\n        return cleanup_stats\n\n    def get_memory_trend(self) -&gt; str:\n        \"\"\"Analyze recent memory usage trend.\"\"\"\n        if len(self.memory_history) &lt; 5:\n            return \"insufficient_data\"\n\n        recent_usage = [entry[\"rss_bytes\"] for entry in self.memory_history[-5:]]\n\n        if all(\n            recent_usage[i] &lt;= recent_usage[i + 1] for i in range(len(recent_usage) - 1)\n        ):\n            return \"increasing\"\n        elif all(\n            recent_usage[i] &gt;= recent_usage[i + 1] for i in range(len(recent_usage) - 1)\n        ):\n            return \"decreasing\"\n        else:\n            return \"stable\"\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.calculate_adaptive_chunk_size","title":"<code>calculate_adaptive_chunk_size(base_chunk_size, operation_type)</code>","text":"<p>Calculate optimal chunk size based on current memory pressure.</p> Source code in <code>app/utils.py</code> <pre><code>def calculate_adaptive_chunk_size(\n    self, base_chunk_size: int, operation_type: str\n) -&gt; int:\n    \"\"\"Calculate optimal chunk size based on current memory pressure.\"\"\"\n    pressure_level = self.get_memory_pressure_level()\n    adjustment_factor = self.chunk_size_factors[pressure_level]\n\n    # Operation-specific base adjustments\n    operation_factors = {\n        \"tokenization\": 1.0,\n        \"ngram_generation\": 0.6,  # More memory intensive\n        \"unique_extraction\": 1.2,\n        \"join_operations\": 0.8,\n    }\n\n    operation_factor = operation_factors.get(operation_type, 1.0)\n    adjusted_size = int(base_chunk_size * adjustment_factor * operation_factor)\n\n    # Ensure minimum viable chunk size\n    min_chunk_size = max(1000, base_chunk_size // 10)\n    return max(adjusted_size, min_chunk_size)\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.enhanced_gc_cleanup","title":"<code>enhanced_gc_cleanup()</code>","text":"<p>Perform comprehensive garbage collection with metrics.</p> Source code in <code>app/utils.py</code> <pre><code>def enhanced_gc_cleanup(self) -&gt; Dict:\n    \"\"\"Perform comprehensive garbage collection with metrics.\"\"\"\n    memory_before = self.get_current_memory_usage()\n\n    # Multiple GC passes for thorough cleanup\n    for i in range(3):\n        collected = gc.collect()\n        if collected == 0:\n            break\n\n    memory_after = self.get_current_memory_usage()\n\n    cleanup_stats = {\n        \"memory_freed_mb\": (memory_before[\"rss_mb\"] - memory_after[\"rss_mb\"]),\n        \"memory_before_mb\": memory_before[\"rss_mb\"],\n        \"memory_after_mb\": memory_after[\"rss_mb\"],\n        \"pressure_before\": memory_before[\"pressure_level\"],\n        \"pressure_after\": memory_after[\"pressure_level\"],\n    }\n\n    self.logger.debug(\n        \"Memory cleanup completed\",\n        extra={\n            \"memory_freed_mb\": cleanup_stats[\"memory_freed_mb\"],\n            \"memory_before_mb\": cleanup_stats[\"memory_before_mb\"],\n            \"memory_after_mb\": cleanup_stats[\"memory_after_mb\"],\n            \"pressure_before\": cleanup_stats[\"pressure_before\"],\n            \"pressure_after\": cleanup_stats[\"pressure_after\"],\n        },\n    )\n    return cleanup_stats\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.get_current_memory_usage","title":"<code>get_current_memory_usage()</code>","text":"<p>Get comprehensive current memory statistics.</p> Source code in <code>app/utils.py</code> <pre><code>def get_current_memory_usage(self) -&gt; Dict:\n    \"\"\"Get comprehensive current memory statistics.\"\"\"\n    memory_info = self.process.memory_info()\n    system_memory = psutil.virtual_memory()\n\n    current_rss = memory_info.rss\n    current_vms = memory_info.vms\n\n    usage_stats = {\n        \"rss_bytes\": current_rss,\n        \"vms_bytes\": current_vms,\n        \"rss_mb\": current_rss / 1024**2,\n        \"vms_mb\": current_vms / 1024**2,\n        \"rss_gb\": current_rss / 1024**3,\n        \"system_available_gb\": system_memory.available / 1024**3,\n        \"system_used_percent\": system_memory.percent,\n        \"process_memory_percent\": (current_rss / self.max_memory_bytes) * 100,\n        \"pressure_level\": self.get_memory_pressure_level().value,\n    }\n\n    # Add to history for trend analysis\n    self.memory_history.append(\n        {\n            \"timestamp\": time.time(),\n            \"rss_bytes\": current_rss,\n            \"pressure_level\": usage_stats[\"pressure_level\"],\n        }\n    )\n\n    # Maintain history size\n    if len(self.memory_history) &gt; self.max_history_size:\n        self.memory_history.pop(0)\n\n    return usage_stats\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.get_memory_pressure_level","title":"<code>get_memory_pressure_level()</code>","text":"<p>Determine current memory pressure level.</p> Source code in <code>app/utils.py</code> <pre><code>def get_memory_pressure_level(self) -&gt; MemoryPressureLevel:\n    \"\"\"Determine current memory pressure level.\"\"\"\n    current_usage = self.process.memory_info().rss\n    usage_ratio = current_usage / self.max_memory_bytes\n\n    if usage_ratio &gt;= self.thresholds[MemoryPressureLevel.CRITICAL]:\n        return MemoryPressureLevel.CRITICAL\n    elif usage_ratio &gt;= self.thresholds[MemoryPressureLevel.HIGH]:\n        return MemoryPressureLevel.HIGH\n    elif usage_ratio &gt;= self.thresholds[MemoryPressureLevel.MEDIUM]:\n        return MemoryPressureLevel.MEDIUM\n    else:\n        return MemoryPressureLevel.LOW\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.get_memory_trend","title":"<code>get_memory_trend()</code>","text":"<p>Analyze recent memory usage trend.</p> Source code in <code>app/utils.py</code> <pre><code>def get_memory_trend(self) -&gt; str:\n    \"\"\"Analyze recent memory usage trend.\"\"\"\n    if len(self.memory_history) &lt; 5:\n        return \"insufficient_data\"\n\n    recent_usage = [entry[\"rss_bytes\"] for entry in self.memory_history[-5:]]\n\n    if all(\n        recent_usage[i] &lt;= recent_usage[i + 1] for i in range(len(recent_usage) - 1)\n    ):\n        return \"increasing\"\n    elif all(\n        recent_usage[i] &gt;= recent_usage[i + 1] for i in range(len(recent_usage) - 1)\n    ):\n        return \"decreasing\"\n    else:\n        return \"stable\"\n</code></pre>"},{"location":"reference/app/#app.utils.MemoryManager.should_trigger_gc","title":"<code>should_trigger_gc(force_threshold=0.7)</code>","text":"<p>Determine if garbage collection should be triggered.</p> Source code in <code>app/utils.py</code> <pre><code>def should_trigger_gc(self, force_threshold: float = 0.7) -&gt; bool:\n    \"\"\"Determine if garbage collection should be triggered.\"\"\"\n    current_usage = self.process.memory_info().rss\n    usage_ratio = current_usage / self.max_memory_bytes\n\n    return usage_ratio &gt;= force_threshold\n</code></pre>"},{"location":"reference/app/#app.utils.is_space_separated","title":"<code>is_space_separated(text)</code>","text":"<p>Determine if text uses space-separated tokenization or character-based tokenization.</p> <p>Uses Unicode script detection to identify if text primarily contains scripts that use spaces for word separation (Latin, Cyrillic, Arabic, etc.) vs. scripts that don't use spaces (Chinese, Japanese, Thai, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Expr]</code> <p>Input text string or polars expression</p> required <p>Returns:</p> Type Description <code>Union[bool, Expr]</code> <p>Boolean or polars expression indicating if text is space-separated</p> Source code in <code>app/utils.py</code> <pre><code>def is_space_separated(text: Union[str, pl.Expr]) -&gt; Union[bool, pl.Expr]:\n    \"\"\"\n    Determine if text uses space-separated tokenization or character-based tokenization.\n\n    Uses Unicode script detection to identify if text primarily contains scripts\n    that use spaces for word separation (Latin, Cyrillic, Arabic, etc.) vs.\n    scripts that don't use spaces (Chinese, Japanese, Thai, etc.).\n\n    Args:\n        text: Input text string or polars expression\n\n    Returns:\n        Boolean or polars expression indicating if text is space-separated\n    \"\"\"\n    if isinstance(text, str):\n        # For direct string input, use Python regex\n        if not text.strip():\n            return True  # Empty text defaults to space-separated\n\n        if UNICODE_SUPPORT:\n            # Use regex module with Unicode property support\n            space_separated_chars = len(\n                regex.findall(\n                    r\"[\\p{Latin}\\p{Cyrillic}\\p{Arabic}\\p{Armenian}\\p{Georgian}\\p{Greek}\\p{Hebrew}\\p{Hangul}]\",\n                    text,\n                )\n            )\n            non_space_chars = len(\n                regex.findall(\n                    r\"[\\p{Han}\\p{Hiragana}\\p{Katakana}\\p{Thai}\\p{Lao}\\p{Myanmar}\\p{Khmer}]\",\n                    text,\n                )\n            )\n        else:\n            # Fallback to Unicode ranges\n            # Latin: U+0000-U+024F, U+1E00-U+1EFF\n            # Cyrillic: U+0400-U+04FF, U+0500-U+052F\n            # Arabic: U+0600-U+06FF, U+0750-U+077F\n            # Greek: U+0370-U+03FF\n            # Hebrew: U+0590-U+05FF\n            space_separated_pattern = r\"[\\u0000-\\u024F\\u1E00-\\u1EFF\\u0400-\\u04FF\\u0500-\\u052F\\u0600-\\u06FF\\u0750-\\u077F\\u0370-\\u03FF\\u0590-\\u05FF\\uAC00-\\uD7AF]\"\n\n            # CJK: U+4E00-U+9FFF (Han), U+3040-U+309F (Hiragana), U+30A0-U+30FF (Katakana)\n            # Thai: U+0E00-U+0E7F\n            # Myanmar: U+1000-U+109F\n            non_space_pattern = (\n                r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF\\u0E00-\\u0E7F\\u1000-\\u109F]\"\n            )\n\n            space_separated_chars = len(re.findall(space_separated_pattern, text))\n            non_space_chars = len(re.findall(non_space_pattern, text))\n\n        # If we have any characters, determine majority script type\n        total_script_chars = space_separated_chars + non_space_chars\n        if total_script_chars == 0:\n            return True  # No script-specific characters, default to space-separated\n\n        # Space-separated if majority of script characters are from space-separated scripts\n        return space_separated_chars &gt;= non_space_chars\n\n    else:\n        # For polars expressions, use Unicode ranges (more compatible)\n        # Space-separated scripts pattern\n        space_separated_pattern = r\"[\\u0000-\\u024F\\u1E00-\\u1EFF\\u0400-\\u04FF\\u0500-\\u052F\\u0600-\\u06FF\\u0750-\\u077F\\u0370-\\u03FF\\u0590-\\u05FF\\uAC00-\\uD7AF]\"\n        # Non-space scripts pattern\n        non_space_pattern = (\n            r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF\\u0E00-\\u0E7F\\u1000-\\u109F]\"\n        )\n\n        return text.str.count_matches(\n            space_separated_pattern\n        ) &gt;= text.str.count_matches(non_space_pattern)\n</code></pre>"},{"location":"reference/app/#app.utils.is_space_separated(text)","title":"<code>text</code>","text":""},{"location":"reference/app/#app.utils.parquet_row_count","title":"<code>parquet_row_count(filename)</code>","text":"<p>Get the number of rows in a parquet file efficiently.</p> Source code in <code>app/utils.py</code> <pre><code>def parquet_row_count(filename: str) -&gt; int:\n    \"\"\"Get the number of rows in a parquet file efficiently.\"\"\"\n    with pq.ParquetFile(filename) as pf:\n        return pf.metadata.num_rows\n</code></pre>"},{"location":"reference/app/#app.utils.tokenize_text","title":"<code>tokenize_text(ldf, text_column, progress_manager=None, memory_manager=None)</code>","text":"<p>Memory-efficient tokenization engine with adaptive memory management.</p> <p>Enhanced features: - Real-time memory monitoring during processing - Dynamic chunk size adjustment based on memory pressure - Mid-process memory monitoring and adaptation - Graceful fallback to smaller chunks when memory pressure increases - Progress reporting with memory statistics</p> <p>Parameters:</p> Name Type Description Default <code>LazyFrame</code> <p>Input LazyFrame containing text data</p> required <code>str</code> <p>Name of the column containing text to tokenize</p> required <code>Optional[ProgressManager]</code> <p>Optional progress manager for detailed tokenization progress reporting</p> <code>None</code> <code>Optional[MemoryManager]</code> <p>Optional MemoryManager for adaptive processing</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>LazyFrame with additional 'tokens' column containing list of tokens</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_column does not exist in the LazyFrame</p> <code>TypeError</code> <p>If input is not a polars LazyFrame</p> <code>MemoryError</code> <p>If processing fails even with minimum chunk sizes</p> Source code in <code>app/utils.py</code> <pre><code>def tokenize_text(\n    ldf: pl.LazyFrame,\n    text_column: str,\n    progress_manager: Optional[\"ProgressManager\"] = None,\n    memory_manager: Optional[MemoryManager] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Memory-efficient tokenization engine with adaptive memory management.\n\n    Enhanced features:\n    - Real-time memory monitoring during processing\n    - Dynamic chunk size adjustment based on memory pressure\n    - Mid-process memory monitoring and adaptation\n    - Graceful fallback to smaller chunks when memory pressure increases\n    - Progress reporting with memory statistics\n\n    Args:\n        ldf: Input LazyFrame containing text data\n        text_column: Name of the column containing text to tokenize\n        progress_manager: Optional progress manager for detailed tokenization progress reporting\n        memory_manager: Optional MemoryManager for adaptive processing\n\n    Returns:\n        LazyFrame with additional 'tokens' column containing list of tokens\n\n    Raises:\n        ValueError: If text_column does not exist in the LazyFrame\n        TypeError: If input is not a polars LazyFrame\n        MemoryError: If processing fails even with minimum chunk sizes\n    \"\"\"\n    # Input validation\n    if not isinstance(ldf, pl.LazyFrame):\n        raise TypeError(f\"Expected polars LazyFrame, got {type(ldf)}\")\n\n    if not isinstance(text_column, str):\n        raise TypeError(f\"text_column must be a string, got {type(text_column)}\")\n\n    # No validation needed for progress_manager - it's expected to be a progress manager instance or None\n\n    # Create memory manager if not provided\n    if memory_manager is None:\n        memory_manager = MemoryManager(max_memory_gb=4.0, process_name=\"tokenizer\")\n\n    # Log tokenization start\n    logger.info(\n        \"Starting text tokenization\",\n        extra={\n            \"text_column\": text_column,\n            \"has_progress_manager\": progress_manager is not None,\n            \"memory_manager_provided\": memory_manager is not None,\n        },\n    )\n\n    # Check if column exists by trying to reference it\n    try:\n        # This will validate that the column exists when the lazy frame is executed\n        test_col = pl.col(text_column)\n    except Exception as e:\n        raise ValueError(f\"Invalid column name '{text_column}': {e}\")\n\n    # Define the comprehensive tokenization regex pattern\n    # Order is critical for proper matching precedence\n    token_pattern = \"|\".join(\n        [\n            r\"[Hh][Tt][Tt][Pp][Ss]?://[a-zA-Z0-9._~:/?#@!$&amp;'()*+,;=\\-]+\",  # URLs (case insensitive HTTP/HTTPS)\n            r\"@\\w+\",  # @mentions\n            r\"#\\w+\",  # #hashtags\n            r\"[a-zA-Z\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]{2,}[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]+\",  # Mixed Latin+CJK (Latin part 2+ chars)\n            r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]+[a-zA-Z\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]+\",  # CJK-Latin-CJK (requires Latin chars)\n            r\"[\\uAC00-\\uD7AF]+\",  # Korean words (Hangul)\n            r\"[\\u0400-\\u04FF\\u0500-\\u052F]+\",  # Cyrillic words\n            r\"[a-zA-Z\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF][a-zA-Z0-9\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF.!?,;:()'\\\"\\\\-]*\",  # Latin words with accented chars and punctuation\n            r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]\",  # Individual CJK characters\n            (\n                r\"[^\\s\\p{P}\\p{Sm}\\p{Sc}]\"\n                if UNICODE_SUPPORT\n                else r\"[a-zA-Z0-9\\u00C0-\\u9FFF\\uAC00-\\uD7AF\\u0400-\\u052F]\"\n            ),  # Any other non-whitespace excluding punctuation and math/currency symbols\n        ]\n    )\n\n    def _tokenize_chunk(chunk_ldf: pl.LazyFrame) -&gt; pl.LazyFrame:\n        \"\"\"Apply tokenization to a chunk of data.\"\"\"\n        return (\n            chunk_ldf.with_columns(\n                [\n                    # Step 1: Normalize whitespace and handle empty strings\n                    pl.col(text_column)\n                    .str.strip_chars()\n                    .str.replace_all(\n                        r\"\\s+\", \" \"\n                    )  # Normalize multiple whitespace to single space\n                    .alias(\"_normalized_text\")\n                ]\n            )\n            .with_columns(\n                [\n                    # Step 2: Conditional tokenization based on language type\n                    # For space-separated languages, split by spaces first then handle special patterns\n                    # For non-space languages (CJK), use character-level splitting with entity preservation\n                    pl.when(is_space_separated(pl.col(\"_normalized_text\")))\n                    .then(\n                        # Space-separated language processing\n                        pl.col(\"_normalized_text\").str.extract_all(token_pattern)\n                    )\n                    .otherwise(\n                        # Non-space language processing: preserve entities, split characters\n                        pl.col(\"_normalized_text\").str.extract_all(\n                            \"|\".join(\n                                [\n                                    r\"[Hh][Tt][Tt][Pp][Ss]?://[a-zA-Z0-9._~:/?#@!$&amp;'()*+,;=\\-]+\",  # URLs\n                                    r\"@\\w+\",  # @mentions\n                                    r\"#\\w+\",  # #hashtags\n                                    r\"[a-zA-Z\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+\",  # Pure Latin sequences with accented chars\n                                    r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]\",  # Individual CJK characters\n                                    (\n                                        r\"[^\\s\\p{P}\\p{Sm}\\p{Sc}]\"\n                                        if UNICODE_SUPPORT\n                                        else r\"[a-zA-Z0-9\\u00C0-\\u9FFF\\uAC00-\\uD7AF\\u0400-\\u052F]\"\n                                    ),  # Any other non-whitespace excluding punctuation and math/currency symbols\n                                ]\n                            )\n                        )\n                    )\n                    .alias(\"_raw_tokens\")\n                ]\n            )\n            .with_columns(\n                [\n                    # Step 3: Process tokens (normalize case, handle social media entities)\n                    pl.col(\"_raw_tokens\")\n                    .list.eval(\n                        pl.when(\n                            # Social media entities: keep as-is (case preserved for URLs)\n                            pl.element().str.contains(\n                                r\"^([Hh][Tt][Tt][Pp][Ss]?://|@|#)\"\n                            )\n                        )\n                        .then(pl.element())\n                        .when(\n                            # Mixed scripts (e.g., \"iPhone\u7528\u6237\"): keep as single token but lowercase\n                            pl.element().str.contains(r\"[a-zA-Z]\")\n                            &amp; pl.element().str.contains(\n                                r\"[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]\"\n                            )\n                        )\n                        .then(pl.element().str.to_lowercase())\n                        .otherwise(pl.element().str.to_lowercase())\n                    )\n                    .alias(\"tokens\")\n                ]\n            )\n            .with_columns(\n                [\n                    # Step 4: Filter out empty tokens and whitespace-only tokens\n                    pl.col(\"tokens\")\n                    .list.eval(\n                        pl.element().filter(\n                            (pl.element().str.len_chars() &gt; 0)\n                            &amp; (pl.element().str.strip_chars() != \"\")\n                        )\n                    )\n                    .alias(\"tokens\")\n                ]\n            )\n            .drop([\"_normalized_text\", \"_raw_tokens\"])\n        )\n\n    # Memory-efficient row counting with minimal footprint\n    def _get_dataset_size():\n        \"\"\"Get dataset size with minimal memory usage, return None if not possible.\"\"\"\n        try:\n            # Primary method: Use count aggregation - most memory efficient\n            return ldf.select(pl.len()).collect().item()\n        except Exception:\n            try:\n                # Secondary method: Try with height property if available\n                # Some lazy frames might support this more efficiently\n                return ldf.select(pl.count()).collect().item()\n            except Exception:\n                try:\n                    # Tertiary method: Use sample-based estimation for problematic cases\n                    # This is a fallback for very problematic data sources\n                    initial_chunk_size = memory_manager.calculate_adaptive_chunk_size(\n                        50000, \"tokenization\"\n                    )\n                    sample_size = min(1000, initial_chunk_size // 10)\n                    sample_df = ldf.limit(sample_size).collect()\n                    if len(sample_df) == 0:\n                        return 0\n                    elif len(sample_df) &lt; sample_size:\n                        # We got less than requested, likely end of data\n                        return len(sample_df)\n                    else:\n                        # Cannot determine size efficiently - will use streaming\n                        return None\n                except Exception:\n                    # Complete fallback - cannot determine size\n                    return None\n\n    total_rows = _get_dataset_size()\n\n    logger.debug(\n        \"Dataset size determined\",\n        extra={\n            \"total_rows\": total_rows,\n            \"size_determination_method\": (\n                \"count_aggregation\" if total_rows is not None else \"unknown\"\n            ),\n        },\n    )\n\n    # Handle empty dataset efficiently\n    if total_rows == 0:\n        logger.info(\n            \"Empty dataset detected, returning empty tokens\", extra={\"total_rows\": 0}\n        )\n        return ldf.with_columns([pl.lit([]).alias(\"tokens\")])\n\n    # Calculate initial adaptive chunk size based on memory pressure\n    initial_chunk_size = 50000\n    adaptive_chunk_size = memory_manager.calculate_adaptive_chunk_size(\n        initial_chunk_size, \"tokenization\"\n    )\n\n    logger.debug(\n        \"Adaptive chunk size calculated\",\n        extra={\n            \"initial_chunk_size\": initial_chunk_size,\n            \"adaptive_chunk_size\": adaptive_chunk_size,\n            \"memory_pressure\": memory_manager.get_memory_pressure_level().value,\n        },\n    )\n\n    # If dataset is small, check if we should process without chunking\n    if total_rows is not None and total_rows &lt;= adaptive_chunk_size:\n        # Small dataset - process normally with memory monitoring\n        logger.info(\n            \"Processing small dataset without chunking\",\n            extra={\n                \"total_rows\": total_rows,\n                \"adaptive_chunk_size\": adaptive_chunk_size,\n                \"processing_mode\": \"single_chunk\",\n            },\n        )\n\n        memory_before = memory_manager.get_current_memory_usage()\n        result = _tokenize_chunk(ldf)\n        memory_after = memory_manager.get_current_memory_usage()\n\n        # Log memory usage for small datasets\n        memory_used = memory_after[\"rss_mb\"] - memory_before[\"rss_mb\"]\n        logger.debug(\n            \"Small dataset tokenization completed\",\n            extra={\n                \"total_rows\": total_rows,\n                \"memory_used_mb\": memory_used,\n                \"memory_before_mb\": memory_before[\"rss_mb\"],\n                \"memory_after_mb\": memory_after[\"rss_mb\"],\n            },\n        )\n\n        return result\n\n    # For large datasets or unknown sizes, use memory-adaptive chunked processing\n    try:\n        if total_rows is not None:\n            # Known size approach - adaptive chunking with memory monitoring\n            logger.info(\n                \"Starting chunked tokenization for large dataset\",\n                extra={\n                    \"total_rows\": total_rows,\n                    \"initial_chunk_size\": adaptive_chunk_size,\n                    \"processing_mode\": \"known_size_chunking\",\n                },\n            )\n\n            chunk_lazyframes = []\n            current_chunk_size = adaptive_chunk_size\n            processed_rows = 0\n\n            # Set up progress manager with estimated total chunks\n            if progress_manager:\n                estimated_total_chunks = (\n                    total_rows + adaptive_chunk_size - 1\n                ) // adaptive_chunk_size\n                progress_manager.add_substep(\n                    \"tokenize\",\n                    \"tokenize_chunks\",\n                    \"Processing tokenization chunks\",\n                    estimated_total_chunks,\n                )\n                progress_manager.start_substep(\"tokenize\", \"tokenize_chunks\")\n\n            while processed_rows &lt; total_rows:\n                # Check memory pressure and adjust chunk size if needed\n                pressure_level = memory_manager.get_memory_pressure_level()\n\n                if pressure_level == MemoryPressureLevel.CRITICAL:\n                    # Reduce chunk size dramatically for critical pressure\n                    old_chunk_size = current_chunk_size\n                    current_chunk_size = max(1000, current_chunk_size // 4)\n                    logger.warning(\n                        \"Critical memory pressure - reducing chunk size dramatically\",\n                        extra={\n                            \"pressure_level\": \"CRITICAL\",\n                            \"old_chunk_size\": old_chunk_size,\n                            \"new_chunk_size\": current_chunk_size,\n                            \"processed_rows\": processed_rows,\n                        },\n                    )\n                elif pressure_level == MemoryPressureLevel.HIGH:\n                    # Reduce chunk size moderately for high pressure\n                    old_chunk_size = current_chunk_size\n                    current_chunk_size = max(5000, current_chunk_size // 2)\n                    logger.warning(\n                        \"High memory pressure - reducing chunk size\",\n                        extra={\n                            \"pressure_level\": \"HIGH\",\n                            \"old_chunk_size\": old_chunk_size,\n                            \"new_chunk_size\": current_chunk_size,\n                            \"processed_rows\": processed_rows,\n                        },\n                    )\n\n                # Calculate actual chunk size for this iteration\n                remaining_rows = total_rows - processed_rows\n                actual_chunk_size = min(current_chunk_size, remaining_rows)\n\n                # Process chunk with memory monitoring\n                chunk_ldf = ldf.slice(processed_rows, actual_chunk_size)\n\n                try:\n                    processed_chunk_ldf = _tokenize_chunk(chunk_ldf)\n                    chunk_lazyframes.append(processed_chunk_ldf)\n\n                    processed_rows += actual_chunk_size\n\n                    # Report progress with current chunk number\n                    if progress_manager:\n                        chunk_num = len(chunk_lazyframes)\n                        try:\n                            progress_manager.update_substep(\n                                \"tokenize\", \"tokenize_chunks\", chunk_num\n                            )\n                        except Exception as e:\n                            logger.warning(\n                                \"Progress update failed during tokenization\",\n                                extra={\n                                    \"chunk_num\": chunk_num,\n                                    \"processed_rows\": processed_rows,\n                                    \"error\": str(e),\n                                },\n                            )\n\n                    # Force garbage collection after each chunk in high memory pressure\n                    if pressure_level in [\n                        MemoryPressureLevel.HIGH,\n                        MemoryPressureLevel.CRITICAL,\n                    ]:\n                        cleanup_stats = memory_manager.enhanced_gc_cleanup()\n                        if cleanup_stats[\"memory_freed_mb\"] &gt; 20:\n                            logger.debug(\n                                \"Significant memory freed after tokenization chunk\",\n                                extra={\n                                    \"memory_freed_mb\": cleanup_stats[\"memory_freed_mb\"],\n                                    \"pressure_level\": pressure_level.value,\n                                    \"chunk_number\": len(chunk_lazyframes),\n                                },\n                            )\n\n                except MemoryError as e:\n                    # Emergency fallback - reduce chunk size dramatically and retry\n                    if current_chunk_size &gt; 1000:\n                        old_chunk_size = current_chunk_size\n                        current_chunk_size = max(500, current_chunk_size // 8)\n                        logger.error(\n                            \"Memory error in tokenization - emergency chunk size reduction\",\n                            extra={\n                                \"old_chunk_size\": old_chunk_size,\n                                \"new_chunk_size\": current_chunk_size,\n                                \"processed_rows\": processed_rows,\n                                \"error\": str(e),\n                            },\n                        )\n                        continue\n                    else:\n                        # Even minimum chunk size failed - this is a critical error\n                        logger.critical(\n                            \"Cannot process even minimal chunks during tokenization\",\n                            extra={\n                                \"chunk_size\": current_chunk_size,\n                                \"processed_rows\": processed_rows,\n                                \"error\": str(e),\n                            },\n                        )\n                        raise MemoryError(\n                            f\"Cannot process even minimal chunks during tokenization: {e}\"\n                        ) from e\n\n            # Return concatenated results\n            if not chunk_lazyframes:\n                logger.warning(\n                    \"No chunks processed successfully in known-size tokenization\"\n                )\n                # Complete progress step even if no chunks processed\n                if progress_manager:\n                    progress_manager.complete_substep(\"tokenize\", \"tokenize_chunks\")\n                return ldf.with_columns([pl.lit([]).alias(\"tokens\")])\n\n            logger.info(\n                \"Chunked tokenization completed successfully\",\n                extra={\n                    \"total_chunks_processed\": len(chunk_lazyframes),\n                    \"total_rows_processed\": processed_rows,\n                    \"final_chunk_size\": current_chunk_size,\n                },\n            )\n\n            # Complete progress step on success\n            if progress_manager:\n                progress_manager.complete_substep(\"tokenize\", \"tokenize_chunks\")\n\n            return pl.concat(chunk_lazyframes)\n\n        else:\n            # Unknown size - streaming approach with memory-aware chunk sizing\n            logger.info(\n                \"Starting streaming tokenization for unknown-size dataset\",\n                extra={\n                    \"initial_chunk_size\": adaptive_chunk_size,\n                    \"processing_mode\": \"streaming_unknown_size\",\n                },\n            )\n\n            chunk_lazyframes = []\n            chunk_idx = 0\n            estimated_chunks = 10  # Start with conservative estimate\n            consecutive_empty_chunks = 0\n            max_empty_chunks = 3  # Stop after this many consecutive empty chunks\n            current_chunk_size = adaptive_chunk_size\n\n            # Set up progress manager for streaming with initial estimate\n            if progress_manager:\n                progress_manager.add_substep(\n                    \"tokenize\",\n                    \"stream_tokenize\",\n                    \"Streaming tokenization chunks\",\n                    estimated_chunks,\n                )\n                progress_manager.start_substep(\"tokenize\", \"stream_tokenize\")\n\n            while consecutive_empty_chunks &lt; max_empty_chunks:\n                # Check memory pressure and adjust chunk size\n                pressure_level = memory_manager.get_memory_pressure_level()\n\n                if pressure_level == MemoryPressureLevel.CRITICAL:\n                    current_chunk_size = max(1000, current_chunk_size // 4)\n                elif pressure_level == MemoryPressureLevel.HIGH:\n                    current_chunk_size = max(5000, current_chunk_size // 2)\n\n                start_idx = chunk_idx * current_chunk_size\n                chunk_ldf = ldf.slice(start_idx, current_chunk_size)\n\n                try:\n                    # More efficient emptiness check using lazy operations\n                    processed_chunk_ldf = _tokenize_chunk(chunk_ldf)\n\n                    # Use lazy operations to check if chunk has data\n                    chunk_has_data_check = processed_chunk_ldf.select(pl.len()).limit(1)\n\n                    try:\n                        chunk_len = chunk_has_data_check.collect().item()\n\n                        if chunk_len == 0:\n                            consecutive_empty_chunks += 1\n                            chunk_idx += 1\n                            continue\n                        else:\n                            consecutive_empty_chunks = 0  # Reset counter\n\n                    except Exception:\n                        # If we can't determine chunk size, assume it's empty\n                        consecutive_empty_chunks += 1\n                        chunk_idx += 1\n                        continue\n\n                    # Add non-empty chunk to results\n                    chunk_lazyframes.append(processed_chunk_ldf)\n\n                    # Update progress estimate dynamically\n                    chunk_idx += 1\n                    if chunk_idx &gt; estimated_chunks:\n                        estimated_chunks = chunk_idx + 10  # Increase estimate\n                        # Update progress step total with new estimate\n                        if progress_manager:\n                            try:\n                                # Note: ProgressManager might not support updating totals,\n                                # but we can try or just update current progress\n                                progress_manager.update_substep(\n                                    \"tokenize\", \"stream_tokenize\", chunk_idx\n                                )\n                            except Exception as e:\n                                logger.debug(\n                                    \"Progress total update failed\",\n                                    extra={\"error\": str(e)},\n                                )\n\n                    # Report progress with current chunk\n                    if progress_manager:\n                        try:\n                            progress_manager.update_substep(\n                                \"tokenize\", \"stream_tokenize\", chunk_idx\n                            )\n                        except Exception as e:\n                            logger.warning(\n                                \"Progress update failed during streaming tokenization\",\n                                extra={\n                                    \"chunk_idx\": chunk_idx,\n                                    \"estimated_chunks\": estimated_chunks,\n                                    \"error\": str(e),\n                                },\n                            )\n\n                    # Force garbage collection in high memory pressure\n                    if pressure_level in [\n                        MemoryPressureLevel.HIGH,\n                        MemoryPressureLevel.CRITICAL,\n                    ]:\n                        cleanup_stats = memory_manager.enhanced_gc_cleanup()\n                        if cleanup_stats[\"memory_freed_mb\"] &gt; 20:\n                            logger.debug(\n                                \"Significant memory freed after streaming tokenization chunk\",\n                                extra={\n                                    \"memory_freed_mb\": cleanup_stats[\"memory_freed_mb\"],\n                                    \"pressure_level\": pressure_level.value,\n                                    \"chunk_index\": chunk_idx,\n                                },\n                            )\n\n                except MemoryError as e:\n                    # Emergency fallback - reduce chunk size dramatically and retry\n                    if current_chunk_size &gt; 1000:\n                        old_chunk_size = current_chunk_size\n                        current_chunk_size = max(500, current_chunk_size // 8)\n                        logger.error(\n                            \"Memory error in streaming tokenization - emergency chunk size reduction\",\n                            extra={\n                                \"old_chunk_size\": old_chunk_size,\n                                \"new_chunk_size\": current_chunk_size,\n                                \"chunk_index\": chunk_idx,\n                                \"error\": str(e),\n                            },\n                        )\n                        continue\n                    else:\n                        # Even minimum chunk size failed - critical error\n                        logger.critical(\n                            \"Cannot process even minimal chunks during streaming tokenization\",\n                            extra={\n                                \"chunk_size\": current_chunk_size,\n                                \"chunk_index\": chunk_idx,\n                                \"error\": str(e),\n                            },\n                        )\n                        raise MemoryError(\n                            f\"Cannot process even minimal chunks during streaming tokenization: {e}\"\n                        ) from e\n\n                except Exception:\n                    # If chunk processing fails, likely no more data\n                    consecutive_empty_chunks += 1\n                    chunk_idx += 1\n\n            # Complete progress step for streaming\n            if progress_manager:\n                progress_manager.complete_substep(\"tokenize\", \"stream_tokenize\")\n\n            if not chunk_lazyframes:\n                logger.warning(\n                    \"No chunks processed successfully in streaming tokenization\"\n                )\n                # Progress was already completed above\n                return ldf.with_columns([pl.lit([]).alias(\"tokens\")])\n\n            logger.info(\n                \"Streaming tokenization completed successfully\",\n                extra={\n                    \"total_chunks_processed\": len(chunk_lazyframes),\n                    \"final_chunk_size\": current_chunk_size,\n                    \"consecutive_empty_chunks\": consecutive_empty_chunks,\n                },\n            )\n            return pl.concat(chunk_lazyframes)\n\n    except Exception as e:\n        # If chunked processing fails completely, fall back to non-chunked processing\n        # This maintains backward compatibility and ensures functionality\n        logger.warning(\n            \"Chunked tokenization failed, attempting fallback to single-chunk processing\",\n            extra={\n                \"error\": str(e),\n                \"error_type\": type(e).__name__,\n                \"fallback_mode\": \"single_chunk\",\n            },\n        )\n\n        try:\n            result = _tokenize_chunk(ldf)\n            logger.info(\n                \"Fallback tokenization completed successfully\",\n                extra={\"fallback_mode\": \"single_chunk\"},\n            )\n            return result\n        except Exception as fallback_error:\n            # If even fallback fails, provide informative error\n            logger.critical(\n                \"Tokenization failed in both chunked and fallback modes\",\n                extra={\n                    \"chunked_error\": str(e),\n                    \"chunked_error_type\": type(e).__name__,\n                    \"fallback_error\": str(fallback_error),\n                    \"fallback_error_type\": type(fallback_error).__name__,\n                },\n            )\n            raise RuntimeError(\n                f\"Tokenization failed in both chunked and fallback modes. \"\n                f\"Chunked error: {str(e)}. Fallback error: {str(fallback_error)}\"\n            ) from e\n</code></pre>"},{"location":"reference/app/#app.utils.tokenize_text(ldf)","title":"<code>ldf</code>","text":""},{"location":"reference/app/#app.utils.tokenize_text(text_column)","title":"<code>text_column</code>","text":""},{"location":"reference/app/#app.utils.tokenize_text(progress_manager)","title":"<code>progress_manager</code>","text":""},{"location":"reference/app/#app.utils.tokenize_text(memory_manager)","title":"<code>memory_manager</code>","text":""},{"location":"reference/components/","title":"Components","text":""},{"location":"reference/components/#components","title":"<code>components</code>","text":"<p>The application's terminal components that will be accessed by the entry module.</p>"},{"location":"reference/components/#components.splash","title":"<code>splash</code>","text":""},{"location":"reference/importing/","title":"Importing","text":""},{"location":"reference/importing/#importing","title":"<code>importing</code>","text":""},{"location":"reference/importing/#importing.importer","title":"<code>importer</code>","text":""},{"location":"reference/importing/#importing.importer.Importer","title":"<code>Importer</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>importing/importer.py</code> <pre><code>class Importer[SessionType](ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"\n        The name of the importer. It will be quoted in the UI in texts such as\n        \"Imported as `name`, so keep it to a format name.\"\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def suggest(self, input_path: str) -&gt; bool:\n        \"\"\"\n        Check if the importer can handle the given file. This should be fairly\n        restrictive based on reasonable assumptions, as it is only used for the\n        initial importer suggestion. The user can always override the suggestion.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        \"\"\"\n        Produces an initial import session object that contains all the configuration\n        needed for the import. The user can either accept this configuration or\n        customize it.\n\n        Return None here if the importer cannot figure out how to configure the\n        import parameters. This doesn't necessarily mean that the file cannot be\n        loaded; the UI will force the user to customize the import session if the\n        user wants to proceed with this importer.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def manual_init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        pass\n\n    @abstractmethod\n    def modify_session(\n        self,\n        input_path: str,\n        import_session: SessionType,\n        reset_screen: Callable[[SessionType], None],\n    ) -&gt; Optional[SessionType]:\n        \"\"\"\n        Performs the interactive UI sequence that customizes the import session\n        from the initial one.\n\n        Return None here if the user interrupts the customization process.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The name of the importer. It will be quoted in the UI in texts such as \"Imported as <code>name</code>, so keep it to a format name.\"</p>"},{"location":"reference/importing/#importing.importer.Importer.init_session","title":"<code>init_session(input_path)</code>  <code>abstractmethod</code>","text":"<p>Produces an initial import session object that contains all the configuration needed for the import. The user can either accept this configuration or customize it.</p> <p>Return None here if the importer cannot figure out how to configure the import parameters. This doesn't necessarily mean that the file cannot be loaded; the UI will force the user to customize the import session if the user wants to proceed with this importer.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef init_session(self, input_path: str) -&gt; Optional[SessionType]:\n    \"\"\"\n    Produces an initial import session object that contains all the configuration\n    needed for the import. The user can either accept this configuration or\n    customize it.\n\n    Return None here if the importer cannot figure out how to configure the\n    import parameters. This doesn't necessarily mean that the file cannot be\n    loaded; the UI will force the user to customize the import session if the\n    user wants to proceed with this importer.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.modify_session","title":"<code>modify_session(input_path, import_session, reset_screen)</code>  <code>abstractmethod</code>","text":"<p>Performs the interactive UI sequence that customizes the import session from the initial one.</p> <p>Return None here if the user interrupts the customization process.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef modify_session(\n    self,\n    input_path: str,\n    import_session: SessionType,\n    reset_screen: Callable[[SessionType], None],\n) -&gt; Optional[SessionType]:\n    \"\"\"\n    Performs the interactive UI sequence that customizes the import session\n    from the initial one.\n\n    Return None here if the user interrupts the customization process.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.suggest","title":"<code>suggest(input_path)</code>  <code>abstractmethod</code>","text":"<p>Check if the importer can handle the given file. This should be fairly restrictive based on reasonable assumptions, as it is only used for the initial importer suggestion. The user can always override the suggestion.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef suggest(self, input_path: str) -&gt; bool:\n    \"\"\"\n    Check if the importer can handle the given file. This should be fairly\n    restrictive based on reasonable assumptions, as it is only used for the\n    initial importer suggestion. The user can always override the suggestion.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession","title":"<code>ImporterSession</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The ImporterSession interface handles the ongoing configuration of an import. It keeps the configuration state, knows how to print the configuration to the console, and can load a preview of the data from the input file.</p> Source code in <code>importing/importer.py</code> <pre><code>class ImporterSession(ABC):\n    \"\"\"\n    The ImporterSession interface handles the ongoing configuration of an import.\n    It keeps the configuration state, knows how to print the configuration to the\n    console, and can load a preview of the data from the input file.\n    \"\"\"\n\n    @abstractmethod\n    def print_config(self) -&gt; None:\n        \"\"\"\n        Print the configuration of the import session to the console.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n        \"\"\"\n        Attempt to load a preview of the data from the input file.\n\n        Return None here if it is sure that the file cannot be loaded with the current\n        configuration. Only throw an execption in the case of unexpected errors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def import_as_parquet(self, output_path: str) -&gt; None:\n        \"\"\"\n        Import the data from the input file to the output file in the Parquet format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.import_as_parquet","title":"<code>import_as_parquet(output_path)</code>  <code>abstractmethod</code>","text":"<p>Import the data from the input file to the output file in the Parquet format.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef import_as_parquet(self, output_path: str) -&gt; None:\n    \"\"\"\n    Import the data from the input file to the output file in the Parquet format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.load_preview","title":"<code>load_preview(n_records)</code>  <code>abstractmethod</code>","text":"<p>Attempt to load a preview of the data from the input file.</p> <p>Return None here if it is sure that the file cannot be loaded with the current configuration. Only throw an execption in the case of unexpected errors.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n    \"\"\"\n    Attempt to load a preview of the data from the input file.\n\n    Return None here if it is sure that the file cannot be loaded with the current\n    configuration. Only throw an execption in the case of unexpected errors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.print_config","title":"<code>print_config()</code>  <code>abstractmethod</code>","text":"<p>Print the configuration of the import session to the console.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef print_config(self) -&gt; None:\n    \"\"\"\n    Print the configuration of the import session to the console.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/meta/","title":"Meta","text":""},{"location":"reference/meta/#meta","title":"<code>meta</code>","text":""},{"location":"reference/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/preprocessing/#preprocessing","title":"<code>preprocessing</code>","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic","title":"<code>series_semantic</code>","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic.constant_series","title":"<code>constant_series(series, constant)</code>","text":"<p>Create a series with a constant value for each row of <code>series</code>.</p> Source code in <code>preprocessing/series_semantic.py</code> <pre><code>def constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"reference/storage/","title":"Storage","text":""},{"location":"reference/storage/#storage","title":"<code>storage</code>","text":""},{"location":"reference/storage/#storage.Storage","title":"<code>Storage</code>","text":"Source code in <code>storage/__init__.py</code> <pre><code>class Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return self._read_parquet_smart(output_path)\n\n    def _read_parquet_smart(self, path: str):\n        \"\"\"\n        Smart parquet reader that handles both single files and multi-file datasets.\n\n        - If path is a file, reads it directly\n        - If path is a directory, reads all parquet files within it as a dataset\n        - If path doesn't exist as file, try as directory with /*.parquet pattern\n        \"\"\"\n        import os\n\n        if os.path.isfile(path):\n            # Single file case\n            return pl.read_parquet(path)\n        elif os.path.isdir(path):\n            # Multi-file dataset case - read all parquet files in directory\n            return pl.read_parquet(os.path.join(path, \"*.parquet\"))\n        else:\n            # Path doesn't exist as file, try multi-file pattern\n            # This handles transition cases where path might be file.parquet vs directory\n            if path.endswith(\".parquet\"):\n                # Try directory version: replace file.parquet with file_dataset/*.parquet\n                base_path = path[:-8]  # Remove .parquet\n                dataset_path = f\"{base_path}_dataset\"\n                if os.path.isdir(dataset_path):\n                    return pl.read_parquet(os.path.join(dataset_path, \"*.parquet\"))\n\n            # Fallback to original path (will raise appropriate error if not found)\n            return pl.read_parquet(path)\n\n    def get_secondary_output_parquet_path(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        analyzer_suite=None,\n    ):\n        base_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n\n        # Check if this output should use multi-file dataset\n        if analyzer_suite:\n            try:\n                # Look up the analyzer interface\n                analyzer = analyzer_suite.get_secondary_analyzer_by_id(secondary_id)\n                if analyzer:\n                    # Find the specific output in the interface\n                    for output in analyzer.interface.outputs:\n                        if output.id == output_id and output.uses_multi_file_dataset:\n                            # Return directory path for multi-file datasets\n                            return os.path.join(base_path, f\"{output_id}_dataset\")\n            except (AttributeError, KeyError):\n                # Fallback to single file if interface lookup fails\n                pass\n\n        # Default single file behavior\n        return os.path.join(base_path, f\"{output_id}.parquet\")\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n</code></pre>"},{"location":"reference/terminal_tools/","title":"Terminal Tools","text":""},{"location":"reference/terminal_tools/#terminal-tools","title":"Terminal tools","text":""},{"location":"reference/terminal_tools/#terminal_tools","title":"<code>terminal_tools</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager","title":"<code>ProgressManager</code>","text":"<p>Full-featured progress manager with hierarchical tracking and memory monitoring.</p> <p>Features: - Hierarchical progress (steps with optional substeps) - Real-time terminal display with 60fps updates - Positional insertion for dynamic step ordering - Memory pressure monitoring and reporting - Context manager support for automatic lifecycle - Rich formatting with progress bars and status indicators</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class ProgressManager:\n    \"\"\"Full-featured progress manager with hierarchical tracking and memory monitoring.\n\n    Features:\n    - Hierarchical progress (steps with optional substeps)\n    - Real-time terminal display with 60fps updates\n    - Positional insertion for dynamic step ordering\n    - Memory pressure monitoring and reporting\n    - Context manager support for automatic lifecycle\n    - Rich formatting with progress bars and status indicators\n    \"\"\"\n\n    def __init__(\n        self,\n        title: str,\n        memory_manager: Optional[\"MemoryManager\"] = None,\n    ):\n        \"\"\"Initialize the unified progress manager.\n\n        Args:\n            title: The overall title for the progress display\n            memory_manager: Optional MemoryManager for memory monitoring\n        \"\"\"\n        self.title = title\n        self.memory_manager = memory_manager\n        self.last_memory_warning = None if memory_manager is None else 0\n\n        self.state_manager = ProgressStateManager()\n        self.display: Optional[TextualInlineProgressDisplay] = None\n        self._started = False\n\n    def add_step(\n        self,\n        step_id: str,\n        title: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new step to the progress display.\"\"\"\n        self.state_manager.add_step(step_id, title, total, insert_at)\n        if self._started:\n            self._update_display()\n\n    def add_substep(\n        self,\n        parent_step_id: str,\n        substep_id: str,\n        description: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new substep to a parent step.\"\"\"\n        self.state_manager.add_substep(\n            parent_step_id, substep_id, description, total, insert_at\n        )\n        if self._started:\n            self._update_display()\n\n    def start_step(self, step_id: str):\n        \"\"\"Start/activate a specific step.\"\"\"\n        self.state_manager.start_step(step_id)\n        if self._started:\n            self._update_display()\n\n    def update_step(self, step_id: str, progress: float, total: int = None):\n        \"\"\"Update the progress of a specific step.\"\"\"\n        self.state_manager.update_step(step_id, progress, total)\n        if self._started:\n            self._update_display()\n\n    def complete_step(self, step_id: str):\n        \"\"\"Mark a step as completed.\"\"\"\n        self.state_manager.complete_step(step_id)\n        if self._started:\n            self._update_display()\n\n    def fail_step(self, step_id: str, error_msg: str = None):\n        \"\"\"Mark a step as failed.\"\"\"\n        self.state_manager.fail_step(step_id, error_msg)\n        if self._started:\n            self._update_display()\n\n    def start_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Start/activate a specific substep.\"\"\"\n        self.state_manager.start_substep(parent_step_id, substep_id)\n        if self._started:\n            self._update_display()\n\n    def update_substep(\n        self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n    ):\n        \"\"\"Update the progress of a specific substep.\"\"\"\n        self.state_manager.update_substep(parent_step_id, substep_id, progress, total)\n        if self._started:\n            self._update_display()\n\n    def complete_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Mark a substep as completed.\"\"\"\n        self.state_manager.complete_substep(parent_step_id, substep_id)\n        if self._started:\n            self._update_display()\n\n    def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n        \"\"\"Mark a substep as failed.\"\"\"\n        self.state_manager.fail_substep(parent_step_id, substep_id, error_msg)\n        if self._started:\n            self._update_display()\n\n    def _update_display(self):\n        \"\"\"Update the display with current progress state.\"\"\"\n        if self._started and self.display:\n            table = self.state_manager.build_progress_table()\n            self.display.update_table(table)\n\n    # Lifecycle management\n    def start(self):\n        \"\"\"Start the progress display.\"\"\"\n        if not self._started:\n            self._started = True\n            self.display = TextualInlineProgressDisplay(self.title)\n            self.display.start()\n            self._update_display()\n\n    def finish(self):\n        \"\"\"Finish and cleanup the progress display.\"\"\"\n        if not self._started:\n            return\n\n        self._started = False\n\n        if self.display:\n            self.display.stop()\n            self.display = None\n\n        time.sleep(0.1)\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        if exc_type is None and self.memory_manager is not None:\n            try:\n                self.display_memory_summary()\n            except Exception:\n                pass\n\n        if exc_type is KeyboardInterrupt:\n            try:\n                self.finish()\n            except Exception:\n                pass\n        else:\n            self.finish()\n\n    # API compatibility properties - delegate to state manager\n    @property\n    def steps(self) -&gt; Dict[str, dict]:\n        \"\"\"Access to steps for backward compatibility.\"\"\"\n        return self.state_manager.steps\n\n    @property\n    def substeps(self) -&gt; Dict[str, Dict[str, dict]]:\n        \"\"\"Access to substeps for backward compatibility.\"\"\"\n        return self.state_manager.substeps\n\n    @property\n    def step_order(self) -&gt; List[str]:\n        \"\"\"Access to step order for backward compatibility.\"\"\"\n        return self.state_manager.step_order\n\n    @property\n    def active_step(self) -&gt; Optional[str]:\n        \"\"\"Access to active step for backward compatibility.\"\"\"\n        return self.state_manager.active_step\n\n    @property\n    def active_substeps(self) -&gt; Dict[str, Optional[str]]:\n        \"\"\"Access to active substeps for backward compatibility.\"\"\"\n        return self.state_manager.active_substeps\n\n    @property\n    def SYMBOLS(self) -&gt; Dict[str, str]:\n        \"\"\"Access to symbols for backward compatibility.\"\"\"\n        return self.state_manager.SYMBOLS\n\n    # Additional compatibility properties for tests\n    @property\n    def live(self):\n        \"\"\"Live display object for backward compatibility.\"\"\"\n        return None\n\n    @property\n    def table(self):\n        \"\"\"Current progress table for backward compatibility.\"\"\"\n        return self.state_manager.build_progress_table()\n\n    def _rebuild_table(self):\n        \"\"\"Rebuild table for backward compatibility.\"\"\"\n        pass\n\n    def refresh_display(self):\n        \"\"\"Refresh the display manually.\"\"\"\n        if self._started:\n            self._update_display()\n\n    @property\n    def console(self):\n        \"\"\"Rich Console instance for direct printing.\"\"\"\n        if self.display and self.display.console:\n            return self.display.console\n        if not hasattr(self, \"_console\"):\n            from rich.console import Console\n\n            self._console = Console()\n        return self._console\n\n    # Memory integration methods\n    def update_step_with_memory(\n        self, step_id: str, current: int, memory_context: str = \"\"\n    ) -&gt; None:\n        \"\"\"Update progress step with current memory usage information.\"\"\"\n        if self.memory_manager is None:\n            self.update_step(step_id, current)\n            return\n\n        # Get current memory stats\n        try:\n            memory_stats = self.memory_manager.get_current_memory_usage()\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Memory monitoring failed, continuing with standard progress update\",\n                extra={\n                    \"step_id\": step_id,\n                    \"current\": current,\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n            self.update_step(step_id, current)\n            return\n\n        # Update the progress step\n        self.update_step(step_id, current)\n\n        # Check for memory pressure and warn if necessary\n        try:\n            from app.utils import MemoryPressureLevel\n\n            pressure_level_str = memory_stats[\"pressure_level\"]\n            pressure_level = next(\n                (\n                    level\n                    for level in MemoryPressureLevel\n                    if level.value == pressure_level_str\n                ),\n                MemoryPressureLevel.LOW,\n            )\n\n            if pressure_level in [\n                MemoryPressureLevel.HIGH,\n                MemoryPressureLevel.CRITICAL,\n            ]:\n                self._display_memory_warning(\n                    pressure_level, memory_stats, memory_context\n                )\n\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Failed to process memory pressure level in progress reporting\",\n                extra={\n                    \"step_id\": step_id,\n                    \"pressure_level_str\": memory_stats.get(\"pressure_level\", \"unknown\"),\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n\n        try:\n            if self.memory_manager.should_trigger_gc():\n                cleanup_stats = self.memory_manager.enhanced_gc_cleanup()\n                if cleanup_stats[\"memory_freed_mb\"] &gt; 50:  # Significant cleanup\n                    console = Console()\n                    console.print(\n                        f\"[green]Freed {cleanup_stats['memory_freed_mb']:.1f}MB memory[/green]\"\n                    )\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Failed to trigger garbage collection in progress reporting\",\n                extra={\n                    \"step_id\": step_id,\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n\n    def _display_memory_warning(\n        self, pressure_level: \"MemoryPressureLevel\", memory_stats: dict, context: str\n    ):\n        \"\"\"Display memory pressure warning with context.\"\"\"\n        current_time = time.time()\n\n        # Rate limit warnings to avoid spam (minimum 30 seconds between warnings)\n        if (\n            self.last_memory_warning is not None\n            and current_time - self.last_memory_warning &lt; 30\n        ):\n            return\n\n        self.last_memory_warning = current_time\n\n        # Create warning message\n        console = Console()\n\n        rss_mb = memory_stats.get(\"rss_mb\", \"unknown\")\n        available_mb = memory_stats.get(\"available_mb\", \"unknown\")\n        pressure_level_str = pressure_level.value.upper()\n\n        warning_color = \"yellow\" if pressure_level.name == \"HIGH\" else \"red\"\n\n        warning_text = (\n            f\"[{warning_color}]Memory Pressure: {pressure_level_str}[/{warning_color}]\\n\"\n            f\"Current usage: {rss_mb}MB | Available: {available_mb}MB\"\n        )\n\n        if context:\n            warning_text += f\"\\nContext: {context}\"\n\n        warning_panel = Panel(\n            warning_text,\n            title=\"\u26a0\ufe0f Memory Alert\",\n            border_style=warning_color,\n        )\n\n        console.print(warning_panel)\n\n    def display_memory_summary(self):\n        \"\"\"Display memory usage summary at the end of analysis.\"\"\"\n        if self.memory_manager is None:\n            return\n\n        try:\n            final_memory = self.memory_manager.get_current_memory_usage()\n            memory_trend = self.memory_manager.get_memory_trend()\n            console = Console()\n\n            summary_text = (\n                f\"Analysis completed successfully!\\n\"\n                f\"Peak memory usage: {final_memory.get('peak_rss_mb', 'unknown')}MB\\n\"\n                f\"Final memory usage: {final_memory.get('rss_mb', 'unknown')}MB\\n\"\n                f\"Available memory: {final_memory.get('available_mb', 'unknown')}MB\\n\"\n                f\"Memory trend: {memory_trend}\\n\"\n                f\"Final pressure level: {final_memory['pressure_level']}\"\n            )\n\n            summary_panel = Panel(\n                summary_text,\n                title=\"Memory Summary\",\n                border_style=\"green\",\n            )\n            console.print(summary_panel)\n\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\"Failed to display memory summary\", extra={\"error\": str(e)})\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.SYMBOLS","title":"<code>SYMBOLS</code>  <code>property</code>","text":"<p>Access to symbols for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.active_step","title":"<code>active_step</code>  <code>property</code>","text":"<p>Access to active step for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.active_substeps","title":"<code>active_substeps</code>  <code>property</code>","text":"<p>Access to active substeps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.console","title":"<code>console</code>  <code>property</code>","text":"<p>Rich Console instance for direct printing.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.live","title":"<code>live</code>  <code>property</code>","text":"<p>Live display object for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.step_order","title":"<code>step_order</code>  <code>property</code>","text":"<p>Access to step order for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.steps","title":"<code>steps</code>  <code>property</code>","text":"<p>Access to steps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.substeps","title":"<code>substeps</code>  <code>property</code>","text":"<p>Access to substeps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.table","title":"<code>table</code>  <code>property</code>","text":"<p>Current progress table for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    if exc_type is None and self.memory_manager is not None:\n        try:\n            self.display_memory_summary()\n        except Exception:\n            pass\n\n    if exc_type is KeyboardInterrupt:\n        try:\n            self.finish()\n        except Exception:\n            pass\n    else:\n        self.finish()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.__init__","title":"<code>__init__(title, memory_manager=None)</code>","text":"<p>Initialize the unified progress manager.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The overall title for the progress display</p> required <code>Optional[MemoryManager]</code> <p>Optional MemoryManager for memory monitoring</p> <code>None</code> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(\n    self,\n    title: str,\n    memory_manager: Optional[\"MemoryManager\"] = None,\n):\n    \"\"\"Initialize the unified progress manager.\n\n    Args:\n        title: The overall title for the progress display\n        memory_manager: Optional MemoryManager for memory monitoring\n    \"\"\"\n    self.title = title\n    self.memory_manager = memory_manager\n    self.last_memory_warning = None if memory_manager is None else 0\n\n    self.state_manager = ProgressStateManager()\n    self.display: Optional[TextualInlineProgressDisplay] = None\n    self._started = False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.__init__(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.__init__(memory_manager)","title":"<code>memory_manager</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.add_step","title":"<code>add_step(step_id, title, total=None, insert_at=None)</code>","text":"<p>Add a new step to the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_step(\n    self,\n    step_id: str,\n    title: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new step to the progress display.\"\"\"\n    self.state_manager.add_step(step_id, title, total, insert_at)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.add_substep","title":"<code>add_substep(parent_step_id, substep_id, description, total=None, insert_at=None)</code>","text":"<p>Add a new substep to a parent step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_substep(\n    self,\n    parent_step_id: str,\n    substep_id: str,\n    description: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new substep to a parent step.\"\"\"\n    self.state_manager.add_substep(\n        parent_step_id, substep_id, description, total, insert_at\n    )\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.complete_step","title":"<code>complete_step(step_id)</code>","text":"<p>Mark a step as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_step(self, step_id: str):\n    \"\"\"Mark a step as completed.\"\"\"\n    self.state_manager.complete_step(step_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.complete_substep","title":"<code>complete_substep(parent_step_id, substep_id)</code>","text":"<p>Mark a substep as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Mark a substep as completed.\"\"\"\n    self.state_manager.complete_substep(parent_step_id, substep_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.display_memory_summary","title":"<code>display_memory_summary()</code>","text":"<p>Display memory usage summary at the end of analysis.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def display_memory_summary(self):\n    \"\"\"Display memory usage summary at the end of analysis.\"\"\"\n    if self.memory_manager is None:\n        return\n\n    try:\n        final_memory = self.memory_manager.get_current_memory_usage()\n        memory_trend = self.memory_manager.get_memory_trend()\n        console = Console()\n\n        summary_text = (\n            f\"Analysis completed successfully!\\n\"\n            f\"Peak memory usage: {final_memory.get('peak_rss_mb', 'unknown')}MB\\n\"\n            f\"Final memory usage: {final_memory.get('rss_mb', 'unknown')}MB\\n\"\n            f\"Available memory: {final_memory.get('available_mb', 'unknown')}MB\\n\"\n            f\"Memory trend: {memory_trend}\\n\"\n            f\"Final pressure level: {final_memory['pressure_level']}\"\n        )\n\n        summary_panel = Panel(\n            summary_text,\n            title=\"Memory Summary\",\n            border_style=\"green\",\n        )\n        console.print(summary_panel)\n\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\"Failed to display memory summary\", extra={\"error\": str(e)})\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.fail_step","title":"<code>fail_step(step_id, error_msg=None)</code>","text":"<p>Mark a step as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_step(self, step_id: str, error_msg: str = None):\n    \"\"\"Mark a step as failed.\"\"\"\n    self.state_manager.fail_step(step_id, error_msg)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.fail_substep","title":"<code>fail_substep(parent_step_id, substep_id, error_msg=None)</code>","text":"<p>Mark a substep as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n    \"\"\"Mark a substep as failed.\"\"\"\n    self.state_manager.fail_substep(parent_step_id, substep_id, error_msg)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.finish","title":"<code>finish()</code>","text":"<p>Finish and cleanup the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def finish(self):\n    \"\"\"Finish and cleanup the progress display.\"\"\"\n    if not self._started:\n        return\n\n    self._started = False\n\n    if self.display:\n        self.display.stop()\n        self.display = None\n\n    time.sleep(0.1)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.refresh_display","title":"<code>refresh_display()</code>","text":"<p>Refresh the display manually.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def refresh_display(self):\n    \"\"\"Refresh the display manually.\"\"\"\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.start","title":"<code>start()</code>","text":"<p>Start the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start(self):\n    \"\"\"Start the progress display.\"\"\"\n    if not self._started:\n        self._started = True\n        self.display = TextualInlineProgressDisplay(self.title)\n        self.display.start()\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.start_step","title":"<code>start_step(step_id)</code>","text":"<p>Start/activate a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_step(self, step_id: str):\n    \"\"\"Start/activate a specific step.\"\"\"\n    self.state_manager.start_step(step_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.start_substep","title":"<code>start_substep(parent_step_id, substep_id)</code>","text":"<p>Start/activate a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Start/activate a specific substep.\"\"\"\n    self.state_manager.start_substep(parent_step_id, substep_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.update_step","title":"<code>update_step(step_id, progress, total=None)</code>","text":"<p>Update the progress of a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_step(self, step_id: str, progress: float, total: int = None):\n    \"\"\"Update the progress of a specific step.\"\"\"\n    self.state_manager.update_step(step_id, progress, total)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.update_step_with_memory","title":"<code>update_step_with_memory(step_id, current, memory_context='')</code>","text":"<p>Update progress step with current memory usage information.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_step_with_memory(\n    self, step_id: str, current: int, memory_context: str = \"\"\n) -&gt; None:\n    \"\"\"Update progress step with current memory usage information.\"\"\"\n    if self.memory_manager is None:\n        self.update_step(step_id, current)\n        return\n\n    # Get current memory stats\n    try:\n        memory_stats = self.memory_manager.get_current_memory_usage()\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Memory monitoring failed, continuing with standard progress update\",\n            extra={\n                \"step_id\": step_id,\n                \"current\": current,\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n        self.update_step(step_id, current)\n        return\n\n    # Update the progress step\n    self.update_step(step_id, current)\n\n    # Check for memory pressure and warn if necessary\n    try:\n        from app.utils import MemoryPressureLevel\n\n        pressure_level_str = memory_stats[\"pressure_level\"]\n        pressure_level = next(\n            (\n                level\n                for level in MemoryPressureLevel\n                if level.value == pressure_level_str\n            ),\n            MemoryPressureLevel.LOW,\n        )\n\n        if pressure_level in [\n            MemoryPressureLevel.HIGH,\n            MemoryPressureLevel.CRITICAL,\n        ]:\n            self._display_memory_warning(\n                pressure_level, memory_stats, memory_context\n            )\n\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Failed to process memory pressure level in progress reporting\",\n            extra={\n                \"step_id\": step_id,\n                \"pressure_level_str\": memory_stats.get(\"pressure_level\", \"unknown\"),\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n\n    try:\n        if self.memory_manager.should_trigger_gc():\n            cleanup_stats = self.memory_manager.enhanced_gc_cleanup()\n            if cleanup_stats[\"memory_freed_mb\"] &gt; 50:  # Significant cleanup\n                console = Console()\n                console.print(\n                    f\"[green]Freed {cleanup_stats['memory_freed_mb']:.1f}MB memory[/green]\"\n                )\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Failed to trigger garbage collection in progress reporting\",\n            extra={\n                \"step_id\": step_id,\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressManager.update_substep","title":"<code>update_substep(parent_step_id, substep_id, progress, total=None)</code>","text":"<p>Update the progress of a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_substep(\n    self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n):\n    \"\"\"Update the progress of a specific substep.\"\"\"\n    self.state_manager.update_substep(parent_step_id, substep_id, progress, total)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter","title":"<code>ProgressReporter</code>","text":"<p>Basic progress reporter with simple start/finish lifecycle.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class ProgressReporter:\n    \"\"\"Basic progress reporter with simple start/finish lifecycle.\"\"\"\n\n    def __init__(self, title: str):\n        \"\"\"Initialize progress reporter.\n\n        Args:\n            title: Title to display for this progress operation\n        \"\"\"\n        self.title = title\n        self._start_time = None\n        self._last_update = None\n\n    def __enter__(self):\n        \"\"\"Context manager entry - records start time.\"\"\"\n        self._start_time = time.time()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - records finish time.\"\"\"\n        pass\n\n    def update(self, current: int, total: Optional[int] = None, message: str = \"\"):\n        \"\"\"Update progress (basic implementation for compatibility).\"\"\"\n        self._last_update = time.time()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry - records start time.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry - records start time.\"\"\"\n    self._start_time = time.time()\n    return self\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit - records finish time.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - records finish time.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter.__init__","title":"<code>__init__(title)</code>","text":"<p>Initialize progress reporter.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Title to display for this progress operation</p> required Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(self, title: str):\n    \"\"\"Initialize progress reporter.\n\n    Args:\n        title: Title to display for this progress operation\n    \"\"\"\n    self.title = title\n    self._start_time = None\n    self._last_update = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter.__init__(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.ProgressReporter.update","title":"<code>update(current, total=None, message='')</code>","text":"<p>Update progress (basic implementation for compatibility).</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update(self, current: int, total: Optional[int] = None, message: str = \"\"):\n    \"\"\"Update progress (basic implementation for compatibility).\"\"\"\n    self._last_update = time.time()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception","title":"<code>inception</code>","text":"<p>The inception module aids in creating nested terminal blocks (hence the name \"inception\"). It provides a <code>Context</code> class that manages a list of <code>Scope</code> instances. Each <code>Scope</code> instance represents a block of text that are buffered in memory and printed to the terminal at each refresh.</p>"},{"location":"reference/terminal_tools/#terminal_tools.inception.Scope","title":"<code>Scope</code>","text":"Source code in <code>terminal_tools/inception.py</code> <pre><code>class Scope:\n    def __init__(self, context: TerminalContext, text: str):\n        self.context = context\n        self.text = text\n\n    def print(self):\n        print(self.text)\n\n    def refresh(self):\n        \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n        self.context._refresh()\n\n    def __enter__(self):\n        self.context._append_scope(self)\n        self.context._refresh()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.context._remove_scope(self)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception.Scope.refresh","title":"<code>refresh()</code>","text":"<p>Clear the terminal and repaint with every scope up and including to this one</p> Source code in <code>terminal_tools/inception.py</code> <pre><code>def refresh(self):\n    \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n    self.context._refresh()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception.TerminalContext","title":"<code>TerminalContext</code>","text":"Source code in <code>terminal_tools/inception.py</code> <pre><code>class TerminalContext:\n    def __init__(self):\n        self.scopes: list[Scope] = []\n        self._suppress_clear = False\n\n    def nest(self, text: str):\n        scope = Scope(context=self, text=text)\n        return scope\n\n    def _append_scope(self, block: \"Scope\"):\n        self.scopes.append(block)\n\n    def _remove_scope(self, block: \"Scope\"):\n        self.scopes.remove(block)\n\n    def _refresh(self):\n        if not self._suppress_clear:\n            clear_terminal()\n        for scope in self.scopes:\n            scope.print()\n\n    def suppress_clear(self, suppress: bool = True):\n        \"\"\"Suppress terminal clearing to avoid conflicts with Textual displays.\"\"\"\n        self._suppress_clear = suppress\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception.TerminalContext.suppress_clear","title":"<code>suppress_clear(suppress=True)</code>","text":"<p>Suppress terminal clearing to avoid conflicts with Textual displays.</p> Source code in <code>terminal_tools/inception.py</code> <pre><code>def suppress_clear(self, suppress: bool = True):\n    \"\"\"Suppress terminal clearing to avoid conflicts with Textual displays.\"\"\"\n    self._suppress_clear = suppress\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress","title":"<code>progress</code>","text":"<p>Progress reporting functionality for terminal-based analysis workflows.</p> <p>Provides hierarchical progress tracking with real-time terminal display: - ProgressReporter: Basic progress reporting with context manager support - ProgressManager: Full-featured progress manager with step and substep tracking - ProgressStateManager: Core progress state management and validation - TextualInlineProgressDisplay: Textual-based inline progress display - SimpleProgressApp: Minimal Textual app for inline progress visualization</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager","title":"<code>ProgressManager</code>","text":"<p>Full-featured progress manager with hierarchical tracking and memory monitoring.</p> <p>Features: - Hierarchical progress (steps with optional substeps) - Real-time terminal display with 60fps updates - Positional insertion for dynamic step ordering - Memory pressure monitoring and reporting - Context manager support for automatic lifecycle - Rich formatting with progress bars and status indicators</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class ProgressManager:\n    \"\"\"Full-featured progress manager with hierarchical tracking and memory monitoring.\n\n    Features:\n    - Hierarchical progress (steps with optional substeps)\n    - Real-time terminal display with 60fps updates\n    - Positional insertion for dynamic step ordering\n    - Memory pressure monitoring and reporting\n    - Context manager support for automatic lifecycle\n    - Rich formatting with progress bars and status indicators\n    \"\"\"\n\n    def __init__(\n        self,\n        title: str,\n        memory_manager: Optional[\"MemoryManager\"] = None,\n    ):\n        \"\"\"Initialize the unified progress manager.\n\n        Args:\n            title: The overall title for the progress display\n            memory_manager: Optional MemoryManager for memory monitoring\n        \"\"\"\n        self.title = title\n        self.memory_manager = memory_manager\n        self.last_memory_warning = None if memory_manager is None else 0\n\n        self.state_manager = ProgressStateManager()\n        self.display: Optional[TextualInlineProgressDisplay] = None\n        self._started = False\n\n    def add_step(\n        self,\n        step_id: str,\n        title: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new step to the progress display.\"\"\"\n        self.state_manager.add_step(step_id, title, total, insert_at)\n        if self._started:\n            self._update_display()\n\n    def add_substep(\n        self,\n        parent_step_id: str,\n        substep_id: str,\n        description: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new substep to a parent step.\"\"\"\n        self.state_manager.add_substep(\n            parent_step_id, substep_id, description, total, insert_at\n        )\n        if self._started:\n            self._update_display()\n\n    def start_step(self, step_id: str):\n        \"\"\"Start/activate a specific step.\"\"\"\n        self.state_manager.start_step(step_id)\n        if self._started:\n            self._update_display()\n\n    def update_step(self, step_id: str, progress: float, total: int = None):\n        \"\"\"Update the progress of a specific step.\"\"\"\n        self.state_manager.update_step(step_id, progress, total)\n        if self._started:\n            self._update_display()\n\n    def complete_step(self, step_id: str):\n        \"\"\"Mark a step as completed.\"\"\"\n        self.state_manager.complete_step(step_id)\n        if self._started:\n            self._update_display()\n\n    def fail_step(self, step_id: str, error_msg: str = None):\n        \"\"\"Mark a step as failed.\"\"\"\n        self.state_manager.fail_step(step_id, error_msg)\n        if self._started:\n            self._update_display()\n\n    def start_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Start/activate a specific substep.\"\"\"\n        self.state_manager.start_substep(parent_step_id, substep_id)\n        if self._started:\n            self._update_display()\n\n    def update_substep(\n        self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n    ):\n        \"\"\"Update the progress of a specific substep.\"\"\"\n        self.state_manager.update_substep(parent_step_id, substep_id, progress, total)\n        if self._started:\n            self._update_display()\n\n    def complete_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Mark a substep as completed.\"\"\"\n        self.state_manager.complete_substep(parent_step_id, substep_id)\n        if self._started:\n            self._update_display()\n\n    def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n        \"\"\"Mark a substep as failed.\"\"\"\n        self.state_manager.fail_substep(parent_step_id, substep_id, error_msg)\n        if self._started:\n            self._update_display()\n\n    def _update_display(self):\n        \"\"\"Update the display with current progress state.\"\"\"\n        if self._started and self.display:\n            table = self.state_manager.build_progress_table()\n            self.display.update_table(table)\n\n    # Lifecycle management\n    def start(self):\n        \"\"\"Start the progress display.\"\"\"\n        if not self._started:\n            self._started = True\n            self.display = TextualInlineProgressDisplay(self.title)\n            self.display.start()\n            self._update_display()\n\n    def finish(self):\n        \"\"\"Finish and cleanup the progress display.\"\"\"\n        if not self._started:\n            return\n\n        self._started = False\n\n        if self.display:\n            self.display.stop()\n            self.display = None\n\n        time.sleep(0.1)\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        if exc_type is None and self.memory_manager is not None:\n            try:\n                self.display_memory_summary()\n            except Exception:\n                pass\n\n        if exc_type is KeyboardInterrupt:\n            try:\n                self.finish()\n            except Exception:\n                pass\n        else:\n            self.finish()\n\n    # API compatibility properties - delegate to state manager\n    @property\n    def steps(self) -&gt; Dict[str, dict]:\n        \"\"\"Access to steps for backward compatibility.\"\"\"\n        return self.state_manager.steps\n\n    @property\n    def substeps(self) -&gt; Dict[str, Dict[str, dict]]:\n        \"\"\"Access to substeps for backward compatibility.\"\"\"\n        return self.state_manager.substeps\n\n    @property\n    def step_order(self) -&gt; List[str]:\n        \"\"\"Access to step order for backward compatibility.\"\"\"\n        return self.state_manager.step_order\n\n    @property\n    def active_step(self) -&gt; Optional[str]:\n        \"\"\"Access to active step for backward compatibility.\"\"\"\n        return self.state_manager.active_step\n\n    @property\n    def active_substeps(self) -&gt; Dict[str, Optional[str]]:\n        \"\"\"Access to active substeps for backward compatibility.\"\"\"\n        return self.state_manager.active_substeps\n\n    @property\n    def SYMBOLS(self) -&gt; Dict[str, str]:\n        \"\"\"Access to symbols for backward compatibility.\"\"\"\n        return self.state_manager.SYMBOLS\n\n    # Additional compatibility properties for tests\n    @property\n    def live(self):\n        \"\"\"Live display object for backward compatibility.\"\"\"\n        return None\n\n    @property\n    def table(self):\n        \"\"\"Current progress table for backward compatibility.\"\"\"\n        return self.state_manager.build_progress_table()\n\n    def _rebuild_table(self):\n        \"\"\"Rebuild table for backward compatibility.\"\"\"\n        pass\n\n    def refresh_display(self):\n        \"\"\"Refresh the display manually.\"\"\"\n        if self._started:\n            self._update_display()\n\n    @property\n    def console(self):\n        \"\"\"Rich Console instance for direct printing.\"\"\"\n        if self.display and self.display.console:\n            return self.display.console\n        if not hasattr(self, \"_console\"):\n            from rich.console import Console\n\n            self._console = Console()\n        return self._console\n\n    # Memory integration methods\n    def update_step_with_memory(\n        self, step_id: str, current: int, memory_context: str = \"\"\n    ) -&gt; None:\n        \"\"\"Update progress step with current memory usage information.\"\"\"\n        if self.memory_manager is None:\n            self.update_step(step_id, current)\n            return\n\n        # Get current memory stats\n        try:\n            memory_stats = self.memory_manager.get_current_memory_usage()\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Memory monitoring failed, continuing with standard progress update\",\n                extra={\n                    \"step_id\": step_id,\n                    \"current\": current,\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n            self.update_step(step_id, current)\n            return\n\n        # Update the progress step\n        self.update_step(step_id, current)\n\n        # Check for memory pressure and warn if necessary\n        try:\n            from app.utils import MemoryPressureLevel\n\n            pressure_level_str = memory_stats[\"pressure_level\"]\n            pressure_level = next(\n                (\n                    level\n                    for level in MemoryPressureLevel\n                    if level.value == pressure_level_str\n                ),\n                MemoryPressureLevel.LOW,\n            )\n\n            if pressure_level in [\n                MemoryPressureLevel.HIGH,\n                MemoryPressureLevel.CRITICAL,\n            ]:\n                self._display_memory_warning(\n                    pressure_level, memory_stats, memory_context\n                )\n\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Failed to process memory pressure level in progress reporting\",\n                extra={\n                    \"step_id\": step_id,\n                    \"pressure_level_str\": memory_stats.get(\"pressure_level\", \"unknown\"),\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n\n        try:\n            if self.memory_manager.should_trigger_gc():\n                cleanup_stats = self.memory_manager.enhanced_gc_cleanup()\n                if cleanup_stats[\"memory_freed_mb\"] &gt; 50:  # Significant cleanup\n                    console = Console()\n                    console.print(\n                        f\"[green]Freed {cleanup_stats['memory_freed_mb']:.1f}MB memory[/green]\"\n                    )\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\n                \"Failed to trigger garbage collection in progress reporting\",\n                extra={\n                    \"step_id\": step_id,\n                    \"memory_context\": memory_context,\n                    \"error\": str(e),\n                },\n            )\n\n    def _display_memory_warning(\n        self, pressure_level: \"MemoryPressureLevel\", memory_stats: dict, context: str\n    ):\n        \"\"\"Display memory pressure warning with context.\"\"\"\n        current_time = time.time()\n\n        # Rate limit warnings to avoid spam (minimum 30 seconds between warnings)\n        if (\n            self.last_memory_warning is not None\n            and current_time - self.last_memory_warning &lt; 30\n        ):\n            return\n\n        self.last_memory_warning = current_time\n\n        # Create warning message\n        console = Console()\n\n        rss_mb = memory_stats.get(\"rss_mb\", \"unknown\")\n        available_mb = memory_stats.get(\"available_mb\", \"unknown\")\n        pressure_level_str = pressure_level.value.upper()\n\n        warning_color = \"yellow\" if pressure_level.name == \"HIGH\" else \"red\"\n\n        warning_text = (\n            f\"[{warning_color}]Memory Pressure: {pressure_level_str}[/{warning_color}]\\n\"\n            f\"Current usage: {rss_mb}MB | Available: {available_mb}MB\"\n        )\n\n        if context:\n            warning_text += f\"\\nContext: {context}\"\n\n        warning_panel = Panel(\n            warning_text,\n            title=\"\u26a0\ufe0f Memory Alert\",\n            border_style=warning_color,\n        )\n\n        console.print(warning_panel)\n\n    def display_memory_summary(self):\n        \"\"\"Display memory usage summary at the end of analysis.\"\"\"\n        if self.memory_manager is None:\n            return\n\n        try:\n            final_memory = self.memory_manager.get_current_memory_usage()\n            memory_trend = self.memory_manager.get_memory_trend()\n            console = Console()\n\n            summary_text = (\n                f\"Analysis completed successfully!\\n\"\n                f\"Peak memory usage: {final_memory.get('peak_rss_mb', 'unknown')}MB\\n\"\n                f\"Final memory usage: {final_memory.get('rss_mb', 'unknown')}MB\\n\"\n                f\"Available memory: {final_memory.get('available_mb', 'unknown')}MB\\n\"\n                f\"Memory trend: {memory_trend}\\n\"\n                f\"Final pressure level: {final_memory['pressure_level']}\"\n            )\n\n            summary_panel = Panel(\n                summary_text,\n                title=\"Memory Summary\",\n                border_style=\"green\",\n            )\n            console.print(summary_panel)\n\n        except Exception as e:\n            from app.logger import get_logger\n\n            logger = get_logger(__name__)\n            logger.warning(\"Failed to display memory summary\", extra={\"error\": str(e)})\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.SYMBOLS","title":"<code>SYMBOLS</code>  <code>property</code>","text":"<p>Access to symbols for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.active_step","title":"<code>active_step</code>  <code>property</code>","text":"<p>Access to active step for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.active_substeps","title":"<code>active_substeps</code>  <code>property</code>","text":"<p>Access to active substeps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.console","title":"<code>console</code>  <code>property</code>","text":"<p>Rich Console instance for direct printing.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.live","title":"<code>live</code>  <code>property</code>","text":"<p>Live display object for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.step_order","title":"<code>step_order</code>  <code>property</code>","text":"<p>Access to step order for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.steps","title":"<code>steps</code>  <code>property</code>","text":"<p>Access to steps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.substeps","title":"<code>substeps</code>  <code>property</code>","text":"<p>Access to substeps for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.table","title":"<code>table</code>  <code>property</code>","text":"<p>Current progress table for backward compatibility.</p>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    if exc_type is None and self.memory_manager is not None:\n        try:\n            self.display_memory_summary()\n        except Exception:\n            pass\n\n    if exc_type is KeyboardInterrupt:\n        try:\n            self.finish()\n        except Exception:\n            pass\n    else:\n        self.finish()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.__init__","title":"<code>__init__(title, memory_manager=None)</code>","text":"<p>Initialize the unified progress manager.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The overall title for the progress display</p> required <code>Optional[MemoryManager]</code> <p>Optional MemoryManager for memory monitoring</p> <code>None</code> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(\n    self,\n    title: str,\n    memory_manager: Optional[\"MemoryManager\"] = None,\n):\n    \"\"\"Initialize the unified progress manager.\n\n    Args:\n        title: The overall title for the progress display\n        memory_manager: Optional MemoryManager for memory monitoring\n    \"\"\"\n    self.title = title\n    self.memory_manager = memory_manager\n    self.last_memory_warning = None if memory_manager is None else 0\n\n    self.state_manager = ProgressStateManager()\n    self.display: Optional[TextualInlineProgressDisplay] = None\n    self._started = False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.__init__(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.__init__(memory_manager)","title":"<code>memory_manager</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.add_step","title":"<code>add_step(step_id, title, total=None, insert_at=None)</code>","text":"<p>Add a new step to the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_step(\n    self,\n    step_id: str,\n    title: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new step to the progress display.\"\"\"\n    self.state_manager.add_step(step_id, title, total, insert_at)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.add_substep","title":"<code>add_substep(parent_step_id, substep_id, description, total=None, insert_at=None)</code>","text":"<p>Add a new substep to a parent step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_substep(\n    self,\n    parent_step_id: str,\n    substep_id: str,\n    description: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new substep to a parent step.\"\"\"\n    self.state_manager.add_substep(\n        parent_step_id, substep_id, description, total, insert_at\n    )\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.complete_step","title":"<code>complete_step(step_id)</code>","text":"<p>Mark a step as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_step(self, step_id: str):\n    \"\"\"Mark a step as completed.\"\"\"\n    self.state_manager.complete_step(step_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.complete_substep","title":"<code>complete_substep(parent_step_id, substep_id)</code>","text":"<p>Mark a substep as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Mark a substep as completed.\"\"\"\n    self.state_manager.complete_substep(parent_step_id, substep_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.display_memory_summary","title":"<code>display_memory_summary()</code>","text":"<p>Display memory usage summary at the end of analysis.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def display_memory_summary(self):\n    \"\"\"Display memory usage summary at the end of analysis.\"\"\"\n    if self.memory_manager is None:\n        return\n\n    try:\n        final_memory = self.memory_manager.get_current_memory_usage()\n        memory_trend = self.memory_manager.get_memory_trend()\n        console = Console()\n\n        summary_text = (\n            f\"Analysis completed successfully!\\n\"\n            f\"Peak memory usage: {final_memory.get('peak_rss_mb', 'unknown')}MB\\n\"\n            f\"Final memory usage: {final_memory.get('rss_mb', 'unknown')}MB\\n\"\n            f\"Available memory: {final_memory.get('available_mb', 'unknown')}MB\\n\"\n            f\"Memory trend: {memory_trend}\\n\"\n            f\"Final pressure level: {final_memory['pressure_level']}\"\n        )\n\n        summary_panel = Panel(\n            summary_text,\n            title=\"Memory Summary\",\n            border_style=\"green\",\n        )\n        console.print(summary_panel)\n\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\"Failed to display memory summary\", extra={\"error\": str(e)})\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.fail_step","title":"<code>fail_step(step_id, error_msg=None)</code>","text":"<p>Mark a step as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_step(self, step_id: str, error_msg: str = None):\n    \"\"\"Mark a step as failed.\"\"\"\n    self.state_manager.fail_step(step_id, error_msg)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.fail_substep","title":"<code>fail_substep(parent_step_id, substep_id, error_msg=None)</code>","text":"<p>Mark a substep as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n    \"\"\"Mark a substep as failed.\"\"\"\n    self.state_manager.fail_substep(parent_step_id, substep_id, error_msg)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.finish","title":"<code>finish()</code>","text":"<p>Finish and cleanup the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def finish(self):\n    \"\"\"Finish and cleanup the progress display.\"\"\"\n    if not self._started:\n        return\n\n    self._started = False\n\n    if self.display:\n        self.display.stop()\n        self.display = None\n\n    time.sleep(0.1)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.refresh_display","title":"<code>refresh_display()</code>","text":"<p>Refresh the display manually.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def refresh_display(self):\n    \"\"\"Refresh the display manually.\"\"\"\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.start","title":"<code>start()</code>","text":"<p>Start the progress display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start(self):\n    \"\"\"Start the progress display.\"\"\"\n    if not self._started:\n        self._started = True\n        self.display = TextualInlineProgressDisplay(self.title)\n        self.display.start()\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.start_step","title":"<code>start_step(step_id)</code>","text":"<p>Start/activate a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_step(self, step_id: str):\n    \"\"\"Start/activate a specific step.\"\"\"\n    self.state_manager.start_step(step_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.start_substep","title":"<code>start_substep(parent_step_id, substep_id)</code>","text":"<p>Start/activate a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Start/activate a specific substep.\"\"\"\n    self.state_manager.start_substep(parent_step_id, substep_id)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.update_step","title":"<code>update_step(step_id, progress, total=None)</code>","text":"<p>Update the progress of a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_step(self, step_id: str, progress: float, total: int = None):\n    \"\"\"Update the progress of a specific step.\"\"\"\n    self.state_manager.update_step(step_id, progress, total)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.update_step_with_memory","title":"<code>update_step_with_memory(step_id, current, memory_context='')</code>","text":"<p>Update progress step with current memory usage information.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_step_with_memory(\n    self, step_id: str, current: int, memory_context: str = \"\"\n) -&gt; None:\n    \"\"\"Update progress step with current memory usage information.\"\"\"\n    if self.memory_manager is None:\n        self.update_step(step_id, current)\n        return\n\n    # Get current memory stats\n    try:\n        memory_stats = self.memory_manager.get_current_memory_usage()\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Memory monitoring failed, continuing with standard progress update\",\n            extra={\n                \"step_id\": step_id,\n                \"current\": current,\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n        self.update_step(step_id, current)\n        return\n\n    # Update the progress step\n    self.update_step(step_id, current)\n\n    # Check for memory pressure and warn if necessary\n    try:\n        from app.utils import MemoryPressureLevel\n\n        pressure_level_str = memory_stats[\"pressure_level\"]\n        pressure_level = next(\n            (\n                level\n                for level in MemoryPressureLevel\n                if level.value == pressure_level_str\n            ),\n            MemoryPressureLevel.LOW,\n        )\n\n        if pressure_level in [\n            MemoryPressureLevel.HIGH,\n            MemoryPressureLevel.CRITICAL,\n        ]:\n            self._display_memory_warning(\n                pressure_level, memory_stats, memory_context\n            )\n\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Failed to process memory pressure level in progress reporting\",\n            extra={\n                \"step_id\": step_id,\n                \"pressure_level_str\": memory_stats.get(\"pressure_level\", \"unknown\"),\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n\n    try:\n        if self.memory_manager.should_trigger_gc():\n            cleanup_stats = self.memory_manager.enhanced_gc_cleanup()\n            if cleanup_stats[\"memory_freed_mb\"] &gt; 50:  # Significant cleanup\n                console = Console()\n                console.print(\n                    f\"[green]Freed {cleanup_stats['memory_freed_mb']:.1f}MB memory[/green]\"\n                )\n    except Exception as e:\n        from app.logger import get_logger\n\n        logger = get_logger(__name__)\n        logger.warning(\n            \"Failed to trigger garbage collection in progress reporting\",\n            extra={\n                \"step_id\": step_id,\n                \"memory_context\": memory_context,\n                \"error\": str(e),\n            },\n        )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressManager.update_substep","title":"<code>update_substep(parent_step_id, substep_id, progress, total=None)</code>","text":"<p>Update the progress of a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_substep(\n    self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n):\n    \"\"\"Update the progress of a specific substep.\"\"\"\n    self.state_manager.update_substep(parent_step_id, substep_id, progress, total)\n    if self._started:\n        self._update_display()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter","title":"<code>ProgressReporter</code>","text":"<p>Basic progress reporter with simple start/finish lifecycle.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class ProgressReporter:\n    \"\"\"Basic progress reporter with simple start/finish lifecycle.\"\"\"\n\n    def __init__(self, title: str):\n        \"\"\"Initialize progress reporter.\n\n        Args:\n            title: Title to display for this progress operation\n        \"\"\"\n        self.title = title\n        self._start_time = None\n        self._last_update = None\n\n    def __enter__(self):\n        \"\"\"Context manager entry - records start time.\"\"\"\n        self._start_time = time.time()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - records finish time.\"\"\"\n        pass\n\n    def update(self, current: int, total: Optional[int] = None, message: str = \"\"):\n        \"\"\"Update progress (basic implementation for compatibility).\"\"\"\n        self._last_update = time.time()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry - records start time.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry - records start time.\"\"\"\n    self._start_time = time.time()\n    return self\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit - records finish time.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - records finish time.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter.__init__","title":"<code>__init__(title)</code>","text":"<p>Initialize progress reporter.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Title to display for this progress operation</p> required Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(self, title: str):\n    \"\"\"Initialize progress reporter.\n\n    Args:\n        title: Title to display for this progress operation\n    \"\"\"\n    self.title = title\n    self._start_time = None\n    self._last_update = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter.__init__(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressReporter.update","title":"<code>update(current, total=None, message='')</code>","text":"<p>Update progress (basic implementation for compatibility).</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update(self, current: int, total: Optional[int] = None, message: str = \"\"):\n    \"\"\"Update progress (basic implementation for compatibility).\"\"\"\n    self._last_update = time.time()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager","title":"<code>ProgressStateManager</code>","text":"<p>Core progress state management with validation and tracking.</p> <p>Manages hierarchical progress tracking with steps and substeps, including state transitions, validation, and Rich table generation.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class ProgressStateManager:\n    \"\"\"Core progress state management with validation and tracking.\n\n    Manages hierarchical progress tracking with steps and substeps,\n    including state transitions, validation, and Rich table generation.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize progress state tracking.\"\"\"\n        # Progress tracking data structures\n        self.steps: Dict[str, dict] = {}\n        self.substeps: Dict[str, Dict[str, dict]] = {}\n        self.step_order: List[str] = []\n        self.active_step: Optional[str] = None\n        self.active_substeps: Dict[str, Optional[str]] = {}\n\n        # State symbols for different progress states\n        self.SYMBOLS = {\n            \"pending\": \"\u23f8\",\n            \"active\": \"\u23f3\",\n            \"completed\": \"\u2713\",\n            \"failed\": \"\u274c\",\n        }\n\n    def add_step(\n        self,\n        step_id: str,\n        title: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new step to the progress tracking.\n\n        Args:\n            step_id: Unique identifier for the step\n            title: Display title for the step\n            total: Total number of items for progress tracking (optional)\n            insert_at: Position to insert step (None=append, int=index, str=after_step_id)\n        \"\"\"\n        if step_id in self.steps:\n            raise ValueError(f\"Step '{step_id}' already exists\")\n\n        self.steps[step_id] = {\n            \"title\": title,\n            \"total\": total,\n            \"progress\": 0,\n            \"state\": \"pending\",\n            \"error_msg\": None,\n            \"substep_progress\": 0.0,\n        }\n\n        # Handle positional insertion\n        if insert_at is None:\n            self.step_order.append(step_id)\n        elif isinstance(insert_at, int):\n            if 0 &lt;= insert_at &lt;= len(self.step_order):\n                self.step_order.insert(insert_at, step_id)\n            else:\n                self.step_order.append(step_id)\n        elif isinstance(insert_at, str):\n            try:\n                target_index = self.step_order.index(insert_at)\n                self.step_order.insert(target_index + 1, step_id)\n            except ValueError:\n                self.step_order.append(step_id)\n        else:\n            self.step_order.append(step_id)\n\n    def add_substep(\n        self,\n        parent_step_id: str,\n        substep_id: str,\n        description: str,\n        total: int = None,\n        insert_at: Union[None, int, str] = None,\n    ):\n        \"\"\"Add a new substep to a parent step.\n\n        Args:\n            parent_step_id: ID of the parent step\n            substep_id: Unique identifier for the substep\n            description: Display description for the substep\n            total: Total number of items for progress tracking (optional)\n            insert_at: Position to insert substep within parent\n        \"\"\"\n        if parent_step_id not in self.steps:\n            raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n        if parent_step_id not in self.substeps:\n            self.substeps[parent_step_id] = {}\n\n        if substep_id in self.substeps[parent_step_id]:\n            raise ValueError(\n                f\"Substep '{substep_id}' already exists in parent '{parent_step_id}'\"\n            )\n\n        substep_data = {\n            \"description\": description,\n            \"total\": total,\n            \"progress\": 0,\n            \"state\": \"pending\",\n            \"error_msg\": None,\n            \"parent_step_id\": parent_step_id,\n        }\n\n        # Handle positional insertion for substeps\n        parent_substeps = self.substeps[parent_step_id]\n        if insert_at is None:\n            parent_substeps[substep_id] = substep_data\n        elif isinstance(insert_at, int):\n            substep_items = list(parent_substeps.items())\n            if 0 &lt;= insert_at &lt;= len(substep_items):\n                substep_items.insert(insert_at, (substep_id, substep_data))\n            else:\n                substep_items.append((substep_id, substep_data))\n            self.substeps[parent_step_id] = dict(substep_items)\n        elif isinstance(insert_at, str):\n            substep_items = list(parent_substeps.items())\n            try:\n                target_index = next(\n                    i for i, (k, v) in enumerate(substep_items) if k == insert_at\n                )\n                substep_items.insert(target_index + 1, (substep_id, substep_data))\n                self.substeps[parent_step_id] = dict(substep_items)\n            except (StopIteration, ValueError):\n                parent_substeps[substep_id] = substep_data\n        else:\n            parent_substeps[substep_id] = substep_data\n\n    def start_step(self, step_id: str):\n        \"\"\"Start/activate a specific step.\"\"\"\n        if step_id not in self.steps:\n            raise ValueError(f\"Step '{step_id}' not found\")\n\n        # Complete any currently active step first\n        if self.active_step and self.steps[self.active_step][\"state\"] == \"active\":\n            self.complete_step(self.active_step)\n\n        self.active_step = step_id\n        self.steps[step_id][\"state\"] = \"active\"\n\n    def update_step(self, step_id: str, progress: float, total: int = None):\n        \"\"\"Update the progress of a specific step.\"\"\"\n        if not step_id or not isinstance(step_id, str):\n            raise ValueError(\"Invalid step_id: must be a non-empty string\")\n\n        if step_id not in self.steps:\n            raise ValueError(f\"Step '{step_id}' not found\")\n\n        if not isinstance(progress, (int, float)):\n            raise TypeError(\"Progress must be a number\")\n\n        step_info = self.steps[step_id]\n\n        # Handle optional total update\n        if total is not None:\n            if not isinstance(total, int) or total &lt;= 0:\n                raise ValueError(f\"total must be a positive integer, got {total}\")\n            if progress &gt; total:\n                raise ValueError(f\"Progress {progress} exceeds new total {total}\")\n            step_info[\"total\"] = total\n\n        # Validate progress bounds\n        if progress &lt; 0:\n            raise ValueError(f\"Progress cannot be negative, got {progress}\")\n\n        if step_info[\"total\"] is not None and progress &gt; step_info[\"total\"]:\n            raise ValueError(f\"Progress {progress} exceeds total {step_info['total']}\")\n\n        step_info[\"progress\"] = progress\n\n    def complete_step(self, step_id: str):\n        \"\"\"Mark a step as completed.\"\"\"\n        if step_id not in self.steps:\n            raise ValueError(f\"Step '{step_id}' not found\")\n\n        step_info = self.steps[step_id]\n        step_info[\"state\"] = \"completed\"\n\n        if step_info[\"total\"] is not None:\n            step_info[\"progress\"] = step_info[\"total\"]\n\n        if step_id == self.active_step:\n            self.active_step = None\n\n    def fail_step(self, step_id: str, error_msg: str = None):\n        \"\"\"Mark a step as failed.\"\"\"\n        if step_id not in self.steps:\n            raise ValueError(f\"Step '{step_id}' not found\")\n\n        step_info = self.steps[step_id]\n        step_info[\"state\"] = \"failed\"\n        step_info[\"error_msg\"] = error_msg\n\n        if step_id == self.active_step:\n            self.active_step = None\n\n    def start_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Start/activate a specific substep.\"\"\"\n        if parent_step_id not in self.steps:\n            raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n        if (\n            parent_step_id not in self.substeps\n            or substep_id not in self.substeps[parent_step_id]\n        ):\n            raise ValueError(\n                f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n            )\n\n        # Make sure parent step is active\n        if self.steps[parent_step_id][\"state\"] != \"active\":\n            self.steps[parent_step_id][\"state\"] = \"active\"\n            if not self.active_step:\n                self.active_step = parent_step_id\n\n        # Complete any currently active substep for this parent first\n        if parent_step_id in self.active_substeps:\n            current_active = self.active_substeps[parent_step_id]\n            if (\n                current_active\n                and current_active in self.substeps[parent_step_id]\n                and self.substeps[parent_step_id][current_active][\"state\"] == \"active\"\n            ):\n                self.complete_substep(parent_step_id, current_active)\n\n        self.active_substeps[parent_step_id] = substep_id\n        self.substeps[parent_step_id][substep_id][\"state\"] = \"active\"\n\n    def update_substep(\n        self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n    ):\n        \"\"\"Update the progress of a specific substep.\"\"\"\n        if parent_step_id not in self.steps:\n            raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n        if (\n            parent_step_id not in self.substeps\n            or substep_id not in self.substeps[parent_step_id]\n        ):\n            raise ValueError(\n                f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n            )\n\n        substep_info = self.substeps[parent_step_id][substep_id]\n\n        # Handle optional total update\n        if total is not None:\n            if not isinstance(total, int) or total &lt;= 0:\n                raise ValueError(f\"total must be a positive integer, got {total}\")\n            if progress &gt; total:\n                raise ValueError(f\"Progress {progress} exceeds new total {total}\")\n            substep_info[\"total\"] = total\n\n        # Validate progress bounds\n        if progress &lt; 0:\n            raise ValueError(f\"Progress cannot be negative, got {progress}\")\n\n        if substep_info[\"total\"] is not None and progress &gt; substep_info[\"total\"]:\n            raise ValueError(\n                f\"Progress {progress} exceeds total {substep_info['total']}\"\n            )\n\n        substep_info[\"progress\"] = progress\n        self._update_parent_progress(parent_step_id)\n\n    def complete_substep(self, parent_step_id: str, substep_id: str):\n        \"\"\"Mark a substep as completed.\"\"\"\n        if parent_step_id not in self.steps:\n            raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n        if (\n            parent_step_id not in self.substeps\n            or substep_id not in self.substeps[parent_step_id]\n        ):\n            raise ValueError(\n                f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n            )\n\n        substep_info = self.substeps[parent_step_id][substep_id]\n        substep_info[\"state\"] = \"completed\"\n\n        if substep_info[\"total\"] is not None:\n            substep_info[\"progress\"] = substep_info[\"total\"]\n\n        if (\n            parent_step_id in self.active_substeps\n            and self.active_substeps[parent_step_id] == substep_id\n        ):\n            self.active_substeps[parent_step_id] = None\n\n        self._update_parent_progress(parent_step_id)\n\n    def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n        \"\"\"Mark a substep as failed.\"\"\"\n        if parent_step_id not in self.steps:\n            raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n        if (\n            parent_step_id not in self.substeps\n            or substep_id not in self.substeps[parent_step_id]\n        ):\n            raise ValueError(\n                f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n            )\n\n        substep_info = self.substeps[parent_step_id][substep_id]\n        substep_info[\"state\"] = \"failed\"\n        substep_info[\"error_msg\"] = error_msg\n\n        if (\n            parent_step_id in self.active_substeps\n            and self.active_substeps[parent_step_id] == substep_id\n        ):\n            self.active_substeps[parent_step_id] = None\n\n    def _update_parent_progress(self, parent_step_id: str):\n        \"\"\"Update parent step progress based on substep completion.\"\"\"\n        if parent_step_id not in self.substeps or not self.substeps[parent_step_id]:\n            return\n\n        substeps = self.substeps[parent_step_id]\n        completed_substeps = sum(\n            1 for s in substeps.values() if s[\"state\"] == \"completed\"\n        )\n        total_substeps = len(substeps)\n\n        if total_substeps &gt; 0:\n            parent_step = self.steps[parent_step_id]\n            substep_progress_percentage = (completed_substeps / total_substeps) * 100\n            parent_step[\"substep_progress\"] = substep_progress_percentage\n\n            if parent_step[\"total\"] is not None:\n                parent_progress = (completed_substeps / total_substeps) * parent_step[\n                    \"total\"\n                ]\n                parent_step[\"progress\"] = parent_progress\n\n    def build_progress_table(self) -&gt; Table:\n        \"\"\"Build a Rich Table with current progress state.\"\"\"\n        table = Table(show_header=False, show_edge=False, pad_edge=False, box=None)\n        table.add_column(\"Status\", style=\"bold\", width=3, justify=\"center\")\n        table.add_column(\"Task\", ratio=1)\n\n        for step_id in self.step_order:\n            if step_id not in self.steps:\n                continue\n\n            step_info = self.steps[step_id]\n            symbol = self.SYMBOLS[step_info[\"state\"]]\n            title = step_info[\"title\"]\n\n            # Build step text with progress information\n            if step_info[\"total\"] is not None and step_info[\"state\"] in [\n                \"active\",\n                \"completed\",\n            ]:\n                percentage = (\n                    (step_info[\"progress\"] / step_info[\"total\"]) * 100\n                    if step_info[\"total\"] &gt; 0\n                    else 0\n                )\n                step_text = f\"{title} ({step_info['progress']}/{step_info['total']} - {percentage:.0f}%)\"\n            else:\n                step_text = title\n\n            # Add substep summary if exists\n            if step_id in self.substeps and self.substeps[step_id]:\n                substeps = self.substeps[step_id]\n                completed_substeps = sum(\n                    1 for s in substeps.values() if s[\"state\"] == \"completed\"\n                )\n                total_substeps = len(substeps)\n                if step_info[\"state\"] == \"active\" and total_substeps &gt; 0:\n                    substep_percent = (completed_substeps / total_substeps) * 100\n                    step_text += f\" [{substep_percent:.0f}% substeps]\"\n\n            # Add error message if failed\n            if step_info[\"state\"] == \"failed\" and step_info[\"error_msg\"]:\n                step_text += f\" - [red]{step_info['error_msg']}[/red]\"\n\n            # Style based on state\n            style = {\n                \"completed\": \"green\",\n                \"failed\": \"red\",\n                \"active\": \"yellow\",\n                \"pending\": \"dim white\",\n            }.get(step_info[\"state\"], \"dim white\")\n\n            table.add_row(symbol, Text(step_text, style=style))\n\n            # Add substep rows\n            if step_id in self.substeps and self.substeps[step_id]:\n                for substep_id, substep_info in self.substeps[step_id].items():\n                    substep_description = substep_info[\"description\"]\n\n                    # Build substep text with progress\n                    if substep_info[\"total\"] is not None and substep_info[\"state\"] in [\n                        \"active\",\n                        \"completed\",\n                    ]:\n                        substep_percentage = (\n                            (substep_info[\"progress\"] / substep_info[\"total\"]) * 100\n                            if substep_info[\"total\"] &gt; 0\n                            else 0\n                        )\n                        if substep_info[\"state\"] == \"active\":\n                            # Show inline progress bar for active substeps\n                            bar_width = 20\n                            filled_width = int((substep_percentage / 100) * bar_width)\n                            bar = \"\u2588\" * filled_width + \"\u2591\" * (bar_width - filled_width)\n                            substep_text = (\n                                f\"  \u2514\u2500 {substep_description} [{bar}] \"\n                                f\"({substep_info['progress']}/{substep_info['total']} - {substep_percentage:.0f}%)\"\n                            )\n                        else:\n                            substep_text = (\n                                f\"  \u2514\u2500 {substep_description} \"\n                                f\"({substep_info['progress']}/{substep_info['total']} - {substep_percentage:.0f}%)\"\n                            )\n                    else:\n                        substep_text = f\"  \u2514\u2500 {substep_description}\"\n\n                    # Add error message if failed\n                    if substep_info[\"state\"] == \"failed\" and substep_info[\"error_msg\"]:\n                        substep_text += f\" - [red]{substep_info['error_msg']}[/red]\"\n\n                    # Style based on state\n                    sub_style = {\n                        \"completed\": \"green\",\n                        \"failed\": \"red\",\n                        \"active\": \"yellow\",\n                        \"pending\": \"dim white\",\n                    }.get(substep_info[\"state\"], \"dim white\")\n\n                    table.add_row(\"\", Text(substep_text, style=sub_style))\n\n        return table\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize progress state tracking.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize progress state tracking.\"\"\"\n    # Progress tracking data structures\n    self.steps: Dict[str, dict] = {}\n    self.substeps: Dict[str, Dict[str, dict]] = {}\n    self.step_order: List[str] = []\n    self.active_step: Optional[str] = None\n    self.active_substeps: Dict[str, Optional[str]] = {}\n\n    # State symbols for different progress states\n    self.SYMBOLS = {\n        \"pending\": \"\u23f8\",\n        \"active\": \"\u23f3\",\n        \"completed\": \"\u2713\",\n        \"failed\": \"\u274c\",\n    }\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_step","title":"<code>add_step(step_id, title, total=None, insert_at=None)</code>","text":"<p>Add a new step to the progress tracking.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Unique identifier for the step</p> required <code>str</code> <p>Display title for the step</p> required <code>int</code> <p>Total number of items for progress tracking (optional)</p> <code>None</code> <code>Union[None, int, str]</code> <p>Position to insert step (None=append, int=index, str=after_step_id)</p> <code>None</code> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_step(\n    self,\n    step_id: str,\n    title: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new step to the progress tracking.\n\n    Args:\n        step_id: Unique identifier for the step\n        title: Display title for the step\n        total: Total number of items for progress tracking (optional)\n        insert_at: Position to insert step (None=append, int=index, str=after_step_id)\n    \"\"\"\n    if step_id in self.steps:\n        raise ValueError(f\"Step '{step_id}' already exists\")\n\n    self.steps[step_id] = {\n        \"title\": title,\n        \"total\": total,\n        \"progress\": 0,\n        \"state\": \"pending\",\n        \"error_msg\": None,\n        \"substep_progress\": 0.0,\n    }\n\n    # Handle positional insertion\n    if insert_at is None:\n        self.step_order.append(step_id)\n    elif isinstance(insert_at, int):\n        if 0 &lt;= insert_at &lt;= len(self.step_order):\n            self.step_order.insert(insert_at, step_id)\n        else:\n            self.step_order.append(step_id)\n    elif isinstance(insert_at, str):\n        try:\n            target_index = self.step_order.index(insert_at)\n            self.step_order.insert(target_index + 1, step_id)\n        except ValueError:\n            self.step_order.append(step_id)\n    else:\n        self.step_order.append(step_id)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_step(step_id)","title":"<code>step_id</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_step(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_step(total)","title":"<code>total</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_step(insert_at)","title":"<code>insert_at</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep","title":"<code>add_substep(parent_step_id, substep_id, description, total=None, insert_at=None)</code>","text":"<p>Add a new substep to a parent step.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>ID of the parent step</p> required <code>str</code> <p>Unique identifier for the substep</p> required <code>str</code> <p>Display description for the substep</p> required <code>int</code> <p>Total number of items for progress tracking (optional)</p> <code>None</code> <code>Union[None, int, str]</code> <p>Position to insert substep within parent</p> <code>None</code> Source code in <code>terminal_tools/progress.py</code> <pre><code>def add_substep(\n    self,\n    parent_step_id: str,\n    substep_id: str,\n    description: str,\n    total: int = None,\n    insert_at: Union[None, int, str] = None,\n):\n    \"\"\"Add a new substep to a parent step.\n\n    Args:\n        parent_step_id: ID of the parent step\n        substep_id: Unique identifier for the substep\n        description: Display description for the substep\n        total: Total number of items for progress tracking (optional)\n        insert_at: Position to insert substep within parent\n    \"\"\"\n    if parent_step_id not in self.steps:\n        raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n    if parent_step_id not in self.substeps:\n        self.substeps[parent_step_id] = {}\n\n    if substep_id in self.substeps[parent_step_id]:\n        raise ValueError(\n            f\"Substep '{substep_id}' already exists in parent '{parent_step_id}'\"\n        )\n\n    substep_data = {\n        \"description\": description,\n        \"total\": total,\n        \"progress\": 0,\n        \"state\": \"pending\",\n        \"error_msg\": None,\n        \"parent_step_id\": parent_step_id,\n    }\n\n    # Handle positional insertion for substeps\n    parent_substeps = self.substeps[parent_step_id]\n    if insert_at is None:\n        parent_substeps[substep_id] = substep_data\n    elif isinstance(insert_at, int):\n        substep_items = list(parent_substeps.items())\n        if 0 &lt;= insert_at &lt;= len(substep_items):\n            substep_items.insert(insert_at, (substep_id, substep_data))\n        else:\n            substep_items.append((substep_id, substep_data))\n        self.substeps[parent_step_id] = dict(substep_items)\n    elif isinstance(insert_at, str):\n        substep_items = list(parent_substeps.items())\n        try:\n            target_index = next(\n                i for i, (k, v) in enumerate(substep_items) if k == insert_at\n            )\n            substep_items.insert(target_index + 1, (substep_id, substep_data))\n            self.substeps[parent_step_id] = dict(substep_items)\n        except (StopIteration, ValueError):\n            parent_substeps[substep_id] = substep_data\n    else:\n        parent_substeps[substep_id] = substep_data\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep(parent_step_id)","title":"<code>parent_step_id</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep(substep_id)","title":"<code>substep_id</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep(description)","title":"<code>description</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep(total)","title":"<code>total</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.add_substep(insert_at)","title":"<code>insert_at</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.build_progress_table","title":"<code>build_progress_table()</code>","text":"<p>Build a Rich Table with current progress state.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def build_progress_table(self) -&gt; Table:\n    \"\"\"Build a Rich Table with current progress state.\"\"\"\n    table = Table(show_header=False, show_edge=False, pad_edge=False, box=None)\n    table.add_column(\"Status\", style=\"bold\", width=3, justify=\"center\")\n    table.add_column(\"Task\", ratio=1)\n\n    for step_id in self.step_order:\n        if step_id not in self.steps:\n            continue\n\n        step_info = self.steps[step_id]\n        symbol = self.SYMBOLS[step_info[\"state\"]]\n        title = step_info[\"title\"]\n\n        # Build step text with progress information\n        if step_info[\"total\"] is not None and step_info[\"state\"] in [\n            \"active\",\n            \"completed\",\n        ]:\n            percentage = (\n                (step_info[\"progress\"] / step_info[\"total\"]) * 100\n                if step_info[\"total\"] &gt; 0\n                else 0\n            )\n            step_text = f\"{title} ({step_info['progress']}/{step_info['total']} - {percentage:.0f}%)\"\n        else:\n            step_text = title\n\n        # Add substep summary if exists\n        if step_id in self.substeps and self.substeps[step_id]:\n            substeps = self.substeps[step_id]\n            completed_substeps = sum(\n                1 for s in substeps.values() if s[\"state\"] == \"completed\"\n            )\n            total_substeps = len(substeps)\n            if step_info[\"state\"] == \"active\" and total_substeps &gt; 0:\n                substep_percent = (completed_substeps / total_substeps) * 100\n                step_text += f\" [{substep_percent:.0f}% substeps]\"\n\n        # Add error message if failed\n        if step_info[\"state\"] == \"failed\" and step_info[\"error_msg\"]:\n            step_text += f\" - [red]{step_info['error_msg']}[/red]\"\n\n        # Style based on state\n        style = {\n            \"completed\": \"green\",\n            \"failed\": \"red\",\n            \"active\": \"yellow\",\n            \"pending\": \"dim white\",\n        }.get(step_info[\"state\"], \"dim white\")\n\n        table.add_row(symbol, Text(step_text, style=style))\n\n        # Add substep rows\n        if step_id in self.substeps and self.substeps[step_id]:\n            for substep_id, substep_info in self.substeps[step_id].items():\n                substep_description = substep_info[\"description\"]\n\n                # Build substep text with progress\n                if substep_info[\"total\"] is not None and substep_info[\"state\"] in [\n                    \"active\",\n                    \"completed\",\n                ]:\n                    substep_percentage = (\n                        (substep_info[\"progress\"] / substep_info[\"total\"]) * 100\n                        if substep_info[\"total\"] &gt; 0\n                        else 0\n                    )\n                    if substep_info[\"state\"] == \"active\":\n                        # Show inline progress bar for active substeps\n                        bar_width = 20\n                        filled_width = int((substep_percentage / 100) * bar_width)\n                        bar = \"\u2588\" * filled_width + \"\u2591\" * (bar_width - filled_width)\n                        substep_text = (\n                            f\"  \u2514\u2500 {substep_description} [{bar}] \"\n                            f\"({substep_info['progress']}/{substep_info['total']} - {substep_percentage:.0f}%)\"\n                        )\n                    else:\n                        substep_text = (\n                            f\"  \u2514\u2500 {substep_description} \"\n                            f\"({substep_info['progress']}/{substep_info['total']} - {substep_percentage:.0f}%)\"\n                        )\n                else:\n                    substep_text = f\"  \u2514\u2500 {substep_description}\"\n\n                # Add error message if failed\n                if substep_info[\"state\"] == \"failed\" and substep_info[\"error_msg\"]:\n                    substep_text += f\" - [red]{substep_info['error_msg']}[/red]\"\n\n                # Style based on state\n                sub_style = {\n                    \"completed\": \"green\",\n                    \"failed\": \"red\",\n                    \"active\": \"yellow\",\n                    \"pending\": \"dim white\",\n                }.get(substep_info[\"state\"], \"dim white\")\n\n                table.add_row(\"\", Text(substep_text, style=sub_style))\n\n    return table\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.complete_step","title":"<code>complete_step(step_id)</code>","text":"<p>Mark a step as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_step(self, step_id: str):\n    \"\"\"Mark a step as completed.\"\"\"\n    if step_id not in self.steps:\n        raise ValueError(f\"Step '{step_id}' not found\")\n\n    step_info = self.steps[step_id]\n    step_info[\"state\"] = \"completed\"\n\n    if step_info[\"total\"] is not None:\n        step_info[\"progress\"] = step_info[\"total\"]\n\n    if step_id == self.active_step:\n        self.active_step = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.complete_substep","title":"<code>complete_substep(parent_step_id, substep_id)</code>","text":"<p>Mark a substep as completed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def complete_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Mark a substep as completed.\"\"\"\n    if parent_step_id not in self.steps:\n        raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n    if (\n        parent_step_id not in self.substeps\n        or substep_id not in self.substeps[parent_step_id]\n    ):\n        raise ValueError(\n            f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n        )\n\n    substep_info = self.substeps[parent_step_id][substep_id]\n    substep_info[\"state\"] = \"completed\"\n\n    if substep_info[\"total\"] is not None:\n        substep_info[\"progress\"] = substep_info[\"total\"]\n\n    if (\n        parent_step_id in self.active_substeps\n        and self.active_substeps[parent_step_id] == substep_id\n    ):\n        self.active_substeps[parent_step_id] = None\n\n    self._update_parent_progress(parent_step_id)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.fail_step","title":"<code>fail_step(step_id, error_msg=None)</code>","text":"<p>Mark a step as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_step(self, step_id: str, error_msg: str = None):\n    \"\"\"Mark a step as failed.\"\"\"\n    if step_id not in self.steps:\n        raise ValueError(f\"Step '{step_id}' not found\")\n\n    step_info = self.steps[step_id]\n    step_info[\"state\"] = \"failed\"\n    step_info[\"error_msg\"] = error_msg\n\n    if step_id == self.active_step:\n        self.active_step = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.fail_substep","title":"<code>fail_substep(parent_step_id, substep_id, error_msg=None)</code>","text":"<p>Mark a substep as failed.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def fail_substep(self, parent_step_id: str, substep_id: str, error_msg: str = None):\n    \"\"\"Mark a substep as failed.\"\"\"\n    if parent_step_id not in self.steps:\n        raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n    if (\n        parent_step_id not in self.substeps\n        or substep_id not in self.substeps[parent_step_id]\n    ):\n        raise ValueError(\n            f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n        )\n\n    substep_info = self.substeps[parent_step_id][substep_id]\n    substep_info[\"state\"] = \"failed\"\n    substep_info[\"error_msg\"] = error_msg\n\n    if (\n        parent_step_id in self.active_substeps\n        and self.active_substeps[parent_step_id] == substep_id\n    ):\n        self.active_substeps[parent_step_id] = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.start_step","title":"<code>start_step(step_id)</code>","text":"<p>Start/activate a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_step(self, step_id: str):\n    \"\"\"Start/activate a specific step.\"\"\"\n    if step_id not in self.steps:\n        raise ValueError(f\"Step '{step_id}' not found\")\n\n    # Complete any currently active step first\n    if self.active_step and self.steps[self.active_step][\"state\"] == \"active\":\n        self.complete_step(self.active_step)\n\n    self.active_step = step_id\n    self.steps[step_id][\"state\"] = \"active\"\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.start_substep","title":"<code>start_substep(parent_step_id, substep_id)</code>","text":"<p>Start/activate a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start_substep(self, parent_step_id: str, substep_id: str):\n    \"\"\"Start/activate a specific substep.\"\"\"\n    if parent_step_id not in self.steps:\n        raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n    if (\n        parent_step_id not in self.substeps\n        or substep_id not in self.substeps[parent_step_id]\n    ):\n        raise ValueError(\n            f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n        )\n\n    # Make sure parent step is active\n    if self.steps[parent_step_id][\"state\"] != \"active\":\n        self.steps[parent_step_id][\"state\"] = \"active\"\n        if not self.active_step:\n            self.active_step = parent_step_id\n\n    # Complete any currently active substep for this parent first\n    if parent_step_id in self.active_substeps:\n        current_active = self.active_substeps[parent_step_id]\n        if (\n            current_active\n            and current_active in self.substeps[parent_step_id]\n            and self.substeps[parent_step_id][current_active][\"state\"] == \"active\"\n        ):\n            self.complete_substep(parent_step_id, current_active)\n\n    self.active_substeps[parent_step_id] = substep_id\n    self.substeps[parent_step_id][substep_id][\"state\"] = \"active\"\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.update_step","title":"<code>update_step(step_id, progress, total=None)</code>","text":"<p>Update the progress of a specific step.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_step(self, step_id: str, progress: float, total: int = None):\n    \"\"\"Update the progress of a specific step.\"\"\"\n    if not step_id or not isinstance(step_id, str):\n        raise ValueError(\"Invalid step_id: must be a non-empty string\")\n\n    if step_id not in self.steps:\n        raise ValueError(f\"Step '{step_id}' not found\")\n\n    if not isinstance(progress, (int, float)):\n        raise TypeError(\"Progress must be a number\")\n\n    step_info = self.steps[step_id]\n\n    # Handle optional total update\n    if total is not None:\n        if not isinstance(total, int) or total &lt;= 0:\n            raise ValueError(f\"total must be a positive integer, got {total}\")\n        if progress &gt; total:\n            raise ValueError(f\"Progress {progress} exceeds new total {total}\")\n        step_info[\"total\"] = total\n\n    # Validate progress bounds\n    if progress &lt; 0:\n        raise ValueError(f\"Progress cannot be negative, got {progress}\")\n\n    if step_info[\"total\"] is not None and progress &gt; step_info[\"total\"]:\n        raise ValueError(f\"Progress {progress} exceeds total {step_info['total']}\")\n\n    step_info[\"progress\"] = progress\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.ProgressStateManager.update_substep","title":"<code>update_substep(parent_step_id, substep_id, progress, total=None)</code>","text":"<p>Update the progress of a specific substep.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_substep(\n    self, parent_step_id: str, substep_id: str, progress: int, total: int = None\n):\n    \"\"\"Update the progress of a specific substep.\"\"\"\n    if parent_step_id not in self.steps:\n        raise ValueError(f\"Parent step '{parent_step_id}' not found\")\n\n    if (\n        parent_step_id not in self.substeps\n        or substep_id not in self.substeps[parent_step_id]\n    ):\n        raise ValueError(\n            f\"Substep '{substep_id}' not found in parent '{parent_step_id}'\"\n        )\n\n    substep_info = self.substeps[parent_step_id][substep_id]\n\n    # Handle optional total update\n    if total is not None:\n        if not isinstance(total, int) or total &lt;= 0:\n            raise ValueError(f\"total must be a positive integer, got {total}\")\n        if progress &gt; total:\n            raise ValueError(f\"Progress {progress} exceeds new total {total}\")\n        substep_info[\"total\"] = total\n\n    # Validate progress bounds\n    if progress &lt; 0:\n        raise ValueError(f\"Progress cannot be negative, got {progress}\")\n\n    if substep_info[\"total\"] is not None and progress &gt; substep_info[\"total\"]:\n        raise ValueError(\n            f\"Progress {progress} exceeds total {substep_info['total']}\"\n        )\n\n    substep_info[\"progress\"] = progress\n    self._update_parent_progress(parent_step_id)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay","title":"<code>RichProgressDisplay</code>","text":"<p>Rich Live-based progress display for hierarchical progress tracking.</p> <p>Provides smooth progress updates using Rich Live display with table rendering for hierarchical progress visualization.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class RichProgressDisplay:\n    \"\"\"Rich Live-based progress display for hierarchical progress tracking.\n\n    Provides smooth progress updates using Rich Live display\n    with table rendering for hierarchical progress visualization.\n    \"\"\"\n\n    def __init__(self, title: str):\n        \"\"\"Initialize Rich progress display.\n\n        Args:\n            title: Title for the progress display\n        \"\"\"\n        self.title = title\n        self.console = Console()\n        self.live: Optional[Live] = None\n        self._running = False\n\n    def start(self) -&gt; None:\n        \"\"\"Start the Rich Live display.\"\"\"\n        if not self._running:\n            self._running = True\n            # Create initial empty table\n            initial_table = Table(\n                show_header=False, show_edge=False, pad_edge=False, box=None\n            )\n            initial_table.add_column(\"Status\", style=\"bold\", width=3, justify=\"center\")\n            initial_table.add_column(\"Task\", ratio=1)\n\n            panel = Panel(initial_table, title=self.title, border_style=\"blue\")\n            self.live = Live(panel, console=self.console, refresh_per_second=10)\n            self.live.start()\n\n    def update_table(self, table: Table) -&gt; None:\n        \"\"\"Update the progress table (thread-safe).\"\"\"\n        if self._running and self.live:\n            panel = Panel(table, title=self.title, border_style=\"blue\")\n            self.live.update(panel)\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop the Rich Live display.\"\"\"\n        if self._running and self.live:\n            self._running = False\n            self.live.stop()\n            self.live = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay.__init__","title":"<code>__init__(title)</code>","text":"<p>Initialize Rich progress display.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Title for the progress display</p> required Source code in <code>terminal_tools/progress.py</code> <pre><code>def __init__(self, title: str):\n    \"\"\"Initialize Rich progress display.\n\n    Args:\n        title: Title for the progress display\n    \"\"\"\n    self.title = title\n    self.console = Console()\n    self.live: Optional[Live] = None\n    self._running = False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay.__init__(title)","title":"<code>title</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay.start","title":"<code>start()</code>","text":"<p>Start the Rich Live display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the Rich Live display.\"\"\"\n    if not self._running:\n        self._running = True\n        # Create initial empty table\n        initial_table = Table(\n            show_header=False, show_edge=False, pad_edge=False, box=None\n        )\n        initial_table.add_column(\"Status\", style=\"bold\", width=3, justify=\"center\")\n        initial_table.add_column(\"Task\", ratio=1)\n\n        panel = Panel(initial_table, title=self.title, border_style=\"blue\")\n        self.live = Live(panel, console=self.console, refresh_per_second=10)\n        self.live.start()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay.stop","title":"<code>stop()</code>","text":"<p>Stop the Rich Live display.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the Rich Live display.\"\"\"\n    if self._running and self.live:\n        self._running = False\n        self.live.stop()\n        self.live = None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.RichProgressDisplay.update_table","title":"<code>update_table(table)</code>","text":"<p>Update the progress table (thread-safe).</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>def update_table(self, table: Table) -&gt; None:\n    \"\"\"Update the progress table (thread-safe).\"\"\"\n    if self._running and self.live:\n        panel = Panel(table, title=self.title, border_style=\"blue\")\n        self.live.update(panel)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.SimpleProgressApp","title":"<code>SimpleProgressApp</code>","text":"<p>Stub class when Textual is not available.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class SimpleProgressApp:\n    \"\"\"Stub class when Textual is not available.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.progress.TextualInlineProgressDisplay","title":"<code>TextualInlineProgressDisplay</code>","text":"<p>Fallback to Rich display when Textual is not available.</p> Source code in <code>terminal_tools/progress.py</code> <pre><code>class TextualInlineProgressDisplay:\n    \"\"\"Fallback to Rich display when Textual is not available.\"\"\"\n\n    def __init__(self, title: str):\n        self.rich_display = RichProgressDisplay(title)\n\n    def start(self) -&gt; None:\n        self.rich_display.start()\n\n    def update_table(self, table: Table) -&gt; None:\n        self.rich_display.update_table(table)\n\n    def stop(self) -&gt; None:\n        self.rich_display.stop()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts","title":"<code>prompts</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.checkbox","title":"<code>checkbox(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s checkbox and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def checkbox(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s checkbox and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_checkbox(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.confirm","title":"<code>confirm(message, *, cancel_fallback=False, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s confirm input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def confirm(message: str, *, cancel_fallback: Optional[bool] = False, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s confirm input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(\n        lambda: inquirer_confirm(message, **kwargs), cancel_fallback\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector","title":"<code>file_selector(message='select a file', *, state=None)</code>","text":"<p>Lets the user select a file from the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt message. Defaults to \"select a file\".</p> <code>'select a file'</code> <code>str</code> <p>Where to start the directory listing. Defaults to current working directory.</p> required <p>Returns:</p> Type Description <code>(str, optional)</code> <p>The absolute path selected by the user, or None if the user cancels the prompt.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def file_selector(\n    message: str = \"select a file\", *, state: Optional[FileSelectorStateManager] = None\n):\n    \"\"\"Lets the user select a file from the filesystem.\n\n    Args:\n        message (str, optional): The prompt message. Defaults to \"select a file\".\n        initial_path (str, optional): Where to start the directory listing.\n          Defaults to current working directory.\n\n    Returns:\n        (str, optional): The absolute path selected by the user, or None if the\n          user cancels the prompt.\n    \"\"\"\n    initial_dir = state and state.get_current_path()\n    if initial_dir and not os.path.isdir(initial_dir):\n        initial_dir = None\n\n    current_path = os.path.realpath(initial_dir or os.curdir)\n\n    if os.name == \"nt\":\n        drives = get_drives()\n        drive_choices = [(drive, drive) for drive in drives]\n\n    def is_dir(entry: str):\n        return os.path.isdir(os.path.join(current_path, entry))\n\n    while True:\n        print(f\"current path: {current_path}\")\n        choices = [\n            (\"[..]\", \"..\"),\n            *(\n                (f\"[{entry}]\" if is_dir(entry) else entry, entry)\n                for entry in sorted(os.listdir(current_path))\n            ),\n        ]\n\n        # Add change drive option to the list of choices if on Windows\n        if os.name == \"nt\":\n            cur_drive = os.path.splitdrive(current_path)[0]\n            choices.insert(\n                0, (f\"[Change Drive (current - {cur_drive})]\", \"change_drive\")\n            )\n\n        selected_entry = list_input(message, choices=choices)\n\n        if selected_entry is not None and selected_entry == \"change_drive\":\n            selected_drive = list_input(\"Select a drive:\", choices=drive_choices)\n            if selected_drive is None:\n                return None\n\n            current_path = selected_entry = f\"{selected_drive}\\\\\"\n            # clear the prompted lines\n            clear_printed_lines(len(drives) + 1)\n\n        # inquirer will show up to 14 lines including the header\n        # we have one line for the current path to rewrite\n        clear_printed_lines(min(len(choices), 13) + 2)\n\n        if selected_entry is None:\n            return None\n\n        if is_dir(selected_entry):\n            current_path = os.path.realpath(os.path.join(current_path, selected_entry))\n        else:\n            if state is not None:\n                state.set_current_path(current_path)\n            return os.path.join(current_path, selected_entry)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector(message)","title":"<code>message</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector(initial_path)","title":"<code>initial_path</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.get_drives","title":"<code>get_drives()</code>","text":"<p>Returns a list of the logically assigned drives on a windows system.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of drive letters available and accessible on the system.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def get_drives():\n    \"\"\"\n    Returns a list of the logically assigned drives on a windows system.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of drive letters available and accessible on the system.\n    \"\"\"\n\n    drives = []\n    bitmask = windll.kernel32.GetLogicalDrives()\n\n    for letter in ascii_uppercase:\n        if bitmask &amp; 1:\n            drives.append(letter + \":\")\n        bitmask &gt;&gt;= 1\n\n    return drives\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.int_input","title":"<code>int_input(message, *, min=None, max=None, default=None, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def int_input(\n    message: str,\n    *,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n    default: Optional[int] = None,\n    **kwargs,\n) -&gt; Optional[int]:\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n\n    def validate_value(value):\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValidationError(\"Please enter a valid integer.\")\n\n        if min is not None and value &lt; min:\n            raise ValidationError(\n                f\"Please enter a value greater than or equal to {min}.\"\n            )\n\n        if max is not None and value &gt; max:\n            raise ValidationError(f\"Please enter a value less than or equal to {max}.\")\n\n        return True\n\n    result = wrap_keyboard_interrupt(\n        lambda: inquirer_text(\n            message,\n            validate=lambda previous_answers, value: validate_value(value),\n            default=str(default) if default is not None else None,\n            **kwargs,\n        ),\n        None,\n    )\n    return int(result) if result is not None else None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.list_input","title":"<code>list_input(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s list input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def list_input(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s list input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_list_input(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.text","title":"<code>text(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def text(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_text(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.wrap_keyboard_interrupt","title":"<code>wrap_keyboard_interrupt(fn, fallback=None)</code>","text":"<p>Calls <code>fn</code> and catches KeyboardInterrupt, returning <code>fallback</code> if it occurs.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def wrap_keyboard_interrupt(fn, fallback=None):\n    \"\"\"\n    Calls `fn` and catches KeyboardInterrupt, returning `fallback` if it occurs.\n    \"\"\"\n    try:\n        return fn()\n    except KeyboardInterrupt:\n        return fallback\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress","title":"<code>test_progress</code>","text":"<p>Tests for progress reporting functionality.</p> <p>Validates ProgressReporter and ProgressManager behavior including hierarchical progress tracking, memory integration, and positional insertion.</p>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager","title":"<code>TestProgressManager</code>","text":"<p>               Bases: <code>TestCase</code></p> <p>Test suite for ProgressManager functionality.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>class TestProgressManager(unittest.TestCase):\n    \"\"\"Test suite for ProgressManager functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up a ProgressManager instance for testing.\"\"\"\n        self.progress_manager = ProgressManager(\"Test Progress\")\n\n    def test_basic_initialization(self):\n        \"\"\"Test ProgressManager initializes correctly.\"\"\"\n        self.assertEqual(self.progress_manager.title, \"Test Progress\")\n        self.assertEqual(len(self.progress_manager.steps), 0)\n        self.assertEqual(len(self.progress_manager.substeps), 0)\n        self.assertEqual(len(self.progress_manager.step_order), 0)\n        self.assertIsNone(self.progress_manager.active_step)\n        self.assertFalse(self.progress_manager._started)\n\n    def test_add_step_basic(self):\n        \"\"\"Test basic step addition functionality.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\", total=100)\n\n        self.assertIn(\"step1\", self.progress_manager.steps)\n        self.assertEqual(self.progress_manager.steps[\"step1\"][\"title\"], \"First Step\")\n        self.assertEqual(self.progress_manager.steps[\"step1\"][\"total\"], 100)\n        self.assertEqual(self.progress_manager.steps[\"step1\"][\"progress\"], 0)\n        self.assertEqual(self.progress_manager.steps[\"step1\"][\"state\"], \"pending\")\n        self.assertEqual(self.progress_manager.step_order, [\"step1\"])\n\n    def test_add_step_positional_insertion_at_end(self):\n        \"\"\"Test adding steps with insert_at=None (default behavior).\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n        self.progress_manager.add_step(\"step2\", \"Second Step\")\n        self.progress_manager.add_step(\"step3\", \"Third Step\", insert_at=None)\n\n        self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n    def test_add_step_positional_insertion_at_index(self):\n        \"\"\"Test adding steps with numeric insert_at positions.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n        self.progress_manager.add_step(\"step3\", \"Third Step\")\n\n        # Insert at position 1 (between step1 and step3)\n        self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=1)\n        self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n        # Insert at position 0 (beginning)\n        self.progress_manager.add_step(\"step0\", \"Zero Step\", insert_at=0)\n        self.assertEqual(\n            self.progress_manager.step_order, [\"step0\", \"step1\", \"step2\", \"step3\"]\n        )\n\n    def test_add_step_positional_insertion_after_step(self):\n        \"\"\"Test adding steps with string insert_at (after named step).\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n        self.progress_manager.add_step(\"step3\", \"Third Step\")\n\n        # Insert after step1\n        self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=\"step1\")\n        self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n        # Insert after step2\n        self.progress_manager.add_step(\"step2_5\", \"Step 2.5\", insert_at=\"step2\")\n        self.assertEqual(\n            self.progress_manager.step_order, [\"step1\", \"step2\", \"step2_5\", \"step3\"]\n        )\n\n    def test_add_step_positional_insertion_fallbacks(self):\n        \"\"\"Test fallback behavior for invalid insert_at values.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n\n        # Test invalid index (too large) - should fallback to append\n        self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=99)\n        self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\"])\n\n        # Test negative index - should fallback to append\n        self.progress_manager.add_step(\"step3\", \"Third Step\", insert_at=-1)\n        self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n        # Test non-existent step name - should fallback to append\n        self.progress_manager.add_step(\"step4\", \"Fourth Step\", insert_at=\"nonexistent\")\n        self.assertEqual(\n            self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\", \"step4\"]\n        )\n\n        # Test invalid type - should fallback to append\n        self.progress_manager.add_step(\"step5\", \"Fifth Step\", insert_at=3.14)\n        self.assertEqual(\n            self.progress_manager.step_order,\n            [\"step1\", \"step2\", \"step3\", \"step4\", \"step5\"],\n        )\n\n    def test_add_substep_basic(self):\n        \"\"\"Test basic substep addition functionality.\"\"\"\n        self.progress_manager.add_step(\"parent\", \"Parent Step\")\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\", total=50)\n\n        self.assertIn(\"parent\", self.progress_manager.substeps)\n        self.assertIn(\"sub1\", self.progress_manager.substeps[\"parent\"])\n\n        substep = self.progress_manager.substeps[\"parent\"][\"sub1\"]\n        self.assertEqual(substep[\"description\"], \"First substep\")\n        self.assertEqual(substep[\"total\"], 50)\n        self.assertEqual(substep[\"progress\"], 0)\n        self.assertEqual(substep[\"state\"], \"pending\")\n        self.assertEqual(substep[\"parent_step_id\"], \"parent\")\n\n    def test_add_substep_positional_insertion_at_index(self):\n        \"\"\"Test adding substeps with numeric insert_at positions.\"\"\"\n        self.progress_manager.add_step(\"parent\", \"Parent Step\")\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n        self.progress_manager.add_substep(\"parent\", \"sub3\", \"Third substep\")\n\n        # Insert at position 1\n        self.progress_manager.add_substep(\n            \"parent\", \"sub2\", \"Second substep\", insert_at=1\n        )\n        substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n        self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n\n    def test_add_substep_positional_insertion_after_substep(self):\n        \"\"\"Test adding substeps with string insert_at (after named substep).\"\"\"\n        self.progress_manager.add_step(\"parent\", \"Parent Step\")\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n        self.progress_manager.add_substep(\"parent\", \"sub3\", \"Third substep\")\n\n        # Insert after sub1\n        self.progress_manager.add_substep(\n            \"parent\", \"sub2\", \"Second substep\", insert_at=\"sub1\"\n        )\n        substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n        self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n\n    def test_add_substep_positional_insertion_fallbacks(self):\n        \"\"\"Test fallback behavior for invalid substep insert_at values.\"\"\"\n        self.progress_manager.add_step(\"parent\", \"Parent Step\")\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n\n        # Test invalid index - should fallback to append\n        self.progress_manager.add_substep(\n            \"parent\", \"sub2\", \"Second substep\", insert_at=99\n        )\n        substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n        self.assertEqual(substep_order, [\"sub1\", \"sub2\"])\n\n        # Test non-existent substep name - should fallback to append\n        self.progress_manager.add_substep(\n            \"parent\", \"sub3\", \"Third substep\", insert_at=\"nonexistent\"\n        )\n        substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n        self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n\n    def test_add_step_duplicate_error(self):\n        \"\"\"Test that adding duplicate step IDs raises ValueError.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n        with self.assertRaises(ValueError) as context:\n            self.progress_manager.add_step(\"step1\", \"Duplicate Step\")\n        self.assertIn(\"already exists\", str(context.exception))\n\n    def test_add_substep_validation_errors(self):\n        \"\"\"Test substep validation error handling.\"\"\"\n        # Test parent step not found\n        with self.assertRaises(ValueError) as context:\n            self.progress_manager.add_substep(\"nonexistent\", \"sub1\", \"Test substep\")\n        self.assertIn(\"not found\", str(context.exception))\n\n        # Test duplicate substep ID\n        self.progress_manager.add_step(\"parent\", \"Parent Step\")\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n        with self.assertRaises(ValueError) as context:\n            self.progress_manager.add_substep(\"parent\", \"sub1\", \"Duplicate substep\")\n        self.assertIn(\"already exists\", str(context.exception))\n\n    def test_context_manager_protocol(self):\n        \"\"\"Test that ProgressManager supports context manager protocol.\"\"\"\n        with self.progress_manager as pm:\n            self.assertTrue(pm._started)\n            self.assertIs(pm, self.progress_manager)\n\n        # After context exit, should be finished\n        self.assertFalse(self.progress_manager._started)\n\n    def test_display_properties(self):\n        \"\"\"Test display-related properties.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"Test Step\")\n\n        with self.progress_manager:\n            self.assertIsNone(self.progress_manager.live)\n\n    def test_api_compatibility(self):\n        \"\"\"Test backward compatibility API methods.\"\"\"\n        # Test that all key methods exist and have correct signatures\n        self.assertTrue(hasattr(self.progress_manager, \"add_step\"))\n        self.assertTrue(hasattr(self.progress_manager, \"add_substep\"))\n        self.assertTrue(hasattr(self.progress_manager, \"start_step\"))\n        self.assertTrue(hasattr(self.progress_manager, \"update_step\"))\n        self.assertTrue(hasattr(self.progress_manager, \"complete_step\"))\n        self.assertTrue(hasattr(self.progress_manager, \"fail_step\"))\n        self.assertTrue(hasattr(self.progress_manager, \"start_substep\"))\n        self.assertTrue(hasattr(self.progress_manager, \"update_substep\"))\n        self.assertTrue(hasattr(self.progress_manager, \"complete_substep\"))\n        self.assertTrue(hasattr(self.progress_manager, \"fail_substep\"))\n        self.assertTrue(hasattr(self.progress_manager, \"update_step_with_memory\"))\n        self.assertTrue(hasattr(self.progress_manager, \"display_memory_summary\"))\n\n    def test_complex_positional_insertion_scenario(self):\n        \"\"\"Test complex scenario with mixed positional insertions.\"\"\"\n        # Simulate a real-world scenario with dynamic step insertion\n        self.progress_manager.add_step(\"load_data\", \"Loading data\")\n        self.progress_manager.add_step(\"analyze\", \"Analyzing\")\n        self.progress_manager.add_step(\"export\", \"Exporting results\")\n\n        # Insert preprocessing step after data loading\n        self.progress_manager.add_step(\n            \"preprocess\", \"Preprocessing data\", insert_at=\"load_data\"\n        )\n\n        # Insert validation step at the beginning\n        self.progress_manager.add_step(\"validate\", \"Validating inputs\", insert_at=0)\n\n        # Insert cleanup step at end\n        self.progress_manager.add_step(\"cleanup\", \"Cleaning up\")\n\n        expected_order = [\n            \"validate\",\n            \"load_data\",\n            \"preprocess\",\n            \"analyze\",\n            \"export\",\n            \"cleanup\",\n        ]\n        self.assertEqual(self.progress_manager.step_order, expected_order)\n\n        # Add substeps with positional insertion\n        self.progress_manager.add_substep(\"preprocess\", \"filter\", \"Filtering data\")\n        self.progress_manager.add_substep(\"preprocess\", \"normalize\", \"Normalizing data\")\n        self.progress_manager.add_substep(\n            \"preprocess\", \"validate_schema\", \"Validating schema\", insert_at=\"filter\"\n        )\n\n        substep_order = list(self.progress_manager.substeps[\"preprocess\"].keys())\n        expected_substeps = [\"filter\", \"validate_schema\", \"normalize\"]\n        self.assertEqual(substep_order, expected_substeps)\n\n    def test_memory_manager_integration(self):\n        \"\"\"Test ProgressManager integration with memory manager.\"\"\"\n        # Test with mock memory manager\n        from unittest.mock import Mock\n\n        mock_memory_manager = Mock()\n\n        pm_with_memory = ProgressManager(\n            \"Test with Memory\", memory_manager=mock_memory_manager\n        )\n        self.assertEqual(pm_with_memory.memory_manager, mock_memory_manager)\n        self.assertIsNotNone(pm_with_memory.last_memory_warning)\n\n    def test_table_rebuild_functionality(self):\n        \"\"\"Test table rebuilding with positional insertion.\"\"\"\n        self.progress_manager.add_step(\"step1\", \"First Step\")\n        self.progress_manager.add_step(\"step2\", \"Second Step\")\n\n        self.progress_manager._rebuild_table()\n        self.assertIsNotNone(self.progress_manager.table)\n\n        self.progress_manager.add_substep(\"step1\", \"sub1\", \"Substep 1\")\n        self.progress_manager._rebuild_table()\n        self.assertIsNotNone(self.progress_manager.table)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.setUp","title":"<code>setUp()</code>","text":"<p>Set up a ProgressManager instance for testing.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def setUp(self):\n    \"\"\"Set up a ProgressManager instance for testing.\"\"\"\n    self.progress_manager = ProgressManager(\"Test Progress\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_basic","title":"<code>test_add_step_basic()</code>","text":"<p>Test basic step addition functionality.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_basic(self):\n    \"\"\"Test basic step addition functionality.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\", total=100)\n\n    self.assertIn(\"step1\", self.progress_manager.steps)\n    self.assertEqual(self.progress_manager.steps[\"step1\"][\"title\"], \"First Step\")\n    self.assertEqual(self.progress_manager.steps[\"step1\"][\"total\"], 100)\n    self.assertEqual(self.progress_manager.steps[\"step1\"][\"progress\"], 0)\n    self.assertEqual(self.progress_manager.steps[\"step1\"][\"state\"], \"pending\")\n    self.assertEqual(self.progress_manager.step_order, [\"step1\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_duplicate_error","title":"<code>test_add_step_duplicate_error()</code>","text":"<p>Test that adding duplicate step IDs raises ValueError.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_duplicate_error(self):\n    \"\"\"Test that adding duplicate step IDs raises ValueError.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n    with self.assertRaises(ValueError) as context:\n        self.progress_manager.add_step(\"step1\", \"Duplicate Step\")\n    self.assertIn(\"already exists\", str(context.exception))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_positional_insertion_after_step","title":"<code>test_add_step_positional_insertion_after_step()</code>","text":"<p>Test adding steps with string insert_at (after named step).</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_positional_insertion_after_step(self):\n    \"\"\"Test adding steps with string insert_at (after named step).\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n    self.progress_manager.add_step(\"step3\", \"Third Step\")\n\n    # Insert after step1\n    self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=\"step1\")\n    self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n    # Insert after step2\n    self.progress_manager.add_step(\"step2_5\", \"Step 2.5\", insert_at=\"step2\")\n    self.assertEqual(\n        self.progress_manager.step_order, [\"step1\", \"step2\", \"step2_5\", \"step3\"]\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_positional_insertion_at_end","title":"<code>test_add_step_positional_insertion_at_end()</code>","text":"<p>Test adding steps with insert_at=None (default behavior).</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_positional_insertion_at_end(self):\n    \"\"\"Test adding steps with insert_at=None (default behavior).\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n    self.progress_manager.add_step(\"step2\", \"Second Step\")\n    self.progress_manager.add_step(\"step3\", \"Third Step\", insert_at=None)\n\n    self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_positional_insertion_at_index","title":"<code>test_add_step_positional_insertion_at_index()</code>","text":"<p>Test adding steps with numeric insert_at positions.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_positional_insertion_at_index(self):\n    \"\"\"Test adding steps with numeric insert_at positions.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n    self.progress_manager.add_step(\"step3\", \"Third Step\")\n\n    # Insert at position 1 (between step1 and step3)\n    self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=1)\n    self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n    # Insert at position 0 (beginning)\n    self.progress_manager.add_step(\"step0\", \"Zero Step\", insert_at=0)\n    self.assertEqual(\n        self.progress_manager.step_order, [\"step0\", \"step1\", \"step2\", \"step3\"]\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_step_positional_insertion_fallbacks","title":"<code>test_add_step_positional_insertion_fallbacks()</code>","text":"<p>Test fallback behavior for invalid insert_at values.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_step_positional_insertion_fallbacks(self):\n    \"\"\"Test fallback behavior for invalid insert_at values.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n\n    # Test invalid index (too large) - should fallback to append\n    self.progress_manager.add_step(\"step2\", \"Second Step\", insert_at=99)\n    self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\"])\n\n    # Test negative index - should fallback to append\n    self.progress_manager.add_step(\"step3\", \"Third Step\", insert_at=-1)\n    self.assertEqual(self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\"])\n\n    # Test non-existent step name - should fallback to append\n    self.progress_manager.add_step(\"step4\", \"Fourth Step\", insert_at=\"nonexistent\")\n    self.assertEqual(\n        self.progress_manager.step_order, [\"step1\", \"step2\", \"step3\", \"step4\"]\n    )\n\n    # Test invalid type - should fallback to append\n    self.progress_manager.add_step(\"step5\", \"Fifth Step\", insert_at=3.14)\n    self.assertEqual(\n        self.progress_manager.step_order,\n        [\"step1\", \"step2\", \"step3\", \"step4\", \"step5\"],\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_substep_basic","title":"<code>test_add_substep_basic()</code>","text":"<p>Test basic substep addition functionality.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_substep_basic(self):\n    \"\"\"Test basic substep addition functionality.\"\"\"\n    self.progress_manager.add_step(\"parent\", \"Parent Step\")\n    self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\", total=50)\n\n    self.assertIn(\"parent\", self.progress_manager.substeps)\n    self.assertIn(\"sub1\", self.progress_manager.substeps[\"parent\"])\n\n    substep = self.progress_manager.substeps[\"parent\"][\"sub1\"]\n    self.assertEqual(substep[\"description\"], \"First substep\")\n    self.assertEqual(substep[\"total\"], 50)\n    self.assertEqual(substep[\"progress\"], 0)\n    self.assertEqual(substep[\"state\"], \"pending\")\n    self.assertEqual(substep[\"parent_step_id\"], \"parent\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_substep_positional_insertion_after_substep","title":"<code>test_add_substep_positional_insertion_after_substep()</code>","text":"<p>Test adding substeps with string insert_at (after named substep).</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_substep_positional_insertion_after_substep(self):\n    \"\"\"Test adding substeps with string insert_at (after named substep).\"\"\"\n    self.progress_manager.add_step(\"parent\", \"Parent Step\")\n    self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n    self.progress_manager.add_substep(\"parent\", \"sub3\", \"Third substep\")\n\n    # Insert after sub1\n    self.progress_manager.add_substep(\n        \"parent\", \"sub2\", \"Second substep\", insert_at=\"sub1\"\n    )\n    substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n    self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_substep_positional_insertion_at_index","title":"<code>test_add_substep_positional_insertion_at_index()</code>","text":"<p>Test adding substeps with numeric insert_at positions.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_substep_positional_insertion_at_index(self):\n    \"\"\"Test adding substeps with numeric insert_at positions.\"\"\"\n    self.progress_manager.add_step(\"parent\", \"Parent Step\")\n    self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n    self.progress_manager.add_substep(\"parent\", \"sub3\", \"Third substep\")\n\n    # Insert at position 1\n    self.progress_manager.add_substep(\n        \"parent\", \"sub2\", \"Second substep\", insert_at=1\n    )\n    substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n    self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_substep_positional_insertion_fallbacks","title":"<code>test_add_substep_positional_insertion_fallbacks()</code>","text":"<p>Test fallback behavior for invalid substep insert_at values.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_substep_positional_insertion_fallbacks(self):\n    \"\"\"Test fallback behavior for invalid substep insert_at values.\"\"\"\n    self.progress_manager.add_step(\"parent\", \"Parent Step\")\n    self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n\n    # Test invalid index - should fallback to append\n    self.progress_manager.add_substep(\n        \"parent\", \"sub2\", \"Second substep\", insert_at=99\n    )\n    substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n    self.assertEqual(substep_order, [\"sub1\", \"sub2\"])\n\n    # Test non-existent substep name - should fallback to append\n    self.progress_manager.add_substep(\n        \"parent\", \"sub3\", \"Third substep\", insert_at=\"nonexistent\"\n    )\n    substep_order = list(self.progress_manager.substeps[\"parent\"].keys())\n    self.assertEqual(substep_order, [\"sub1\", \"sub2\", \"sub3\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_add_substep_validation_errors","title":"<code>test_add_substep_validation_errors()</code>","text":"<p>Test substep validation error handling.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_add_substep_validation_errors(self):\n    \"\"\"Test substep validation error handling.\"\"\"\n    # Test parent step not found\n    with self.assertRaises(ValueError) as context:\n        self.progress_manager.add_substep(\"nonexistent\", \"sub1\", \"Test substep\")\n    self.assertIn(\"not found\", str(context.exception))\n\n    # Test duplicate substep ID\n    self.progress_manager.add_step(\"parent\", \"Parent Step\")\n    self.progress_manager.add_substep(\"parent\", \"sub1\", \"First substep\")\n    with self.assertRaises(ValueError) as context:\n        self.progress_manager.add_substep(\"parent\", \"sub1\", \"Duplicate substep\")\n    self.assertIn(\"already exists\", str(context.exception))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_api_compatibility","title":"<code>test_api_compatibility()</code>","text":"<p>Test backward compatibility API methods.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_api_compatibility(self):\n    \"\"\"Test backward compatibility API methods.\"\"\"\n    # Test that all key methods exist and have correct signatures\n    self.assertTrue(hasattr(self.progress_manager, \"add_step\"))\n    self.assertTrue(hasattr(self.progress_manager, \"add_substep\"))\n    self.assertTrue(hasattr(self.progress_manager, \"start_step\"))\n    self.assertTrue(hasattr(self.progress_manager, \"update_step\"))\n    self.assertTrue(hasattr(self.progress_manager, \"complete_step\"))\n    self.assertTrue(hasattr(self.progress_manager, \"fail_step\"))\n    self.assertTrue(hasattr(self.progress_manager, \"start_substep\"))\n    self.assertTrue(hasattr(self.progress_manager, \"update_substep\"))\n    self.assertTrue(hasattr(self.progress_manager, \"complete_substep\"))\n    self.assertTrue(hasattr(self.progress_manager, \"fail_substep\"))\n    self.assertTrue(hasattr(self.progress_manager, \"update_step_with_memory\"))\n    self.assertTrue(hasattr(self.progress_manager, \"display_memory_summary\"))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_basic_initialization","title":"<code>test_basic_initialization()</code>","text":"<p>Test ProgressManager initializes correctly.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_basic_initialization(self):\n    \"\"\"Test ProgressManager initializes correctly.\"\"\"\n    self.assertEqual(self.progress_manager.title, \"Test Progress\")\n    self.assertEqual(len(self.progress_manager.steps), 0)\n    self.assertEqual(len(self.progress_manager.substeps), 0)\n    self.assertEqual(len(self.progress_manager.step_order), 0)\n    self.assertIsNone(self.progress_manager.active_step)\n    self.assertFalse(self.progress_manager._started)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_complex_positional_insertion_scenario","title":"<code>test_complex_positional_insertion_scenario()</code>","text":"<p>Test complex scenario with mixed positional insertions.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_complex_positional_insertion_scenario(self):\n    \"\"\"Test complex scenario with mixed positional insertions.\"\"\"\n    # Simulate a real-world scenario with dynamic step insertion\n    self.progress_manager.add_step(\"load_data\", \"Loading data\")\n    self.progress_manager.add_step(\"analyze\", \"Analyzing\")\n    self.progress_manager.add_step(\"export\", \"Exporting results\")\n\n    # Insert preprocessing step after data loading\n    self.progress_manager.add_step(\n        \"preprocess\", \"Preprocessing data\", insert_at=\"load_data\"\n    )\n\n    # Insert validation step at the beginning\n    self.progress_manager.add_step(\"validate\", \"Validating inputs\", insert_at=0)\n\n    # Insert cleanup step at end\n    self.progress_manager.add_step(\"cleanup\", \"Cleaning up\")\n\n    expected_order = [\n        \"validate\",\n        \"load_data\",\n        \"preprocess\",\n        \"analyze\",\n        \"export\",\n        \"cleanup\",\n    ]\n    self.assertEqual(self.progress_manager.step_order, expected_order)\n\n    # Add substeps with positional insertion\n    self.progress_manager.add_substep(\"preprocess\", \"filter\", \"Filtering data\")\n    self.progress_manager.add_substep(\"preprocess\", \"normalize\", \"Normalizing data\")\n    self.progress_manager.add_substep(\n        \"preprocess\", \"validate_schema\", \"Validating schema\", insert_at=\"filter\"\n    )\n\n    substep_order = list(self.progress_manager.substeps[\"preprocess\"].keys())\n    expected_substeps = [\"filter\", \"validate_schema\", \"normalize\"]\n    self.assertEqual(substep_order, expected_substeps)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_context_manager_protocol","title":"<code>test_context_manager_protocol()</code>","text":"<p>Test that ProgressManager supports context manager protocol.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_context_manager_protocol(self):\n    \"\"\"Test that ProgressManager supports context manager protocol.\"\"\"\n    with self.progress_manager as pm:\n        self.assertTrue(pm._started)\n        self.assertIs(pm, self.progress_manager)\n\n    # After context exit, should be finished\n    self.assertFalse(self.progress_manager._started)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_display_properties","title":"<code>test_display_properties()</code>","text":"<p>Test display-related properties.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_display_properties(self):\n    \"\"\"Test display-related properties.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"Test Step\")\n\n    with self.progress_manager:\n        self.assertIsNone(self.progress_manager.live)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_memory_manager_integration","title":"<code>test_memory_manager_integration()</code>","text":"<p>Test ProgressManager integration with memory manager.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_memory_manager_integration(self):\n    \"\"\"Test ProgressManager integration with memory manager.\"\"\"\n    # Test with mock memory manager\n    from unittest.mock import Mock\n\n    mock_memory_manager = Mock()\n\n    pm_with_memory = ProgressManager(\n        \"Test with Memory\", memory_manager=mock_memory_manager\n    )\n    self.assertEqual(pm_with_memory.memory_manager, mock_memory_manager)\n    self.assertIsNotNone(pm_with_memory.last_memory_warning)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManager.test_table_rebuild_functionality","title":"<code>test_table_rebuild_functionality()</code>","text":"<p>Test table rebuilding with positional insertion.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_table_rebuild_functionality(self):\n    \"\"\"Test table rebuilding with positional insertion.\"\"\"\n    self.progress_manager.add_step(\"step1\", \"First Step\")\n    self.progress_manager.add_step(\"step2\", \"Second Step\")\n\n    self.progress_manager._rebuild_table()\n    self.assertIsNotNone(self.progress_manager.table)\n\n    self.progress_manager.add_substep(\"step1\", \"sub1\", \"Substep 1\")\n    self.progress_manager._rebuild_table()\n    self.assertIsNotNone(self.progress_manager.table)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion","title":"<code>TestProgressManagerPositionalInsertion</code>","text":"<p>               Bases: <code>TestCase</code></p> <p>Test positional insertion edge cases and advanced scenarios.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>class TestProgressManagerPositionalInsertion(unittest.TestCase):\n    \"\"\"Test positional insertion edge cases and advanced scenarios.\"\"\"\n\n    def setUp(self):\n        self.pm = ProgressManager(\"Positional Insertion Tests\")\n\n    def test_insertion_at_boundary_conditions(self):\n        \"\"\"Test insertion at boundary conditions (0, exact length).\"\"\"\n        self.pm.add_step(\"middle\", \"Middle Step\")\n\n        # Insert at beginning (index 0)\n        self.pm.add_step(\"first\", \"First Step\", insert_at=0)\n        self.assertEqual(self.pm.step_order, [\"first\", \"middle\"])\n\n        # Insert at exact length (should append)\n        self.pm.add_step(\"last\", \"Last Step\", insert_at=2)\n        self.assertEqual(self.pm.step_order, [\"first\", \"middle\", \"last\"])\n\n    def test_insertion_preserves_existing_data(self):\n        \"\"\"Test that insertions don't corrupt existing step data.\"\"\"\n        self.pm.add_step(\"step1\", \"Step 1\", total=100)\n        self.pm.add_step(\"step3\", \"Step 3\", total=300)\n\n        # Insert between existing steps\n        self.pm.add_step(\"step2\", \"Step 2\", total=200, insert_at=1)\n\n        # Verify all step data is preserved\n        self.assertEqual(self.pm.steps[\"step1\"][\"total\"], 100)\n        self.assertEqual(self.pm.steps[\"step2\"][\"total\"], 200)\n        self.assertEqual(self.pm.steps[\"step3\"][\"total\"], 300)\n        self.assertEqual(self.pm.step_order, [\"step1\", \"step2\", \"step3\"])\n\n    def test_substep_insertion_with_multiple_parents(self):\n        \"\"\"Test substep insertion works correctly with multiple parent steps.\"\"\"\n        self.pm.add_step(\"parent1\", \"Parent 1\")\n        self.pm.add_step(\"parent2\", \"Parent 2\")\n\n        # Add substeps to both parents\n        self.pm.add_substep(\"parent1\", \"p1_sub1\", \"P1 Sub 1\")\n        self.pm.add_substep(\"parent1\", \"p1_sub3\", \"P1 Sub 3\")\n        self.pm.add_substep(\"parent2\", \"p2_sub1\", \"P2 Sub 1\")\n\n        # Insert substep in parent1\n        self.pm.add_substep(\"parent1\", \"p1_sub2\", \"P1 Sub 2\", insert_at=1)\n\n        # Verify parent1 substeps are correctly ordered\n        p1_order = list(self.pm.substeps[\"parent1\"].keys())\n        self.assertEqual(p1_order, [\"p1_sub1\", \"p1_sub2\", \"p1_sub3\"])\n\n        # Verify parent2 is unaffected\n        p2_order = list(self.pm.substeps[\"parent2\"].keys())\n        self.assertEqual(p2_order, [\"p2_sub1\"])\n\n    def test_performance_with_many_insertions(self):\n        \"\"\"Test that positional insertion performs reasonably with many steps.\"\"\"\n        import time\n\n        start_time = time.time()\n\n        # Add many steps with various insertion patterns\n        for i in range(100):\n            if i % 3 == 0:\n                self.pm.add_step(\n                    f\"step_{i}\", f\"Step {i}\", insert_at=0\n                )  # Insert at beginning\n            elif i % 3 == 1:\n                self.pm.add_step(f\"step_{i}\", f\"Step {i}\")  # Append at end\n            else:\n                self.pm.add_step(\n                    f\"step_{i}\", f\"Step {i}\", insert_at=len(self.pm.step_order) // 2\n                )  # Insert at middle\n\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Should complete in reasonable time (less than 1 second for 100 insertions)\n        self.assertLess(execution_time, 1.0)\n        self.assertEqual(len(self.pm.step_order), 100)\n\n    def test_insertion_with_unicode_and_special_characters(self):\n        \"\"\"Test insertion works with unicode and special characters in IDs and titles.\"\"\"\n        self.pm.add_step(\"step_1\", \"Step 1\")\n        self.pm.add_step(\"\u00e9tape_2\", \"\u00c9tape avec accents\", insert_at=\"step_1\")\n        self.pm.add_step(\"\u0448\u0430\u0433_3\", \"\u0428\u0430\u0433 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435\")\n        self.pm.add_step(\"\u6b65\u9aa4_4\", \"\u4e2d\u6587\u6b65\u9aa4\", insert_at=1)\n\n        # Verify order and that unicode is handled correctly\n        expected_order = [\"step_1\", \"\u6b65\u9aa4_4\", \"\u00e9tape_2\", \"\u0448\u0430\u0433_3\"]\n        self.assertEqual(self.pm.step_order, expected_order)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion.test_insertion_at_boundary_conditions","title":"<code>test_insertion_at_boundary_conditions()</code>","text":"<p>Test insertion at boundary conditions (0, exact length).</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_insertion_at_boundary_conditions(self):\n    \"\"\"Test insertion at boundary conditions (0, exact length).\"\"\"\n    self.pm.add_step(\"middle\", \"Middle Step\")\n\n    # Insert at beginning (index 0)\n    self.pm.add_step(\"first\", \"First Step\", insert_at=0)\n    self.assertEqual(self.pm.step_order, [\"first\", \"middle\"])\n\n    # Insert at exact length (should append)\n    self.pm.add_step(\"last\", \"Last Step\", insert_at=2)\n    self.assertEqual(self.pm.step_order, [\"first\", \"middle\", \"last\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion.test_insertion_preserves_existing_data","title":"<code>test_insertion_preserves_existing_data()</code>","text":"<p>Test that insertions don't corrupt existing step data.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_insertion_preserves_existing_data(self):\n    \"\"\"Test that insertions don't corrupt existing step data.\"\"\"\n    self.pm.add_step(\"step1\", \"Step 1\", total=100)\n    self.pm.add_step(\"step3\", \"Step 3\", total=300)\n\n    # Insert between existing steps\n    self.pm.add_step(\"step2\", \"Step 2\", total=200, insert_at=1)\n\n    # Verify all step data is preserved\n    self.assertEqual(self.pm.steps[\"step1\"][\"total\"], 100)\n    self.assertEqual(self.pm.steps[\"step2\"][\"total\"], 200)\n    self.assertEqual(self.pm.steps[\"step3\"][\"total\"], 300)\n    self.assertEqual(self.pm.step_order, [\"step1\", \"step2\", \"step3\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion.test_insertion_with_unicode_and_special_characters","title":"<code>test_insertion_with_unicode_and_special_characters()</code>","text":"<p>Test insertion works with unicode and special characters in IDs and titles.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_insertion_with_unicode_and_special_characters(self):\n    \"\"\"Test insertion works with unicode and special characters in IDs and titles.\"\"\"\n    self.pm.add_step(\"step_1\", \"Step 1\")\n    self.pm.add_step(\"\u00e9tape_2\", \"\u00c9tape avec accents\", insert_at=\"step_1\")\n    self.pm.add_step(\"\u0448\u0430\u0433_3\", \"\u0428\u0430\u0433 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435\")\n    self.pm.add_step(\"\u6b65\u9aa4_4\", \"\u4e2d\u6587\u6b65\u9aa4\", insert_at=1)\n\n    # Verify order and that unicode is handled correctly\n    expected_order = [\"step_1\", \"\u6b65\u9aa4_4\", \"\u00e9tape_2\", \"\u0448\u0430\u0433_3\"]\n    self.assertEqual(self.pm.step_order, expected_order)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion.test_performance_with_many_insertions","title":"<code>test_performance_with_many_insertions()</code>","text":"<p>Test that positional insertion performs reasonably with many steps.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_performance_with_many_insertions(self):\n    \"\"\"Test that positional insertion performs reasonably with many steps.\"\"\"\n    import time\n\n    start_time = time.time()\n\n    # Add many steps with various insertion patterns\n    for i in range(100):\n        if i % 3 == 0:\n            self.pm.add_step(\n                f\"step_{i}\", f\"Step {i}\", insert_at=0\n            )  # Insert at beginning\n        elif i % 3 == 1:\n            self.pm.add_step(f\"step_{i}\", f\"Step {i}\")  # Append at end\n        else:\n            self.pm.add_step(\n                f\"step_{i}\", f\"Step {i}\", insert_at=len(self.pm.step_order) // 2\n            )  # Insert at middle\n\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    # Should complete in reasonable time (less than 1 second for 100 insertions)\n    self.assertLess(execution_time, 1.0)\n    self.assertEqual(len(self.pm.step_order), 100)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressManagerPositionalInsertion.test_substep_insertion_with_multiple_parents","title":"<code>test_substep_insertion_with_multiple_parents()</code>","text":"<p>Test substep insertion works correctly with multiple parent steps.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_substep_insertion_with_multiple_parents(self):\n    \"\"\"Test substep insertion works correctly with multiple parent steps.\"\"\"\n    self.pm.add_step(\"parent1\", \"Parent 1\")\n    self.pm.add_step(\"parent2\", \"Parent 2\")\n\n    # Add substeps to both parents\n    self.pm.add_substep(\"parent1\", \"p1_sub1\", \"P1 Sub 1\")\n    self.pm.add_substep(\"parent1\", \"p1_sub3\", \"P1 Sub 3\")\n    self.pm.add_substep(\"parent2\", \"p2_sub1\", \"P2 Sub 1\")\n\n    # Insert substep in parent1\n    self.pm.add_substep(\"parent1\", \"p1_sub2\", \"P1 Sub 2\", insert_at=1)\n\n    # Verify parent1 substeps are correctly ordered\n    p1_order = list(self.pm.substeps[\"parent1\"].keys())\n    self.assertEqual(p1_order, [\"p1_sub1\", \"p1_sub2\", \"p1_sub3\"])\n\n    # Verify parent2 is unaffected\n    p2_order = list(self.pm.substeps[\"parent2\"].keys())\n    self.assertEqual(p2_order, [\"p2_sub1\"])\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressReporter","title":"<code>TestProgressReporter</code>","text":"<p>Test the basic ProgressReporter class.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>class TestProgressReporter:\n    \"\"\"Test the basic ProgressReporter class.\"\"\"\n\n    def test_init(self):\n        \"\"\"Test ProgressReporter initialization.\"\"\"\n        reporter = ProgressReporter(\"Test Task\")\n        assert reporter.title == \"Test Task\"\n        assert reporter._start_time is None\n        assert reporter._last_update is None\n\n    def test_context_manager(self):\n        \"\"\"Test ProgressReporter as context manager.\"\"\"\n        with ProgressReporter(\"Test\") as reporter:\n            assert reporter._start_time is not None\n            assert isinstance(reporter._start_time, float)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressReporter.test_context_manager","title":"<code>test_context_manager()</code>","text":"<p>Test ProgressReporter as context manager.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_context_manager(self):\n    \"\"\"Test ProgressReporter as context manager.\"\"\"\n    with ProgressReporter(\"Test\") as reporter:\n        assert reporter._start_time is not None\n        assert isinstance(reporter._start_time, float)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.test_progress.TestProgressReporter.test_init","title":"<code>test_init()</code>","text":"<p>Test ProgressReporter initialization.</p> Source code in <code>terminal_tools/test_progress.py</code> <pre><code>def test_init(self):\n    \"\"\"Test ProgressReporter initialization.\"\"\"\n    reporter = ProgressReporter(\"Test Task\")\n    assert reporter.title == \"Test Task\"\n    assert reporter._start_time is None\n    assert reporter._last_update is None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils","title":"<code>utils</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_printed_lines","title":"<code>clear_printed_lines(count)</code>","text":"<p>Clear the last <code>count</code> lines of the terminal. Useful for repainting terminal output.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The number of lines to clear</p> required Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_printed_lines(count: int):\n    \"\"\"\n    Clear the last `count` lines of the terminal. Useful for repainting\n    terminal output.\n\n    Args:\n        count (int): The number of lines to clear\n    \"\"\"\n    for _ in range(count + 1):\n        sys.stdout.write(\"\\033[2K\")  # Clear the current line\n        sys.stdout.write(\"\\033[F\")  # Move cursor up one line\n    sys.stdout.write(\"\\033[2K\\r\")  # Clear the last line and move to start\n    sys.stdout.flush()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_printed_lines(count)","title":"<code>count</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_terminal","title":"<code>clear_terminal()</code>","text":"<p>Clears the terminal</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_terminal():\n    \"\"\"Clears the terminal\"\"\"\n    if os.name == \"nt\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box","title":"<code>draw_box(text, *, padding_spaces=5, padding_lines=1)</code>","text":"<p>Draw a box around the given text, which will be centered in the box.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to be drawn, may be multiline. ANSI formatting and emojis are not supported, as they mess with both the character count calculation and the monospace font.</p> required <code>int</code> <p>Extra spaces on either side of the longest line. Defaults to 5.</p> <code>5</code> <code>int</code> <p>Extra lines above and below the text. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text surrounded by a box.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def draw_box(text: str, *, padding_spaces: int = 5, padding_lines: int = 1) -&gt; str:\n    \"\"\"\n    Draw a box around the given text, which will be centered in the box.\n\n    Args:\n        text (str): The text to be drawn, may be multiline.\n          ANSI formatting and emojis are not supported, as they mess with\n          both the character count calculation and the monospace font.\n\n        padding_spaces (int, optional): Extra spaces on either side of the longest line. Defaults to 5.\n        padding_lines (int, optional): Extra lines above and below the text. Defaults to 1.\n\n    Returns:\n        str: The text surrounded by a box.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    width = max(len(line) for line in lines) + padding_spaces * 2\n\n    box = \"\"\n    box += \"\u250c\" + \"\u2500\" * width + \"\u2510\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    for line in lines:\n        padding = \" \" * padding_spaces\n        box += \"\u2502\" + padding + line.center(width - 2 * padding_spaces) + padding + \"\u2502\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    box += \"\u2514\" + \"\u2500\" * width + \"\u2518\\n\"\n    return box\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(text)","title":"<code>text</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(padding_spaces)","title":"<code>padding_spaces</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(padding_lines)","title":"<code>padding_lines</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.enable_windows_ansi_support","title":"<code>enable_windows_ansi_support()</code>","text":"<p>Set up the Windows terminal to support ANSI escape codes, which will be needed for colored text, line clearing, and other terminal features.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def enable_windows_ansi_support():\n    \"\"\"\n    Set up the Windows terminal to support ANSI escape codes, which will be needed\n    for colored text, line clearing, and other terminal features.\n    \"\"\"\n    if os.name == \"nt\":\n        # Enable ANSI escape code support for Windows\n        # On Windows, calling os.system('') with an empty string doesn't\n        # run any actual command. However, there's an undocumented side\n        # effect: it forces the Windows terminal to initialize or refresh\n        # its state, enabling certain features like the processing of ANSI\n        # escape codes, which might not otherwise be active.\n        os.system(\"\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.is_wsl","title":"<code>is_wsl()</code>","text":"<p>Check if the environment is WSL2.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def is_wsl() -&gt; bool:\n    \"\"\"Check if the environment is WSL2.\"\"\"\n    try:\n        with open(\"/proc/version\", \"r\") as f:\n            return \"microsoft\" in f.read().lower()\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.wait_for_key","title":"<code>wait_for_key(prompt=False)</code>","text":"<p>Waits for the user to press any key</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>If true, a default text</p> <code>False</code> Source code in <code>terminal_tools/utils.py</code> <pre><code>def wait_for_key(prompt: bool = False):\n    \"\"\"Waits for the user to press any key\n\n    Args:\n        prompt (bool, optional): If true, a default text\n        `Press any key to continue` will be shown. Defaults to False.\n    \"\"\"\n    if prompt:\n        print(\"Press any key to continue...\", end=\"\", flush=True)\n    _wait_for_key()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.wait_for_key(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/testing/","title":"Testing","text":""},{"location":"reference/testing/#testing","title":"<code>testing</code>","text":""},{"location":"reference/testing/#testing.context","title":"<code>context</code>","text":""},{"location":"reference/testing/#testing.context.TestInputColumnProvider","title":"<code>TestInputColumnProvider</code>","text":"<p>Simple test version of InputColumnProvider.</p> Source code in <code>testing/context.py</code> <pre><code>class TestInputColumnProvider:\n    \"\"\"Simple test version of InputColumnProvider.\"\"\"\n\n    def __init__(self, user_column_name: str, semantic: SeriesSemantic):\n        self.user_column_name = user_column_name\n        self.semantic = semantic\n</code></pre>"},{"location":"reference/testing/#testing.performance","title":"<code>performance</code>","text":"<p>Performance Testing Suite for Chunking Optimization</p> <p>This package contains comprehensive tests for validating the performance improvements introduced in the N-gram analyzer chunking optimization (Phases 1-4).</p> <p>Test Modules: - test_chunking_optimization.py: Core functionality and system configuration tests - test_performance_benchmarks.py: Real performance measurements and stress tests</p> Usage <p>pytest tests/performance/ -v                    # Run all performance tests pytest tests/performance/ -v -k \"not benchmark\" # Skip expensive benchmark tests pytest tests/performance/ -v --tb=short         # Concise output pytest tests/performance/ -v -s                 # Show print output from benchmarks</p>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks","title":"<code>run_enhanced_benchmarks</code>","text":"<p>Enhanced Performance Test Runner Demonstrates the new robust testing approach using pytest-benchmark and resource-based metrics.</p>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.demonstrate_flaky_test_detection","title":"<code>demonstrate_flaky_test_detection()</code>","text":"<p>Demonstrate detection of flaky tests by running multiple times.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def demonstrate_flaky_test_detection():\n    \"\"\"Demonstrate detection of flaky tests by running multiple times.\"\"\"\n    print(\"\ud83d\udd04 Demonstrating test reliability by running tests multiple times...\")\n\n    # Run deterministic tests multiple times - should always pass\n    success_count = 0\n    total_runs = 5\n\n    for i in range(total_runs):\n        print(f\"  Run {i+1}/{total_runs}...\")\n        cmd = [\n            \"pytest\",\n            \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_efficiency_invariant\",\n            \"-q\",\n            \"-m\",\n            \"\",  # Override default marker filtering\n        ]\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0:\n            success_count += 1\n\n    success_rate = success_count / total_runs * 100\n    print(\n        f\"\ud83d\udcca Deterministic test success rate: {success_rate:.1f}% ({success_count}/{total_runs})\"\n    )\n\n    if success_rate &gt;= 95:\n        print(\"\u2705 Tests are reliable (&gt;95% success rate)\")\n        return True\n    else:\n        print(\"\u274c Tests are flaky (&lt;95% success rate)\")\n        return False\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.main","title":"<code>main()</code>","text":"<p>Main test runner function.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def main():\n    \"\"\"Main test runner function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Enhanced Performance Test Runner\")\n    parser.add_argument(\n        \"--basic\", action=\"store_true\", help=\"Run basic performance tests\"\n    )\n    parser.add_argument(\n        \"--benchmarks\", action=\"store_true\", help=\"Run pytest-benchmark tests\"\n    )\n    parser.add_argument(\n        \"--deterministic\", action=\"store_true\", help=\"Run deterministic tests\"\n    )\n    parser.add_argument(\n        \"--property\", action=\"store_true\", help=\"Run property-based tests\"\n    )\n    parser.add_argument(\"--variance\", action=\"store_true\", help=\"Run variance analysis\")\n    parser.add_argument(\n        \"--comparison\", action=\"store_true\", help=\"Run benchmark comparison\"\n    )\n    parser.add_argument(\n        \"--reliability\", action=\"store_true\", help=\"Test reliability demonstration\"\n    )\n    parser.add_argument(\"--all\", action=\"store_true\", help=\"Run all test categories\")\n\n    args = parser.parse_args()\n\n    if not any(\n        [\n            args.basic,\n            args.benchmarks,\n            args.deterministic,\n            args.property,\n            args.variance,\n            args.comparison,\n            args.reliability,\n            args.all,\n        ]\n    ):\n        args.all = True  # Default to running all tests\n\n    print(\"\ud83d\ude80 Enhanced Performance Testing Suite\")\n    print(\"=\" * 50)\n\n    results = []\n\n    if args.all or args.basic:\n        results.append((\"Basic Performance Tests\", run_basic_performance_tests()))\n\n    if args.all or args.deterministic:\n        results.append((\"Deterministic Tests\", run_deterministic_tests()))\n\n    if args.all or args.property:\n        results.append((\"Property-Based Tests\", run_property_based_tests()))\n\n    if args.all or args.variance:\n        results.append((\"Variance Analysis\", run_variance_analysis()))\n\n    if args.all or args.benchmarks:\n        results.append((\"Enhanced Benchmarks\", run_enhanced_benchmarks()))\n\n    if args.all or args.comparison:\n        results.append((\"Benchmark Comparison\", run_benchmark_comparison()))\n\n    if args.all or args.reliability:\n        results.append(\n            (\"Reliability Demonstration\", demonstrate_flaky_test_detection())\n        )\n\n    # Summary\n    print(\"\\n\" + \"=\" * 50)\n    print(\"\ud83d\udccb TEST SUMMARY\")\n    print(\"=\" * 50)\n\n    total_tests = len(results)\n    passed_tests = sum(1 for _, passed in results if passed)\n\n    for test_name, passed in results:\n        status = \"\u2705 PASSED\" if passed else \"\u274c FAILED\"\n        print(f\"{test_name}: {status}\")\n\n    print(f\"\\nOverall: {passed_tests}/{total_tests} test categories passed\")\n\n    if passed_tests == total_tests:\n        print(\"\ud83c\udf89 All enhanced performance tests are working correctly!\")\n        print(\"\\n\ud83d\udca1 Key Benefits Demonstrated:\")\n        print(\"  \u2022 Eliminated flaky time-based failures\")\n        print(\"  \u2022 Statistical rigor with pytest-benchmark\")\n        print(\"  \u2022 Deterministic resource-based metrics\")\n        print(\"  \u2022 Property-based testing for edge cases\")\n        print(\"  \u2022 Variance analysis for reliability validation\")\n        return 0\n    else:\n        print(\"\u26a0\ufe0f  Some test categories failed - see details above\")\n        return 1\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_basic_performance_tests","title":"<code>run_basic_performance_tests()</code>","text":"<p>Run basic performance tests with adjusted thresholds.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_basic_performance_tests():\n    \"\"\"Run basic performance tests with adjusted thresholds.\"\"\"\n    print(\"\ud83d\udd0d Running basic performance tests with realistic thresholds...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_performance_benchmarks.py\",\n        \"-v\",\n        \"-m\",\n        \"performance\",\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Basic performance tests passed!\")\n    else:\n        print(\"\u274c Basic performance tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_benchmark_comparison","title":"<code>run_benchmark_comparison()</code>","text":"<p>Run benchmark comparison with results saving.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_benchmark_comparison():\n    \"\"\"Run benchmark comparison with results saving.\"\"\"\n    print(\"\ud83c\udfc6 Running benchmark comparison tests...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_processing_benchmark_small\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_processing_benchmark_medium\",\n        \"-v\",\n        \"-m\",\n        \"\",  # Override default marker filtering\n        \"--benchmark-enable\",\n        \"--benchmark-autosave\",\n        \"--benchmark-verbose\",\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Benchmark comparison tests passed!\")\n        print(\"\ud83d\udcbe Benchmark results saved for future comparison\")\n    else:\n        print(\"\u274c Benchmark comparison tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_deterministic_tests","title":"<code>run_deterministic_tests()</code>","text":"<p>Run deterministic resource-based tests.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_deterministic_tests():\n    \"\"\"Run deterministic resource-based tests.\"\"\"\n    print(\"\u26a1 Running deterministic I/O and memory tests...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_efficiency_invariant\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_memory_efficiency_bounds\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_io_operation_counting_deterministic\",\n        \"-v\",\n        \"-m\",\n        \"\",  # Override default marker filtering\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Deterministic tests passed!\")\n    else:\n        print(\"\u274c Deterministic tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_enhanced_benchmarks","title":"<code>run_enhanced_benchmarks()</code>","text":"<p>Run enhanced pytest-benchmark tests.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_enhanced_benchmarks():\n    \"\"\"Run enhanced pytest-benchmark tests.\"\"\"\n    print(\"\ud83d\udcca Running enhanced pytest-benchmark tests...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_enhanced_benchmarks.py\",\n        \"-v\",\n        \"-m\",\n        \"benchmark\",\n        \"--benchmark-enable\",\n        \"--benchmark-verbose\",\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Enhanced benchmark tests passed!\")\n    else:\n        print(\"\u274c Enhanced benchmark tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_property_based_tests","title":"<code>run_property_based_tests()</code>","text":"<p>Run property-based scaling tests.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_property_based_tests():\n    \"\"\"Run property-based scaling tests.\"\"\"\n    print(\"\ud83e\uddea Running property-based chunk scaling tests...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_size_scaling_properties\",\n        \"-v\",\n        \"-m\",\n        \"\",  # Override default marker filtering\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Property-based tests passed!\")\n    else:\n        print(\"\u274c Property-based tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_enhanced_benchmarks.run_variance_analysis","title":"<code>run_variance_analysis()</code>","text":"<p>Run variance analysis tests.</p> Source code in <code>testing/performance/run_enhanced_benchmarks.py</code> <pre><code>def run_variance_analysis():\n    \"\"\"Run variance analysis tests.\"\"\"\n    print(\"\ud83d\udcc8 Running variance analysis tests...\")\n    cmd = [\n        \"pytest\",\n        \"testing/performance/test_enhanced_benchmarks.py::TestEnhancedPerformanceBenchmarks::test_chunk_processing_variance_analysis\",\n        \"-v\",\n        \"-m\",\n        \"\",  # Override default marker filtering\n        \"--tb=short\",\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        print(\"\u2705 Variance analysis tests passed!\")\n    else:\n        print(\"\u274c Variance analysis tests failed:\")\n        print(result.stdout)\n        print(result.stderr)\n\n    return result.returncode == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests","title":"<code>run_performance_tests</code>","text":"<p>Performance Test Runner for Chunking Optimization</p> <p>Provides convenient ways to run performance tests with appropriate configurations and generate performance reports.</p>"},{"location":"reference/testing/#testing.performance.run_performance_tests.generate_performance_report","title":"<code>generate_performance_report()</code>","text":"<p>Generate a performance test report.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def generate_performance_report():\n    \"\"\"Generate a performance test report.\"\"\"\n    print(\"=== Generating Performance Report ===\")\n\n    system_info = get_system_info()\n\n    # Run tests with detailed output\n    report_file = Path(\"performance_test_report.txt\")\n\n    args = [\n        \"testing/performance/\",\n        \"-v\",\n        \"--tb=short\",\n        \"-s\",\n        \"--durations=20\",\n        f\"--html=performance_report.html\",  # If pytest-html is available\n        \"--self-contained-html\",\n    ]\n\n    print(f\"Running tests and generating report...\")\n    start_time = time.time()\n    result = pytest.main(args)\n    end_time = time.time()\n\n    # Create text report\n    with open(report_file, \"w\") as f:\n        f.write(\"Performance Test Report\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Test Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Total Test Time: {end_time - start_time:.1f} seconds\\n\\n\")\n\n        f.write(\"System Information:\\n\")\n        f.write(\"-\" * 20 + \"\\n\")\n        for key, value in system_info.items():\n            f.write(f\"{key}: {value}\\n\")\n        f.write(\"\\n\")\n\n        f.write(\"Test Results:\\n\")\n        f.write(\"-\" * 20 + \"\\n\")\n        f.write(f\"Overall Result: {'PASSED' if result == 0 else 'FAILED'}\\n\")\n        f.write(\"\\nSee performance_report.html for detailed results (if available).\\n\")\n\n    print(f\"Report saved to: {report_file}\")\n    if Path(\"performance_report.html\").exists():\n        print(f\"HTML report saved to: performance_report.html\")\n\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Get system information for test context.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def get_system_info() -&gt; Dict[str, any]:\n    \"\"\"Get system information for test context.\"\"\"\n    memory = psutil.virtual_memory()\n    return {\n        \"total_memory_gb\": memory.total / 1024**3,\n        \"available_memory_gb\": memory.available / 1024**3,\n        \"cpu_count\": psutil.cpu_count(),\n        \"cpu_freq\": psutil.cpu_freq().max if psutil.cpu_freq() else None,\n        \"python_version\": sys.version.split()[0],\n    }\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.main","title":"<code>main()</code>","text":"<p>Main CLI interface.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def main():\n    \"\"\"Main CLI interface.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run performance tests for chunking optimization\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python run_performance_tests.py --validate     # Quick validation tests\n  python run_performance_tests.py --benchmark    # Performance benchmarks  \n  python run_performance_tests.py --full         # Complete test suite\n  python run_performance_tests.py --phase 2      # Phase 2 tests only\n  python run_performance_tests.py --memory       # Tests for current system\n  python run_performance_tests.py --report       # Generate performance report\n        \"\"\",\n    )\n\n    parser.add_argument(\n        \"--validate\", action=\"store_true\", help=\"Run core validation tests (fast)\"\n    )\n    parser.add_argument(\n        \"--benchmark\", action=\"store_true\", help=\"Run performance benchmark tests\"\n    )\n    parser.add_argument(\n        \"--full\", action=\"store_true\", help=\"Run complete performance test suite\"\n    )\n    parser.add_argument(\n        \"--phase\",\n        choices=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n        help=\"Run tests for specific optimization phase\",\n    )\n    parser.add_argument(\n        \"--memory\",\n        action=\"store_true\",\n        help=\"Run tests appropriate for current system memory\",\n    )\n    parser.add_argument(\n        \"--report\",\n        action=\"store_true\",\n        help=\"Generate comprehensive performance report\",\n    )\n    parser.add_argument(\n        \"--info\", action=\"store_true\", help=\"Show system information and exit\"\n    )\n\n    args = parser.parse_args()\n\n    # Show system info if requested or for any test run\n    if args.info:\n        print_system_info()\n        return 0\n\n    if not any(\n        [args.validate, args.benchmark, args.full, args.phase, args.memory, args.report]\n    ):\n        parser.print_help()\n        return 1\n\n    print_system_info()\n    success = True\n\n    try:\n        if args.validate:\n            success &amp;= run_validation_tests()\n\n        if args.benchmark:\n            success &amp;= run_performance_benchmarks()\n\n        if args.full:\n            success &amp;= run_full_suite()\n\n        if args.phase:\n            success &amp;= run_specific_phase(args.phase)\n\n        if args.memory:\n            success &amp;= run_memory_specific_tests()\n\n        if args.report:\n            success &amp;= generate_performance_report()\n\n    except KeyboardInterrupt:\n        print(\"\\nTests interrupted by user.\")\n        return 130\n    except Exception as e:\n        print(f\"\\nError running tests: {e}\")\n        return 1\n\n    print(\"\\n\" + \"=\" * 50)\n    if success:\n        print(\"\u2705 All tests completed successfully!\")\n        return 0\n    else:\n        print(\"\u274c Some tests failed. Check output above for details.\")\n        return 1\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.print_system_info","title":"<code>print_system_info()</code>","text":"<p>Print system information.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def print_system_info():\n    \"\"\"Print system information.\"\"\"\n    info = get_system_info()\n    print(\"=== System Information ===\")\n    print(f\"Total RAM: {info['total_memory_gb']:.1f} GB\")\n    print(f\"Available RAM: {info['available_memory_gb']:.1f} GB\")\n    print(f\"CPU Cores: {info['cpu_count']}\")\n    if info[\"cpu_freq\"]:\n        print(f\"CPU Frequency: {info['cpu_freq']:.0f} MHz\")\n    print(f\"Python Version: {info['python_version']}\")\n    print()\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.run_full_suite","title":"<code>run_full_suite()</code>","text":"<p>Run the complete performance test suite.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def run_full_suite() -&gt; bool:\n    \"\"\"Run the complete performance test suite.\"\"\"\n    print(\"=== Running Full Performance Test Suite ===\")\n\n    args = [\"testing/performance/\", \"-v\", \"--tb=short\", \"-s\", \"--durations=15\"]\n\n    result = pytest.main(args)\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.run_memory_specific_tests","title":"<code>run_memory_specific_tests()</code>","text":"<p>Run tests specific to current system's memory configuration.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def run_memory_specific_tests() -&gt; bool:\n    \"\"\"Run tests specific to current system's memory configuration.\"\"\"\n    system_info = get_system_info()\n    memory_gb = system_info[\"total_memory_gb\"]\n\n    print(f\"=== Running Tests for {memory_gb:.1f}GB System ===\")\n\n    if memory_gb &gt;= 32:\n        print(\"High-memory system detected. Running comprehensive tests.\")\n        test_filter = \"not stress\"  # Skip stress tests unless explicitly requested\n    elif memory_gb &gt;= 16:\n        print(\"Standard-memory system detected. Running standard tests.\")\n        test_filter = \"not stress and not large_dataset\"\n    elif memory_gb &gt;= 8:\n        print(\"Lower-memory system detected. Running basic tests.\")\n        test_filter = \"not stress and not large_dataset and not comprehensive\"\n    else:\n        print(\"Constrained-memory system detected. Running minimal tests.\")\n        test_filter = (\n            \"not stress and not large_dataset and not comprehensive and not benchmark\"\n        )\n\n    args = [\"testing/performance/\", \"-v\", \"--tb=short\", \"-k\", test_filter]\n\n    result = pytest.main(args)\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.run_performance_benchmarks","title":"<code>run_performance_benchmarks()</code>","text":"<p>Run performance benchmark tests.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def run_performance_benchmarks() -&gt; bool:\n    \"\"\"Run performance benchmark tests.\"\"\"\n    print(\"=== Running Performance Benchmarks ===\")\n\n    system_info = get_system_info()\n    if system_info[\"total_memory_gb\"] &lt; 8:\n        print(\"WARNING: System has less than 8GB RAM. Some benchmarks may be skipped.\")\n        print()\n\n    args = [\n        \"testing/performance/test_performance_benchmarks.py\",\n        \"-v\",\n        \"--tb=short\",\n        \"-s\",  # Show output from benchmarks\n        \"--durations=10\",  # Show slowest 10 tests\n    ]\n\n    result = pytest.main(args)\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.run_specific_phase","title":"<code>run_specific_phase(phase)</code>","text":"<p>Run tests for a specific optimization phase.</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def run_specific_phase(phase: str) -&gt; bool:\n    \"\"\"Run tests for a specific optimization phase.\"\"\"\n    phase_keywords = {\n        \"1\": \"memory_detection or auto_detection\",\n        \"2\": \"adaptive_chunk\",\n        \"3\": \"fallback\",\n        \"4\": \"secondary_analyzer or ngram_stats\",\n        \"5\": \"validation or benchmark\",\n    }\n\n    if phase not in phase_keywords:\n        print(f\"Invalid phase: {phase}. Must be 1-5.\")\n        return False\n\n    print(f\"=== Running Phase {phase} Tests ===\")\n\n    args = [\"testing/performance/\", \"-v\", \"--tb=short\", \"-k\", phase_keywords[phase]]\n\n    result = pytest.main(args)\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.run_performance_tests.run_validation_tests","title":"<code>run_validation_tests()</code>","text":"<p>Run core validation tests (fast).</p> Source code in <code>testing/performance/run_performance_tests.py</code> <pre><code>def run_validation_tests() -&gt; bool:\n    \"\"\"Run core validation tests (fast).\"\"\"\n    print(\"=== Running Core Validation Tests ===\")\n\n    args = [\n        \"testing/performance/test_chunking_optimization.py\",\n        \"-v\",\n        \"--tb=short\",\n        \"-k\",\n        \"not benchmark and not stress and not comprehensive\",\n    ]\n\n    result = pytest.main(args)\n    return result == 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization","title":"<code>test_chunking_optimization</code>","text":"<p>Performance Test Suite for Chunking Optimization Phase 5: Testing &amp; Validation for N-gram Analyzer Chunking Optimization</p> <p>This test suite validates the performance improvements and system-specific scaling introduced in Phases 1-4 of the chunking optimization specification.</p>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestAdaptiveChunkSizing","title":"<code>TestAdaptiveChunkSizing</code>","text":"<p>Test adaptive chunk size calculation for different system configurations.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestAdaptiveChunkSizing:\n    \"\"\"Test adaptive chunk size calculation for different system configurations.\"\"\"\n\n    def test_calculate_optimal_chunk_size_memory_factors(self):\n        \"\"\"Test memory factor calculation for different system sizes.\"\"\"\n        from analyzers.ngrams.ngrams_base.main import main\n\n        # Mock memory manager for different system sizes\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test 8GB system (factor = 1.0)\n            mock_vm.return_value.total = 8 * 1024**3\n            memory_manager = MemoryManager()\n\n            # Use the calculate_optimal_chunk_size function from the ngrams_base module\n            # We need to access it through the main function's internal definition\n            # For testing, we'll create a similar function\n            def test_calculate_optimal_chunk_size(\n                dataset_size: int, memory_manager=None\n            ) -&gt; int:\n                if memory_manager:\n                    total_gb = psutil.virtual_memory().total / 1024**3\n                    if total_gb &gt;= 32:\n                        memory_factor = 2.0\n                    elif total_gb &gt;= 16:\n                        memory_factor = 1.5\n                    elif total_gb &gt;= 8:\n                        memory_factor = 1.0\n                    else:\n                        memory_factor = 0.5\n                else:\n                    memory_factor = 1.0\n\n                if dataset_size &lt;= 500_000:\n                    base_chunk = int(200_000 * memory_factor)\n                elif dataset_size &lt;= 2_000_000:\n                    base_chunk = int(150_000 * memory_factor)\n                elif dataset_size &lt;= 5_000_000:\n                    base_chunk = int(100_000 * memory_factor)\n                else:\n                    base_chunk = int(75_000 * memory_factor)\n\n                return max(10_000, min(base_chunk, 500_000))\n\n            # Test different system sizes\n            # 8GB system\n            mock_vm.return_value.total = 8 * 1024**3\n            chunk_size_8gb = test_calculate_optimal_chunk_size(\n                1_000_000, memory_manager\n            )\n            assert chunk_size_8gb == 150_000  # 150K * 1.0\n\n            # 16GB system\n            mock_vm.return_value.total = 16 * 1024**3\n            chunk_size_16gb = test_calculate_optimal_chunk_size(\n                1_000_000, memory_manager\n            )\n            assert chunk_size_16gb == 225_000  # 150K * 1.5\n\n            # 32GB system\n            mock_vm.return_value.total = 32 * 1024**3\n            chunk_size_32gb = test_calculate_optimal_chunk_size(\n                1_000_000, memory_manager\n            )\n            assert chunk_size_32gb == 300_000  # 150K * 2.0\n\n            # 4GB system\n            mock_vm.return_value.total = 4 * 1024**3\n            chunk_size_4gb = test_calculate_optimal_chunk_size(\n                1_000_000, memory_manager\n            )\n            assert chunk_size_4gb == 75_000  # 150K * 0.5\n\n    def test_adaptive_chunk_scaling_by_dataset_size(self):\n        \"\"\"Test that chunk sizes scale appropriately with dataset size.\"\"\"\n\n        def test_calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            memory_factor = 1.5  # Simulate 16GB system\n\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)\n            elif dataset_size &lt;= 2_000_000:\n                base_chunk = int(150_000 * memory_factor)\n            elif dataset_size &lt;= 5_000_000:\n                base_chunk = int(100_000 * memory_factor)\n            else:\n                base_chunk = int(75_000 * memory_factor)\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        memory_manager = MagicMock()\n\n        # Small dataset - largest base chunks\n        small_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n        assert small_chunk == 300_000  # 200K * 1.5\n\n        # Medium dataset - medium base chunks\n        medium_chunk = test_calculate_optimal_chunk_size(1_000_000, memory_manager)\n        assert medium_chunk == 225_000  # 150K * 1.5\n\n        # Large dataset - smaller base chunks\n        large_chunk = test_calculate_optimal_chunk_size(3_000_000, memory_manager)\n        assert large_chunk == 150_000  # 100K * 1.5\n\n        # Very large dataset - smallest base chunks\n        xlarge_chunk = test_calculate_optimal_chunk_size(10_000_000, memory_manager)\n        assert xlarge_chunk == 112_500  # 75K * 1.5\n\n    def test_chunk_size_bounds_enforcement(self):\n        \"\"\"Test that chunk sizes respect minimum and maximum bounds.\"\"\"\n\n        def test_calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            memory_factor = 0.04  # Very small factor to test minimum (0.04 * 200_000 = 8_000 -&gt; min enforced to 10_000)\n\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)\n            else:\n                base_chunk = int(75_000 * memory_factor)\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        memory_manager = MagicMock()\n\n        # Should enforce minimum of 10,000\n        small_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n        assert small_chunk == 10_000\n\n        # Test maximum enforcement with very high memory factor\n        def test_calculate_max_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            memory_factor = 10.0  # Very high factor to test maximum\n            base_chunk = int(200_000 * memory_factor)  # Would be 2M\n            return max(10_000, min(base_chunk, 500_000))\n\n        max_chunk = test_calculate_max_chunk_size(100_000, memory_manager)\n        assert max_chunk == 500_000  # Should be capped at maximum\n\n    def test_base_chunk_increases_validation(self):\n        \"\"\"Test that base chunk sizes have increased from 50K to 150K-200K.\"\"\"\n\n        # This validates the Phase 2 implementation\n        def test_calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            memory_factor = 1.0  # Standard system\n\n            # These are the new base sizes (Phase 2)\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)  # Was 50K, now 200K\n            elif dataset_size &lt;= 2_000_000:\n                base_chunk = int(150_000 * memory_factor)  # Was 50K, now 150K\n            else:\n                base_chunk = int(100_000 * memory_factor)  # Was 50K, now 100K+\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        memory_manager = MagicMock()\n\n        # Verify chunk sizes are significantly larger than old 50K base\n        small_dataset_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n        assert small_dataset_chunk &gt;= 150_000  # At least 3x larger than old 50K\n\n        medium_dataset_chunk = test_calculate_optimal_chunk_size(\n            1_000_000, memory_manager\n        )\n        assert medium_dataset_chunk &gt;= 150_000  # At least 3x larger than old 50K\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestAdaptiveChunkSizing.test_adaptive_chunk_scaling_by_dataset_size","title":"<code>test_adaptive_chunk_scaling_by_dataset_size()</code>","text":"<p>Test that chunk sizes scale appropriately with dataset size.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_adaptive_chunk_scaling_by_dataset_size(self):\n    \"\"\"Test that chunk sizes scale appropriately with dataset size.\"\"\"\n\n    def test_calculate_optimal_chunk_size(\n        dataset_size: int, memory_manager=None\n    ) -&gt; int:\n        memory_factor = 1.5  # Simulate 16GB system\n\n        if dataset_size &lt;= 500_000:\n            base_chunk = int(200_000 * memory_factor)\n        elif dataset_size &lt;= 2_000_000:\n            base_chunk = int(150_000 * memory_factor)\n        elif dataset_size &lt;= 5_000_000:\n            base_chunk = int(100_000 * memory_factor)\n        else:\n            base_chunk = int(75_000 * memory_factor)\n\n        return max(10_000, min(base_chunk, 500_000))\n\n    memory_manager = MagicMock()\n\n    # Small dataset - largest base chunks\n    small_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n    assert small_chunk == 300_000  # 200K * 1.5\n\n    # Medium dataset - medium base chunks\n    medium_chunk = test_calculate_optimal_chunk_size(1_000_000, memory_manager)\n    assert medium_chunk == 225_000  # 150K * 1.5\n\n    # Large dataset - smaller base chunks\n    large_chunk = test_calculate_optimal_chunk_size(3_000_000, memory_manager)\n    assert large_chunk == 150_000  # 100K * 1.5\n\n    # Very large dataset - smallest base chunks\n    xlarge_chunk = test_calculate_optimal_chunk_size(10_000_000, memory_manager)\n    assert xlarge_chunk == 112_500  # 75K * 1.5\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestAdaptiveChunkSizing.test_base_chunk_increases_validation","title":"<code>test_base_chunk_increases_validation()</code>","text":"<p>Test that base chunk sizes have increased from 50K to 150K-200K.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_base_chunk_increases_validation(self):\n    \"\"\"Test that base chunk sizes have increased from 50K to 150K-200K.\"\"\"\n\n    # This validates the Phase 2 implementation\n    def test_calculate_optimal_chunk_size(\n        dataset_size: int, memory_manager=None\n    ) -&gt; int:\n        memory_factor = 1.0  # Standard system\n\n        # These are the new base sizes (Phase 2)\n        if dataset_size &lt;= 500_000:\n            base_chunk = int(200_000 * memory_factor)  # Was 50K, now 200K\n        elif dataset_size &lt;= 2_000_000:\n            base_chunk = int(150_000 * memory_factor)  # Was 50K, now 150K\n        else:\n            base_chunk = int(100_000 * memory_factor)  # Was 50K, now 100K+\n\n        return max(10_000, min(base_chunk, 500_000))\n\n    memory_manager = MagicMock()\n\n    # Verify chunk sizes are significantly larger than old 50K base\n    small_dataset_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n    assert small_dataset_chunk &gt;= 150_000  # At least 3x larger than old 50K\n\n    medium_dataset_chunk = test_calculate_optimal_chunk_size(\n        1_000_000, memory_manager\n    )\n    assert medium_dataset_chunk &gt;= 150_000  # At least 3x larger than old 50K\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestAdaptiveChunkSizing.test_calculate_optimal_chunk_size_memory_factors","title":"<code>test_calculate_optimal_chunk_size_memory_factors()</code>","text":"<p>Test memory factor calculation for different system sizes.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_calculate_optimal_chunk_size_memory_factors(self):\n    \"\"\"Test memory factor calculation for different system sizes.\"\"\"\n    from analyzers.ngrams.ngrams_base.main import main\n\n    # Mock memory manager for different system sizes\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test 8GB system (factor = 1.0)\n        mock_vm.return_value.total = 8 * 1024**3\n        memory_manager = MemoryManager()\n\n        # Use the calculate_optimal_chunk_size function from the ngrams_base module\n        # We need to access it through the main function's internal definition\n        # For testing, we'll create a similar function\n        def test_calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            if memory_manager:\n                total_gb = psutil.virtual_memory().total / 1024**3\n                if total_gb &gt;= 32:\n                    memory_factor = 2.0\n                elif total_gb &gt;= 16:\n                    memory_factor = 1.5\n                elif total_gb &gt;= 8:\n                    memory_factor = 1.0\n                else:\n                    memory_factor = 0.5\n            else:\n                memory_factor = 1.0\n\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)\n            elif dataset_size &lt;= 2_000_000:\n                base_chunk = int(150_000 * memory_factor)\n            elif dataset_size &lt;= 5_000_000:\n                base_chunk = int(100_000 * memory_factor)\n            else:\n                base_chunk = int(75_000 * memory_factor)\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        # Test different system sizes\n        # 8GB system\n        mock_vm.return_value.total = 8 * 1024**3\n        chunk_size_8gb = test_calculate_optimal_chunk_size(\n            1_000_000, memory_manager\n        )\n        assert chunk_size_8gb == 150_000  # 150K * 1.0\n\n        # 16GB system\n        mock_vm.return_value.total = 16 * 1024**3\n        chunk_size_16gb = test_calculate_optimal_chunk_size(\n            1_000_000, memory_manager\n        )\n        assert chunk_size_16gb == 225_000  # 150K * 1.5\n\n        # 32GB system\n        mock_vm.return_value.total = 32 * 1024**3\n        chunk_size_32gb = test_calculate_optimal_chunk_size(\n            1_000_000, memory_manager\n        )\n        assert chunk_size_32gb == 300_000  # 150K * 2.0\n\n        # 4GB system\n        mock_vm.return_value.total = 4 * 1024**3\n        chunk_size_4gb = test_calculate_optimal_chunk_size(\n            1_000_000, memory_manager\n        )\n        assert chunk_size_4gb == 75_000  # 150K * 0.5\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestAdaptiveChunkSizing.test_chunk_size_bounds_enforcement","title":"<code>test_chunk_size_bounds_enforcement()</code>","text":"<p>Test that chunk sizes respect minimum and maximum bounds.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_chunk_size_bounds_enforcement(self):\n    \"\"\"Test that chunk sizes respect minimum and maximum bounds.\"\"\"\n\n    def test_calculate_optimal_chunk_size(\n        dataset_size: int, memory_manager=None\n    ) -&gt; int:\n        memory_factor = 0.04  # Very small factor to test minimum (0.04 * 200_000 = 8_000 -&gt; min enforced to 10_000)\n\n        if dataset_size &lt;= 500_000:\n            base_chunk = int(200_000 * memory_factor)\n        else:\n            base_chunk = int(75_000 * memory_factor)\n\n        return max(10_000, min(base_chunk, 500_000))\n\n    memory_manager = MagicMock()\n\n    # Should enforce minimum of 10,000\n    small_chunk = test_calculate_optimal_chunk_size(100_000, memory_manager)\n    assert small_chunk == 10_000\n\n    # Test maximum enforcement with very high memory factor\n    def test_calculate_max_chunk_size(\n        dataset_size: int, memory_manager=None\n    ) -&gt; int:\n        memory_factor = 10.0  # Very high factor to test maximum\n        base_chunk = int(200_000 * memory_factor)  # Would be 2M\n        return max(10_000, min(base_chunk, 500_000))\n\n    max_chunk = test_calculate_max_chunk_size(100_000, memory_manager)\n    assert max_chunk == 500_000  # Should be capped at maximum\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestErrorHandlingAndEdgeCases","title":"<code>TestErrorHandlingAndEdgeCases</code>","text":"<p>Test error handling and edge cases.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestErrorHandlingAndEdgeCases:\n    \"\"\"Test error handling and edge cases.\"\"\"\n\n    def test_zero_memory_system_handling(self):\n        \"\"\"Test handling of systems with very little memory.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Simulate system with very little memory\n            mock_vm.return_value.total = 512 * 1024**2  # 512MB\n\n            limit = MemoryManager._auto_detect_memory_limit()\n\n            # Should still provide some allocation\n            assert limit &gt; 0\n            assert limit &lt; 1.0  # Should be less than 1GB\n\n            # Should use conservative 20% allocation\n            expected = (512 / 1024) * 0.2  # 512MB * 20% = ~0.1GB\n            assert abs(limit - expected) &lt; 0.05\n\n    def test_memory_manager_initialization_errors(self):\n        \"\"\"Test memory manager handles initialization errors gracefully.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Simulate psutil error\n            mock_vm.side_effect = Exception(\"Memory detection failed\")\n\n            # Should fall back to reasonable default\n            with pytest.raises(Exception):\n                MemoryManager()\n\n    def test_chunk_size_calculation_edge_cases(self):\n        \"\"\"Test chunk size calculation with edge case inputs.\"\"\"\n\n        def test_calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            memory_factor = 1.0 if memory_manager else 1.0\n\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)\n            elif dataset_size &lt;= 2_000_000:\n                base_chunk = int(150_000 * memory_factor)\n            elif dataset_size &lt;= 5_000_000:\n                base_chunk = int(100_000 * memory_factor)\n            else:\n                base_chunk = int(75_000 * memory_factor)\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        # Test with zero dataset size\n        chunk_size = test_calculate_optimal_chunk_size(0)\n        assert chunk_size &gt;= 10_000  # Should enforce minimum\n\n        # Test with very large dataset\n        chunk_size = test_calculate_optimal_chunk_size(100_000_000)\n        assert chunk_size &lt;= 500_000  # Should enforce maximum\n\n        # Test with exactly boundary values\n        chunk_size = test_calculate_optimal_chunk_size(500_000)\n        assert chunk_size &gt; 0\n\n    def test_fallback_mechanisms_under_pressure(self):\n        \"\"\"Test that fallback mechanisms work under genuine memory pressure.\"\"\"\n        memory_manager = MemoryManager(max_memory_gb=0.5)  # Very limited\n\n        # Mock the process memory info to simulate critical pressure\n        with patch.object(memory_manager.process, \"memory_info\") as mock_memory:\n            # Simulate critical memory usage (95% of max)\n            mock_memory.return_value.rss = int(0.95 * memory_manager.max_memory_bytes)\n\n            # Should drastically reduce chunk size under critical pressure\n            base_size = 100_000\n            adaptive_size = memory_manager.calculate_adaptive_chunk_size(\n                base_size, \"ngram_generation\"\n            )\n\n            # Should be significantly reduced\n            assert adaptive_size &lt; base_size * 0.5\n\n            # Should still be above minimum\n            expected_min = max(1000, base_size // 10)\n            assert adaptive_size &gt;= expected_min\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestErrorHandlingAndEdgeCases.test_chunk_size_calculation_edge_cases","title":"<code>test_chunk_size_calculation_edge_cases()</code>","text":"<p>Test chunk size calculation with edge case inputs.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_chunk_size_calculation_edge_cases(self):\n    \"\"\"Test chunk size calculation with edge case inputs.\"\"\"\n\n    def test_calculate_optimal_chunk_size(\n        dataset_size: int, memory_manager=None\n    ) -&gt; int:\n        memory_factor = 1.0 if memory_manager else 1.0\n\n        if dataset_size &lt;= 500_000:\n            base_chunk = int(200_000 * memory_factor)\n        elif dataset_size &lt;= 2_000_000:\n            base_chunk = int(150_000 * memory_factor)\n        elif dataset_size &lt;= 5_000_000:\n            base_chunk = int(100_000 * memory_factor)\n        else:\n            base_chunk = int(75_000 * memory_factor)\n\n        return max(10_000, min(base_chunk, 500_000))\n\n    # Test with zero dataset size\n    chunk_size = test_calculate_optimal_chunk_size(0)\n    assert chunk_size &gt;= 10_000  # Should enforce minimum\n\n    # Test with very large dataset\n    chunk_size = test_calculate_optimal_chunk_size(100_000_000)\n    assert chunk_size &lt;= 500_000  # Should enforce maximum\n\n    # Test with exactly boundary values\n    chunk_size = test_calculate_optimal_chunk_size(500_000)\n    assert chunk_size &gt; 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestErrorHandlingAndEdgeCases.test_fallback_mechanisms_under_pressure","title":"<code>test_fallback_mechanisms_under_pressure()</code>","text":"<p>Test that fallback mechanisms work under genuine memory pressure.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_fallback_mechanisms_under_pressure(self):\n    \"\"\"Test that fallback mechanisms work under genuine memory pressure.\"\"\"\n    memory_manager = MemoryManager(max_memory_gb=0.5)  # Very limited\n\n    # Mock the process memory info to simulate critical pressure\n    with patch.object(memory_manager.process, \"memory_info\") as mock_memory:\n        # Simulate critical memory usage (95% of max)\n        mock_memory.return_value.rss = int(0.95 * memory_manager.max_memory_bytes)\n\n        # Should drastically reduce chunk size under critical pressure\n        base_size = 100_000\n        adaptive_size = memory_manager.calculate_adaptive_chunk_size(\n            base_size, \"ngram_generation\"\n        )\n\n        # Should be significantly reduced\n        assert adaptive_size &lt; base_size * 0.5\n\n        # Should still be above minimum\n        expected_min = max(1000, base_size // 10)\n        assert adaptive_size &gt;= expected_min\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestErrorHandlingAndEdgeCases.test_memory_manager_initialization_errors","title":"<code>test_memory_manager_initialization_errors()</code>","text":"<p>Test memory manager handles initialization errors gracefully.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_manager_initialization_errors(self):\n    \"\"\"Test memory manager handles initialization errors gracefully.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Simulate psutil error\n        mock_vm.side_effect = Exception(\"Memory detection failed\")\n\n        # Should fall back to reasonable default\n        with pytest.raises(Exception):\n            MemoryManager()\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestErrorHandlingAndEdgeCases.test_zero_memory_system_handling","title":"<code>test_zero_memory_system_handling()</code>","text":"<p>Test handling of systems with very little memory.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_zero_memory_system_handling(self):\n    \"\"\"Test handling of systems with very little memory.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Simulate system with very little memory\n        mock_vm.return_value.total = 512 * 1024**2  # 512MB\n\n        limit = MemoryManager._auto_detect_memory_limit()\n\n        # Should still provide some allocation\n        assert limit &gt; 0\n        assert limit &lt; 1.0  # Should be less than 1GB\n\n        # Should use conservative 20% allocation\n        expected = (512 / 1024) * 0.2  # 512MB * 20% = ~0.1GB\n        assert abs(limit - expected) &lt; 0.05\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestFallbackOptimization","title":"<code>TestFallbackOptimization</code>","text":"<p>Test fallback processor optimizations.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestFallbackOptimization:\n    \"\"\"Test fallback processor optimizations.\"\"\"\n\n    def test_fallback_base_chunk_increase(self):\n        \"\"\"Test that fallback base chunks increased from 25K to 100K.\"\"\"\n        memory_manager = MagicMock()\n        memory_manager.calculate_adaptive_chunk_size.return_value = 100_000  # New base\n\n        # The fallback processors should now use 100K as base instead of 25K\n        # This is verified by checking the base value passed to calculate_adaptive_chunk_size\n        memory_manager.calculate_adaptive_chunk_size.assert_not_called()\n\n        # Call the function that would use the base chunk size\n        chunk_size = memory_manager.calculate_adaptive_chunk_size(\n            100_000, \"ngram_generation\"\n        )\n\n        # Should return the new larger base size\n        assert chunk_size == 100_000\n\n    def test_memory_aware_fallback_thresholds(self):\n        \"\"\"Test memory-aware fallback thresholds for different system sizes.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test 32GB system - should have 3M row threshold\n            mock_vm.return_value.total = 32 * 1024**3\n            memory_manager = MemoryManager()\n\n            # Simulate the threshold calculation logic from Phase 3\n            total_gb = psutil.virtual_memory().total / 1024**3\n            if total_gb &gt;= 32:\n                threshold = 3_000_000\n            elif total_gb &gt;= 16:\n                threshold = 1_500_000\n            else:\n                threshold = 500_000\n\n            assert threshold == 3_000_000\n\n            # Test 16GB system - should have 1.5M row threshold\n            mock_vm.return_value.total = 16 * 1024**3\n            total_gb = psutil.virtual_memory().total / 1024**3\n            if total_gb &gt;= 32:\n                threshold = 3_000_000\n            elif total_gb &gt;= 16:\n                threshold = 1_500_000\n            else:\n                threshold = 500_000\n\n            assert threshold == 1_500_000\n\n            # Test 8GB system - should keep 500K row threshold\n            mock_vm.return_value.total = 8 * 1024**3\n            total_gb = psutil.virtual_memory().total / 1024**3\n            if total_gb &gt;= 32:\n                threshold = 3_000_000\n            elif total_gb &gt;= 16:\n                threshold = 1_500_000\n            else:\n                threshold = 500_000\n\n            assert threshold == 500_000\n\n    def test_fallback_threshold_scaling(self):\n        \"\"\"Test that fallback thresholds scale appropriately (500K \u2192 1.5M \u2192 3M).\"\"\"\n        # Validate the 3x and 6x increases for different system tiers\n        old_threshold = 500_000\n\n        # 16GB system gets 3x increase\n        threshold_16gb = 1_500_000\n        assert threshold_16gb / old_threshold == 3.0\n\n        # 32GB system gets 6x increase\n        threshold_32gb = 3_000_000\n        assert threshold_32gb / old_threshold == 6.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestFallbackOptimization.test_fallback_base_chunk_increase","title":"<code>test_fallback_base_chunk_increase()</code>","text":"<p>Test that fallback base chunks increased from 25K to 100K.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_fallback_base_chunk_increase(self):\n    \"\"\"Test that fallback base chunks increased from 25K to 100K.\"\"\"\n    memory_manager = MagicMock()\n    memory_manager.calculate_adaptive_chunk_size.return_value = 100_000  # New base\n\n    # The fallback processors should now use 100K as base instead of 25K\n    # This is verified by checking the base value passed to calculate_adaptive_chunk_size\n    memory_manager.calculate_adaptive_chunk_size.assert_not_called()\n\n    # Call the function that would use the base chunk size\n    chunk_size = memory_manager.calculate_adaptive_chunk_size(\n        100_000, \"ngram_generation\"\n    )\n\n    # Should return the new larger base size\n    assert chunk_size == 100_000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestFallbackOptimization.test_fallback_threshold_scaling","title":"<code>test_fallback_threshold_scaling()</code>","text":"<p>Test that fallback thresholds scale appropriately (500K \u2192 1.5M \u2192 3M).</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_fallback_threshold_scaling(self):\n    \"\"\"Test that fallback thresholds scale appropriately (500K \u2192 1.5M \u2192 3M).\"\"\"\n    # Validate the 3x and 6x increases for different system tiers\n    old_threshold = 500_000\n\n    # 16GB system gets 3x increase\n    threshold_16gb = 1_500_000\n    assert threshold_16gb / old_threshold == 3.0\n\n    # 32GB system gets 6x increase\n    threshold_32gb = 3_000_000\n    assert threshold_32gb / old_threshold == 6.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestFallbackOptimization.test_memory_aware_fallback_thresholds","title":"<code>test_memory_aware_fallback_thresholds()</code>","text":"<p>Test memory-aware fallback thresholds for different system sizes.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_aware_fallback_thresholds(self):\n    \"\"\"Test memory-aware fallback thresholds for different system sizes.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test 32GB system - should have 3M row threshold\n        mock_vm.return_value.total = 32 * 1024**3\n        memory_manager = MemoryManager()\n\n        # Simulate the threshold calculation logic from Phase 3\n        total_gb = psutil.virtual_memory().total / 1024**3\n        if total_gb &gt;= 32:\n            threshold = 3_000_000\n        elif total_gb &gt;= 16:\n            threshold = 1_500_000\n        else:\n            threshold = 500_000\n\n        assert threshold == 3_000_000\n\n        # Test 16GB system - should have 1.5M row threshold\n        mock_vm.return_value.total = 16 * 1024**3\n        total_gb = psutil.virtual_memory().total / 1024**3\n        if total_gb &gt;= 32:\n            threshold = 3_000_000\n        elif total_gb &gt;= 16:\n            threshold = 1_500_000\n        else:\n            threshold = 500_000\n\n        assert threshold == 1_500_000\n\n        # Test 8GB system - should keep 500K row threshold\n        mock_vm.return_value.total = 8 * 1024**3\n        total_gb = psutil.virtual_memory().total / 1024**3\n        if total_gb &gt;= 32:\n            threshold = 3_000_000\n        elif total_gb &gt;= 16:\n            threshold = 1_500_000\n        else:\n            threshold = 500_000\n\n        assert threshold == 500_000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestIntegrationValidation","title":"<code>TestIntegrationValidation</code>","text":"<p>Integration tests validating end-to-end improvements.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestIntegrationValidation:\n    \"\"\"Integration tests validating end-to-end improvements.\"\"\"\n\n    def test_memory_manager_integration_with_ngram_analyzer(self):\n        \"\"\"Test that memory manager integrates properly with n-gram analyzer.\"\"\"\n        # This would test the actual integration, but we'll mock it to avoid\n        # running the full analyzer in tests\n\n        memory_manager = MemoryManager()\n\n        # Simulate the integration points\n        assert memory_manager.max_memory_gb &gt; 0\n        assert hasattr(memory_manager, \"calculate_adaptive_chunk_size\")\n\n        # Test that the memory manager can be passed to analyzer functions\n        base_chunk = 100_000\n        adaptive_chunk = memory_manager.calculate_adaptive_chunk_size(\n            base_chunk, \"ngram_generation\"\n        )\n\n        # Should return a reasonable chunk size\n        assert adaptive_chunk &gt; 0\n        assert adaptive_chunk &lt;= base_chunk * 2  # Allow for some scaling up\n\n    def test_system_specific_performance_characteristics(self):\n        \"\"\"Test that different system configurations get appropriate performance.\"\"\"\n        test_systems = [\n            (4, 0.8),  # 4GB system, 20% allocation\n            (8, 2.0),  # 8GB system, 25% allocation\n            (16, 4.8),  # 16GB system, 30% allocation\n            (32, 12.8),  # 32GB system, 40% allocation\n        ]\n\n        for total_gb, expected_limit in test_systems:\n            with patch(\"psutil.virtual_memory\") as mock_vm:\n                mock_vm.return_value.total = total_gb * 1024**3\n\n                manager = MemoryManager()\n\n                # Should allocate appropriate amount\n                assert abs(manager.max_memory_gb - expected_limit) &lt; 0.1\n\n                # Higher memory systems should get better performance\n                if total_gb &gt;= 16:\n                    assert manager.max_memory_gb &gt;= 4.0\n\n                    # Should have more lenient pressure thresholds\n                    assert manager.thresholds[MemoryPressureLevel.MEDIUM] &gt;= 0.70\n\n    @pytest.mark.skipif(\n        psutil.virtual_memory().total &lt; 8 * 1024**3,\n        reason=\"Requires at least 8GB RAM for meaningful performance test\",\n    )\n    def test_real_system_performance_validation(self):\n        \"\"\"Test performance improvements on real system (when possible).\"\"\"\n        # Only run on systems with sufficient memory\n        system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n        manager = MemoryManager()\n\n        # Auto-detection should work correctly\n        assert manager.max_memory_gb &gt; 0\n        assert manager.max_memory_gb &lt;= system_memory_gb * 0.5  # Reasonable upper bound\n\n        # Should provide better performance than old hardcoded 4GB limit\n        if system_memory_gb &gt;= 16:\n            assert manager.max_memory_gb &gt; 4.0\n        elif system_memory_gb &gt;= 8:\n            assert manager.max_memory_gb &gt;= 2.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestIntegrationValidation.test_memory_manager_integration_with_ngram_analyzer","title":"<code>test_memory_manager_integration_with_ngram_analyzer()</code>","text":"<p>Test that memory manager integrates properly with n-gram analyzer.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_manager_integration_with_ngram_analyzer(self):\n    \"\"\"Test that memory manager integrates properly with n-gram analyzer.\"\"\"\n    # This would test the actual integration, but we'll mock it to avoid\n    # running the full analyzer in tests\n\n    memory_manager = MemoryManager()\n\n    # Simulate the integration points\n    assert memory_manager.max_memory_gb &gt; 0\n    assert hasattr(memory_manager, \"calculate_adaptive_chunk_size\")\n\n    # Test that the memory manager can be passed to analyzer functions\n    base_chunk = 100_000\n    adaptive_chunk = memory_manager.calculate_adaptive_chunk_size(\n        base_chunk, \"ngram_generation\"\n    )\n\n    # Should return a reasonable chunk size\n    assert adaptive_chunk &gt; 0\n    assert adaptive_chunk &lt;= base_chunk * 2  # Allow for some scaling up\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestIntegrationValidation.test_real_system_performance_validation","title":"<code>test_real_system_performance_validation()</code>","text":"<p>Test performance improvements on real system (when possible).</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>@pytest.mark.skipif(\n    psutil.virtual_memory().total &lt; 8 * 1024**3,\n    reason=\"Requires at least 8GB RAM for meaningful performance test\",\n)\ndef test_real_system_performance_validation(self):\n    \"\"\"Test performance improvements on real system (when possible).\"\"\"\n    # Only run on systems with sufficient memory\n    system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n    manager = MemoryManager()\n\n    # Auto-detection should work correctly\n    assert manager.max_memory_gb &gt; 0\n    assert manager.max_memory_gb &lt;= system_memory_gb * 0.5  # Reasonable upper bound\n\n    # Should provide better performance than old hardcoded 4GB limit\n    if system_memory_gb &gt;= 16:\n        assert manager.max_memory_gb &gt; 4.0\n    elif system_memory_gb &gt;= 8:\n        assert manager.max_memory_gb &gt;= 2.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestIntegrationValidation.test_system_specific_performance_characteristics","title":"<code>test_system_specific_performance_characteristics()</code>","text":"<p>Test that different system configurations get appropriate performance.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_system_specific_performance_characteristics(self):\n    \"\"\"Test that different system configurations get appropriate performance.\"\"\"\n    test_systems = [\n        (4, 0.8),  # 4GB system, 20% allocation\n        (8, 2.0),  # 8GB system, 25% allocation\n        (16, 4.8),  # 16GB system, 30% allocation\n        (32, 12.8),  # 32GB system, 40% allocation\n    ]\n\n    for total_gb, expected_limit in test_systems:\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = total_gb * 1024**3\n\n            manager = MemoryManager()\n\n            # Should allocate appropriate amount\n            assert abs(manager.max_memory_gb - expected_limit) &lt; 0.1\n\n            # Higher memory systems should get better performance\n            if total_gb &gt;= 16:\n                assert manager.max_memory_gb &gt;= 4.0\n\n                # Should have more lenient pressure thresholds\n                assert manager.thresholds[MemoryPressureLevel.MEDIUM] &gt;= 0.70\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection","title":"<code>TestMemoryAutoDetection</code>","text":"<p>Test smart memory detection functionality.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestMemoryAutoDetection:\n    \"\"\"Test smart memory detection functionality.\"\"\"\n\n    def test_auto_detection_tiers(self):\n        \"\"\"Test memory detection tiers work correctly.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test 8GB system (25% allocation)\n            mock_vm.return_value.total = 8 * 1024**3\n            limit = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit - 2.0) &lt; 0.1  # 8GB * 0.25 = 2GB\n\n            # Test 16GB system (30% allocation)\n            mock_vm.return_value.total = 16 * 1024**3\n            limit = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit - 4.8) &lt; 0.1  # 16GB * 0.30 = 4.8GB\n\n            # Test 32GB system (40% allocation)\n            mock_vm.return_value.total = 32 * 1024**3\n            limit = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit - 12.8) &lt; 0.1  # 32GB * 0.40 = 12.8GB\n\n            # Test 4GB system (20% allocation - constrained)\n            mock_vm.return_value.total = 4 * 1024**3\n            limit = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit - 0.8) &lt; 0.1  # 4GB * 0.20 = 0.8GB\n\n    def test_auto_detection_vs_manual_override(self):\n        \"\"\"Test that manual override works and auto-detection is bypassed.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 16 * 1024**3\n\n            # Auto-detection should give 4.8GB\n            auto_manager = MemoryManager()\n            assert abs(auto_manager.max_memory_gb - 4.8) &lt; 0.1\n\n            # Manual override should use exact value\n            manual_manager = MemoryManager(max_memory_gb=8.0)\n            assert manual_manager.max_memory_gb == 8.0\n\n    def test_memory_detection_logging(self):\n        \"\"\"Test that memory detection logs appropriate information.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 16 * 1024**3\n\n            with patch(\"app.utils.get_logger\") as mock_logger:\n                mock_log = MagicMock()\n                mock_logger.return_value = mock_log\n\n                MemoryManager()\n\n                # Should log initialization details\n                mock_log.info.assert_called()\n                call_args = mock_log.info.call_args\n                assert \"Memory manager initialized\" in call_args[0][0]\n\n                extra_data = call_args[1][\"extra\"]\n                assert \"system_total_gb\" in extra_data\n                assert \"detected_limit_gb\" in extra_data\n                assert \"allocation_percent\" in extra_data\n                assert extra_data[\"detection_method\"] == \"auto\"\n\n    def test_updated_memory_pressure_thresholds(self):\n        \"\"\"Test that updated pressure thresholds are more lenient.\"\"\"\n        manager = MemoryManager(max_memory_gb=1.0)\n\n        # Test new more lenient thresholds\n        assert manager.thresholds[MemoryPressureLevel.MEDIUM] == 0.70  # Was 0.60\n        assert manager.thresholds[MemoryPressureLevel.HIGH] == 0.80  # Was 0.75\n        assert manager.thresholds[MemoryPressureLevel.CRITICAL] == 0.90  # Was 0.85\n\n    def test_updated_chunk_size_factors(self):\n        \"\"\"Test that chunk size reductions are less aggressive.\"\"\"\n        manager = MemoryManager()\n\n        # Test less aggressive chunk size reduction factors\n        assert manager.chunk_size_factors[MemoryPressureLevel.LOW] == 1.0\n        assert manager.chunk_size_factors[MemoryPressureLevel.MEDIUM] == 0.8  # Was 0.7\n        assert manager.chunk_size_factors[MemoryPressureLevel.HIGH] == 0.6  # Was 0.4\n        assert (\n            manager.chunk_size_factors[MemoryPressureLevel.CRITICAL] == 0.4\n        )  # Was 0.2\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection.test_auto_detection_tiers","title":"<code>test_auto_detection_tiers()</code>","text":"<p>Test memory detection tiers work correctly.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_auto_detection_tiers(self):\n    \"\"\"Test memory detection tiers work correctly.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test 8GB system (25% allocation)\n        mock_vm.return_value.total = 8 * 1024**3\n        limit = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit - 2.0) &lt; 0.1  # 8GB * 0.25 = 2GB\n\n        # Test 16GB system (30% allocation)\n        mock_vm.return_value.total = 16 * 1024**3\n        limit = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit - 4.8) &lt; 0.1  # 16GB * 0.30 = 4.8GB\n\n        # Test 32GB system (40% allocation)\n        mock_vm.return_value.total = 32 * 1024**3\n        limit = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit - 12.8) &lt; 0.1  # 32GB * 0.40 = 12.8GB\n\n        # Test 4GB system (20% allocation - constrained)\n        mock_vm.return_value.total = 4 * 1024**3\n        limit = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit - 0.8) &lt; 0.1  # 4GB * 0.20 = 0.8GB\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection.test_auto_detection_vs_manual_override","title":"<code>test_auto_detection_vs_manual_override()</code>","text":"<p>Test that manual override works and auto-detection is bypassed.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_auto_detection_vs_manual_override(self):\n    \"\"\"Test that manual override works and auto-detection is bypassed.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 16 * 1024**3\n\n        # Auto-detection should give 4.8GB\n        auto_manager = MemoryManager()\n        assert abs(auto_manager.max_memory_gb - 4.8) &lt; 0.1\n\n        # Manual override should use exact value\n        manual_manager = MemoryManager(max_memory_gb=8.0)\n        assert manual_manager.max_memory_gb == 8.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection.test_memory_detection_logging","title":"<code>test_memory_detection_logging()</code>","text":"<p>Test that memory detection logs appropriate information.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_detection_logging(self):\n    \"\"\"Test that memory detection logs appropriate information.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 16 * 1024**3\n\n        with patch(\"app.utils.get_logger\") as mock_logger:\n            mock_log = MagicMock()\n            mock_logger.return_value = mock_log\n\n            MemoryManager()\n\n            # Should log initialization details\n            mock_log.info.assert_called()\n            call_args = mock_log.info.call_args\n            assert \"Memory manager initialized\" in call_args[0][0]\n\n            extra_data = call_args[1][\"extra\"]\n            assert \"system_total_gb\" in extra_data\n            assert \"detected_limit_gb\" in extra_data\n            assert \"allocation_percent\" in extra_data\n            assert extra_data[\"detection_method\"] == \"auto\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection.test_updated_chunk_size_factors","title":"<code>test_updated_chunk_size_factors()</code>","text":"<p>Test that chunk size reductions are less aggressive.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_updated_chunk_size_factors(self):\n    \"\"\"Test that chunk size reductions are less aggressive.\"\"\"\n    manager = MemoryManager()\n\n    # Test less aggressive chunk size reduction factors\n    assert manager.chunk_size_factors[MemoryPressureLevel.LOW] == 1.0\n    assert manager.chunk_size_factors[MemoryPressureLevel.MEDIUM] == 0.8  # Was 0.7\n    assert manager.chunk_size_factors[MemoryPressureLevel.HIGH] == 0.6  # Was 0.4\n    assert (\n        manager.chunk_size_factors[MemoryPressureLevel.CRITICAL] == 0.4\n    )  # Was 0.2\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestMemoryAutoDetection.test_updated_memory_pressure_thresholds","title":"<code>test_updated_memory_pressure_thresholds()</code>","text":"<p>Test that updated pressure thresholds are more lenient.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_updated_memory_pressure_thresholds(self):\n    \"\"\"Test that updated pressure thresholds are more lenient.\"\"\"\n    manager = MemoryManager(max_memory_gb=1.0)\n\n    # Test new more lenient thresholds\n    assert manager.thresholds[MemoryPressureLevel.MEDIUM] == 0.70  # Was 0.60\n    assert manager.thresholds[MemoryPressureLevel.HIGH] == 0.80  # Was 0.75\n    assert manager.thresholds[MemoryPressureLevel.CRITICAL] == 0.90  # Was 0.85\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestPerformanceBenchmarks","title":"<code>TestPerformanceBenchmarks</code>","text":"<p>Performance benchmarking tests.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestPerformanceBenchmarks:\n    \"\"\"Performance benchmarking tests.\"\"\"\n\n    def test_chunk_size_performance_scaling(self):\n        \"\"\"Test that larger chunk sizes provide better performance characteristics.\"\"\"\n        # Create test datasets of different sizes\n        small_data = self._create_test_dataset(10_000)\n        medium_data = self._create_test_dataset(100_000)\n        large_data = self._create_test_dataset(500_000)\n\n        # Test with different chunk sizes\n        old_chunk_size = 50_000  # Old base size\n        new_chunk_size = 150_000  # New base size\n\n        # For small datasets, chunk size should be optimized for data size\n        small_optimal = min(len(small_data), new_chunk_size)\n        assert small_optimal &lt;= new_chunk_size\n\n        # For medium datasets, should use larger chunks\n        medium_optimal = min(len(medium_data), new_chunk_size)\n        assert medium_optimal &gt; old_chunk_size\n\n        # For large datasets, should still be reasonable\n        large_optimal = min(len(large_data), new_chunk_size)\n        assert large_optimal &gt;= old_chunk_size\n\n    def test_memory_efficiency_improvements(self):\n        \"\"\"Test that memory efficiency has improved with new chunking.\"\"\"\n        # Test memory manager with different configurations\n        old_memory_manager = MemoryManager(max_memory_gb=4.0)  # Old hardcoded limit\n\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 16 * 1024**3\n            new_memory_manager = MemoryManager()  # Auto-detected limit\n\n            # New manager should have higher limit on 16GB system\n            assert new_memory_manager.max_memory_gb &gt; old_memory_manager.max_memory_gb\n\n            # Should be approximately 4.8GB for 16GB system\n            assert abs(new_memory_manager.max_memory_gb - 4.8) &lt; 0.1\n\n    def test_io_operation_reduction_estimation(self):\n        \"\"\"Test estimation of I/O operation reduction.\"\"\"\n        # Simulate old vs new chunking for a 2M row dataset\n        dataset_size = 2_000_000\n\n        # Old chunking: 50K base chunks\n        old_chunk_size = 50_000\n        old_num_chunks = (dataset_size + old_chunk_size - 1) // old_chunk_size\n\n        # New chunking: 150K base chunks (3x larger)\n        new_chunk_size = 150_000\n        new_num_chunks = (dataset_size + new_chunk_size - 1) // new_chunk_size\n\n        # Should have significantly fewer I/O operations\n        io_reduction_factor = old_num_chunks / new_num_chunks\n        assert io_reduction_factor &gt;= 2.5  # At least 2.5x fewer operations\n        assert io_reduction_factor &lt;= 4.0  # Reasonable upper bound\n\n    def test_progress_reporting_efficiency(self):\n        \"\"\"Test that progress reporting overhead is reduced with larger chunks.\"\"\"\n        # Larger chunks mean fewer progress updates, reducing overhead\n        dataset_size = 1_000_000\n\n        old_chunk_size = 50_000\n        new_chunk_size = 150_000\n\n        old_progress_updates = dataset_size // old_chunk_size\n        new_progress_updates = dataset_size // new_chunk_size\n\n        # Should have fewer progress updates\n        assert new_progress_updates &lt; old_progress_updates\n\n        # Should be approximately 3x fewer updates\n        reduction_ratio = old_progress_updates / new_progress_updates\n        assert 2.5 &lt;= reduction_ratio &lt;= 3.5\n\n    def _create_test_dataset(self, size: int) -&gt; pl.DataFrame:\n        \"\"\"Create a test dataset of specified size.\"\"\"\n        return pl.DataFrame(\n            {\n                \"message_id\": range(size),\n                \"message_text\": [f\"test message {i} with content\" for i in range(size)],\n                \"author_id\": [f\"user_{i % 100}\" for i in range(size)],\n                \"timestamp\": [\"2023-01-01T00:00:00Z\"] * size,\n            }\n        )\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestPerformanceBenchmarks.test_chunk_size_performance_scaling","title":"<code>test_chunk_size_performance_scaling()</code>","text":"<p>Test that larger chunk sizes provide better performance characteristics.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_chunk_size_performance_scaling(self):\n    \"\"\"Test that larger chunk sizes provide better performance characteristics.\"\"\"\n    # Create test datasets of different sizes\n    small_data = self._create_test_dataset(10_000)\n    medium_data = self._create_test_dataset(100_000)\n    large_data = self._create_test_dataset(500_000)\n\n    # Test with different chunk sizes\n    old_chunk_size = 50_000  # Old base size\n    new_chunk_size = 150_000  # New base size\n\n    # For small datasets, chunk size should be optimized for data size\n    small_optimal = min(len(small_data), new_chunk_size)\n    assert small_optimal &lt;= new_chunk_size\n\n    # For medium datasets, should use larger chunks\n    medium_optimal = min(len(medium_data), new_chunk_size)\n    assert medium_optimal &gt; old_chunk_size\n\n    # For large datasets, should still be reasonable\n    large_optimal = min(len(large_data), new_chunk_size)\n    assert large_optimal &gt;= old_chunk_size\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestPerformanceBenchmarks.test_io_operation_reduction_estimation","title":"<code>test_io_operation_reduction_estimation()</code>","text":"<p>Test estimation of I/O operation reduction.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_io_operation_reduction_estimation(self):\n    \"\"\"Test estimation of I/O operation reduction.\"\"\"\n    # Simulate old vs new chunking for a 2M row dataset\n    dataset_size = 2_000_000\n\n    # Old chunking: 50K base chunks\n    old_chunk_size = 50_000\n    old_num_chunks = (dataset_size + old_chunk_size - 1) // old_chunk_size\n\n    # New chunking: 150K base chunks (3x larger)\n    new_chunk_size = 150_000\n    new_num_chunks = (dataset_size + new_chunk_size - 1) // new_chunk_size\n\n    # Should have significantly fewer I/O operations\n    io_reduction_factor = old_num_chunks / new_num_chunks\n    assert io_reduction_factor &gt;= 2.5  # At least 2.5x fewer operations\n    assert io_reduction_factor &lt;= 4.0  # Reasonable upper bound\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestPerformanceBenchmarks.test_memory_efficiency_improvements","title":"<code>test_memory_efficiency_improvements()</code>","text":"<p>Test that memory efficiency has improved with new chunking.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_efficiency_improvements(self):\n    \"\"\"Test that memory efficiency has improved with new chunking.\"\"\"\n    # Test memory manager with different configurations\n    old_memory_manager = MemoryManager(max_memory_gb=4.0)  # Old hardcoded limit\n\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 16 * 1024**3\n        new_memory_manager = MemoryManager()  # Auto-detected limit\n\n        # New manager should have higher limit on 16GB system\n        assert new_memory_manager.max_memory_gb &gt; old_memory_manager.max_memory_gb\n\n        # Should be approximately 4.8GB for 16GB system\n        assert abs(new_memory_manager.max_memory_gb - 4.8) &lt; 0.1\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestPerformanceBenchmarks.test_progress_reporting_efficiency","title":"<code>test_progress_reporting_efficiency()</code>","text":"<p>Test that progress reporting overhead is reduced with larger chunks.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_progress_reporting_efficiency(self):\n    \"\"\"Test that progress reporting overhead is reduced with larger chunks.\"\"\"\n    # Larger chunks mean fewer progress updates, reducing overhead\n    dataset_size = 1_000_000\n\n    old_chunk_size = 50_000\n    new_chunk_size = 150_000\n\n    old_progress_updates = dataset_size // old_chunk_size\n    new_progress_updates = dataset_size // new_chunk_size\n\n    # Should have fewer progress updates\n    assert new_progress_updates &lt; old_progress_updates\n\n    # Should be approximately 3x fewer updates\n    reduction_ratio = old_progress_updates / new_progress_updates\n    assert 2.5 &lt;= reduction_ratio &lt;= 3.5\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestRegressionPrevention","title":"<code>TestRegressionPrevention</code>","text":"<p>Test that existing functionality is not broken.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestRegressionPrevention:\n    \"\"\"Test that existing functionality is not broken.\"\"\"\n\n    def test_existing_memory_manager_api_unchanged(self):\n        \"\"\"Test that existing MemoryManager API continues to work.\"\"\"\n        # Test all existing methods still work\n        manager = MemoryManager(max_memory_gb=2.0)\n\n        # Core functionality\n        assert hasattr(manager, \"get_current_memory_usage\")\n        assert hasattr(manager, \"get_memory_pressure_level\")\n        assert hasattr(manager, \"calculate_adaptive_chunk_size\")\n        assert hasattr(manager, \"should_trigger_gc\")\n        assert hasattr(manager, \"enhanced_gc_cleanup\")\n        assert hasattr(manager, \"get_memory_trend\")\n\n        # All methods should be callable\n        stats = manager.get_current_memory_usage()\n        assert isinstance(stats, dict)\n\n        pressure = manager.get_memory_pressure_level()\n        assert isinstance(pressure, MemoryPressureLevel)\n\n        chunk_size = manager.calculate_adaptive_chunk_size(10000, \"tokenization\")\n        assert isinstance(chunk_size, int)\n        assert chunk_size &gt; 0\n\n    def test_existing_tests_still_pass(self):\n        \"\"\"Ensure that optimization doesn't break existing functionality.\"\"\"\n        # Test that basic memory management still works\n        manager = MemoryManager(max_memory_gb=1.0)\n\n        # Memory usage detection\n        stats = manager.get_current_memory_usage()\n        required_fields = [\n            \"rss_bytes\",\n            \"vms_bytes\",\n            \"rss_mb\",\n            \"vms_mb\",\n            \"rss_gb\",\n            \"system_available_gb\",\n            \"system_used_percent\",\n            \"process_memory_percent\",\n            \"pressure_level\",\n        ]\n\n        for field in required_fields:\n            assert field in stats\n\n        # Adaptive chunk sizing with different operations\n        operations = [\"tokenization\", \"ngram_generation\", \"unique_extraction\"]\n        for operation in operations:\n            chunk_size = manager.calculate_adaptive_chunk_size(10000, operation)\n            assert chunk_size &gt; 0\n            # Allow for operation-specific scaling (unique_extraction uses 1.2x factor)\n            if operation == \"unique_extraction\":\n                assert chunk_size &lt;= 10000 * 1.2  # Allow for scaling up\n            else:\n                assert chunk_size &lt;= 10000  # Should not exceed base for most operations\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestRegressionPrevention.test_existing_memory_manager_api_unchanged","title":"<code>test_existing_memory_manager_api_unchanged()</code>","text":"<p>Test that existing MemoryManager API continues to work.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_existing_memory_manager_api_unchanged(self):\n    \"\"\"Test that existing MemoryManager API continues to work.\"\"\"\n    # Test all existing methods still work\n    manager = MemoryManager(max_memory_gb=2.0)\n\n    # Core functionality\n    assert hasattr(manager, \"get_current_memory_usage\")\n    assert hasattr(manager, \"get_memory_pressure_level\")\n    assert hasattr(manager, \"calculate_adaptive_chunk_size\")\n    assert hasattr(manager, \"should_trigger_gc\")\n    assert hasattr(manager, \"enhanced_gc_cleanup\")\n    assert hasattr(manager, \"get_memory_trend\")\n\n    # All methods should be callable\n    stats = manager.get_current_memory_usage()\n    assert isinstance(stats, dict)\n\n    pressure = manager.get_memory_pressure_level()\n    assert isinstance(pressure, MemoryPressureLevel)\n\n    chunk_size = manager.calculate_adaptive_chunk_size(10000, \"tokenization\")\n    assert isinstance(chunk_size, int)\n    assert chunk_size &gt; 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestRegressionPrevention.test_existing_tests_still_pass","title":"<code>test_existing_tests_still_pass()</code>","text":"<p>Ensure that optimization doesn't break existing functionality.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_existing_tests_still_pass(self):\n    \"\"\"Ensure that optimization doesn't break existing functionality.\"\"\"\n    # Test that basic memory management still works\n    manager = MemoryManager(max_memory_gb=1.0)\n\n    # Memory usage detection\n    stats = manager.get_current_memory_usage()\n    required_fields = [\n        \"rss_bytes\",\n        \"vms_bytes\",\n        \"rss_mb\",\n        \"vms_mb\",\n        \"rss_gb\",\n        \"system_available_gb\",\n        \"system_used_percent\",\n        \"process_memory_percent\",\n        \"pressure_level\",\n    ]\n\n    for field in required_fields:\n        assert field in stats\n\n    # Adaptive chunk sizing with different operations\n    operations = [\"tokenization\", \"ngram_generation\", \"unique_extraction\"]\n    for operation in operations:\n        chunk_size = manager.calculate_adaptive_chunk_size(10000, operation)\n        assert chunk_size &gt; 0\n        # Allow for operation-specific scaling (unique_extraction uses 1.2x factor)\n        if operation == \"unique_extraction\":\n            assert chunk_size &lt;= 10000 * 1.2  # Allow for scaling up\n        else:\n            assert chunk_size &lt;= 10000  # Should not exceed base for most operations\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSecondaryAnalyzerUpdates","title":"<code>TestSecondaryAnalyzerUpdates</code>","text":"<p>Test secondary analyzer chunk size updates.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestSecondaryAnalyzerUpdates:\n    \"\"\"Test secondary analyzer chunk size updates.\"\"\"\n\n    def test_ngram_stats_chunk_limits_updated(self):\n        \"\"\"Test that N-gram stats chunk limits increased significantly.\"\"\"\n\n        # Simulate the new chunk calculation from Phase 4\n        def calculate_ngram_stats_chunk_size(\n            message_ngram_count: int, ngram_count: int\n        ) -&gt; int:\n            # New formula: max(5_000, min(50_000, 500_000 // max(1, message_ngram_count // ngram_count)))\n            base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n            return max(5_000, min(50_000, base_calc))\n\n        def calculate_ngram_stats_chunk_size_old(\n            message_ngram_count: int, ngram_count: int\n        ) -&gt; int:\n            # Old formula: max(1, min(10_000, 100_000 // max(1, message_ngram_count // ngram_count)))\n            base_calc = 100_000 // max(1, message_ngram_count // ngram_count)\n            return max(1, min(10_000, base_calc))\n\n        # Test with various realistic data sizes\n        test_cases = [\n            (100_000, 1_000),  # Small dataset\n            (500_000, 5_000),  # Medium dataset\n            (1_000_000, 10_000),  # Large dataset\n        ]\n\n        for message_ngram_count, ngram_count in test_cases:\n            new_chunk = calculate_ngram_stats_chunk_size(\n                message_ngram_count, ngram_count\n            )\n            old_chunk = calculate_ngram_stats_chunk_size_old(\n                message_ngram_count, ngram_count\n            )\n\n            # New chunks should be significantly larger\n            assert new_chunk &gt;= old_chunk\n\n            # Minimum should be 5,000 instead of 1\n            assert new_chunk &gt;= 5_000\n\n            # Maximum should be 50,000 instead of 10,000\n            if message_ngram_count // ngram_count &lt;= 10:  # Would hit maximum\n                assert new_chunk &lt;= 50_000\n\n    def test_ngram_stats_minimum_chunk_increase(self):\n        \"\"\"Test that minimum chunk size increased from 1 to 5,000.\"\"\"\n\n        # Test edge case where calculation would give very small result\n        def calculate_ngram_stats_chunk_size(\n            message_ngram_count: int, ngram_count: int\n        ) -&gt; int:\n            base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n            return max(5_000, min(50_000, base_calc))\n\n        # Large message_ngram_count relative to ngram_count should hit minimum\n        chunk_size = calculate_ngram_stats_chunk_size(10_000_000, 100_000)\n        assert chunk_size == 5_000  # Should be minimum, not 1\n\n    def test_ngram_stats_maximum_chunk_increase(self):\n        \"\"\"Test that maximum chunk size increased from 10,000 to 50,000.\"\"\"\n\n        def calculate_ngram_stats_chunk_size(\n            message_ngram_count: int, ngram_count: int\n        ) -&gt; int:\n            base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n            return max(5_000, min(50_000, base_calc))\n\n        # Small message_ngram_count relative to ngram_count should hit maximum\n        chunk_size = calculate_ngram_stats_chunk_size(100, 1000)\n        assert chunk_size == 50_000  # Should be new maximum, not 10,000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSecondaryAnalyzerUpdates.test_ngram_stats_chunk_limits_updated","title":"<code>test_ngram_stats_chunk_limits_updated()</code>","text":"<p>Test that N-gram stats chunk limits increased significantly.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_ngram_stats_chunk_limits_updated(self):\n    \"\"\"Test that N-gram stats chunk limits increased significantly.\"\"\"\n\n    # Simulate the new chunk calculation from Phase 4\n    def calculate_ngram_stats_chunk_size(\n        message_ngram_count: int, ngram_count: int\n    ) -&gt; int:\n        # New formula: max(5_000, min(50_000, 500_000 // max(1, message_ngram_count // ngram_count)))\n        base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n        return max(5_000, min(50_000, base_calc))\n\n    def calculate_ngram_stats_chunk_size_old(\n        message_ngram_count: int, ngram_count: int\n    ) -&gt; int:\n        # Old formula: max(1, min(10_000, 100_000 // max(1, message_ngram_count // ngram_count)))\n        base_calc = 100_000 // max(1, message_ngram_count // ngram_count)\n        return max(1, min(10_000, base_calc))\n\n    # Test with various realistic data sizes\n    test_cases = [\n        (100_000, 1_000),  # Small dataset\n        (500_000, 5_000),  # Medium dataset\n        (1_000_000, 10_000),  # Large dataset\n    ]\n\n    for message_ngram_count, ngram_count in test_cases:\n        new_chunk = calculate_ngram_stats_chunk_size(\n            message_ngram_count, ngram_count\n        )\n        old_chunk = calculate_ngram_stats_chunk_size_old(\n            message_ngram_count, ngram_count\n        )\n\n        # New chunks should be significantly larger\n        assert new_chunk &gt;= old_chunk\n\n        # Minimum should be 5,000 instead of 1\n        assert new_chunk &gt;= 5_000\n\n        # Maximum should be 50,000 instead of 10,000\n        if message_ngram_count // ngram_count &lt;= 10:  # Would hit maximum\n            assert new_chunk &lt;= 50_000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSecondaryAnalyzerUpdates.test_ngram_stats_maximum_chunk_increase","title":"<code>test_ngram_stats_maximum_chunk_increase()</code>","text":"<p>Test that maximum chunk size increased from 10,000 to 50,000.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_ngram_stats_maximum_chunk_increase(self):\n    \"\"\"Test that maximum chunk size increased from 10,000 to 50,000.\"\"\"\n\n    def calculate_ngram_stats_chunk_size(\n        message_ngram_count: int, ngram_count: int\n    ) -&gt; int:\n        base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n        return max(5_000, min(50_000, base_calc))\n\n    # Small message_ngram_count relative to ngram_count should hit maximum\n    chunk_size = calculate_ngram_stats_chunk_size(100, 1000)\n    assert chunk_size == 50_000  # Should be new maximum, not 10,000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSecondaryAnalyzerUpdates.test_ngram_stats_minimum_chunk_increase","title":"<code>test_ngram_stats_minimum_chunk_increase()</code>","text":"<p>Test that minimum chunk size increased from 1 to 5,000.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_ngram_stats_minimum_chunk_increase(self):\n    \"\"\"Test that minimum chunk size increased from 1 to 5,000.\"\"\"\n\n    # Test edge case where calculation would give very small result\n    def calculate_ngram_stats_chunk_size(\n        message_ngram_count: int, ngram_count: int\n    ) -&gt; int:\n        base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n        return max(5_000, min(50_000, base_calc))\n\n    # Large message_ngram_count relative to ngram_count should hit minimum\n    chunk_size = calculate_ngram_stats_chunk_size(10_000_000, 100_000)\n    assert chunk_size == 5_000  # Should be minimum, not 1\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSystemConfigurationValidation","title":"<code>TestSystemConfigurationValidation</code>","text":"<p>Test system configuration detection and validation.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>class TestSystemConfigurationValidation:\n    \"\"\"Test system configuration detection and validation.\"\"\"\n\n    def test_memory_usage_stays_within_bounds(self):\n        \"\"\"Test that memory usage stays within auto-detected limits.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test 16GB system\n            mock_vm.return_value.total = 16 * 1024**3\n            memory_manager = MemoryManager()\n\n            # Should allocate 30% of 16GB = 4.8GB\n            expected_limit = 4.8\n            assert abs(memory_manager.max_memory_gb - expected_limit) &lt; 0.1\n\n            # Memory usage should not exceed the limit during processing\n            initial_memory = memory_manager.get_current_memory_usage()\n\n            # Simulate some memory usage\n            large_data = [list(range(1000)) for _ in range(100)]\n            current_memory = memory_manager.get_current_memory_usage()\n\n            # Should still be within reasonable bounds\n            assert (\n                current_memory[\"rss_gb\"] &lt;= memory_manager.max_memory_gb * 1.2\n            )  # 20% tolerance\n\n    def test_memory_pressure_detection_accuracy(self):\n        \"\"\"Test that memory pressure detection works accurately with new thresholds.\"\"\"\n        manager = MemoryManager(max_memory_gb=1.0)\n\n        with patch.object(manager.process, \"memory_info\") as mock_memory:\n            # Test LOW pressure (below 70%)\n            mock_memory.return_value.rss = int(0.5 * manager.max_memory_bytes)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.LOW\n\n            # Test MEDIUM pressure (70-80%)\n            mock_memory.return_value.rss = int(0.75 * manager.max_memory_bytes)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.MEDIUM\n\n            # Test HIGH pressure (80-90%)\n            mock_memory.return_value.rss = int(0.85 * manager.max_memory_bytes)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.HIGH\n\n            # Test CRITICAL pressure (&gt;90%)\n            mock_memory.return_value.rss = int(0.95 * manager.max_memory_bytes)\n            assert manager.get_memory_pressure_level() == MemoryPressureLevel.CRITICAL\n\n    def test_auto_detection_edge_cases(self):\n        \"\"\"Test auto-detection handles edge cases properly.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test exactly at boundaries\n            mock_vm.return_value.total = 8 * 1024**3  # Exactly 8GB\n            limit_8gb = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit_8gb - 2.0) &lt; 0.1  # Should be 25%\n\n            mock_vm.return_value.total = 16 * 1024**3  # Exactly 16GB\n            limit_16gb = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit_16gb - 4.8) &lt; 0.1  # Should be 30%\n\n            mock_vm.return_value.total = 32 * 1024**3  # Exactly 32GB\n            limit_32gb = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit_32gb - 12.8) &lt; 0.1  # Should be 40%\n\n            # Test very small system\n            mock_vm.return_value.total = 2 * 1024**3  # 2GB\n            limit_2gb = MemoryManager._auto_detect_memory_limit()\n            assert abs(limit_2gb - 0.4) &lt; 0.1  # Should be 20%\n\n    def test_backward_compatibility_preserved(self):\n        \"\"\"Test that manual override still works exactly as before.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 16 * 1024**3\n\n            # Manual override should bypass auto-detection completely\n            manager = MemoryManager(max_memory_gb=2.0)\n            assert manager.max_memory_gb == 2.0\n\n            # Should work with any value, even unreasonable ones\n            manager = MemoryManager(max_memory_gb=100.0)\n            assert manager.max_memory_gb == 100.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSystemConfigurationValidation.test_auto_detection_edge_cases","title":"<code>test_auto_detection_edge_cases()</code>","text":"<p>Test auto-detection handles edge cases properly.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_auto_detection_edge_cases(self):\n    \"\"\"Test auto-detection handles edge cases properly.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test exactly at boundaries\n        mock_vm.return_value.total = 8 * 1024**3  # Exactly 8GB\n        limit_8gb = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit_8gb - 2.0) &lt; 0.1  # Should be 25%\n\n        mock_vm.return_value.total = 16 * 1024**3  # Exactly 16GB\n        limit_16gb = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit_16gb - 4.8) &lt; 0.1  # Should be 30%\n\n        mock_vm.return_value.total = 32 * 1024**3  # Exactly 32GB\n        limit_32gb = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit_32gb - 12.8) &lt; 0.1  # Should be 40%\n\n        # Test very small system\n        mock_vm.return_value.total = 2 * 1024**3  # 2GB\n        limit_2gb = MemoryManager._auto_detect_memory_limit()\n        assert abs(limit_2gb - 0.4) &lt; 0.1  # Should be 20%\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSystemConfigurationValidation.test_backward_compatibility_preserved","title":"<code>test_backward_compatibility_preserved()</code>","text":"<p>Test that manual override still works exactly as before.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_backward_compatibility_preserved(self):\n    \"\"\"Test that manual override still works exactly as before.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 16 * 1024**3\n\n        # Manual override should bypass auto-detection completely\n        manager = MemoryManager(max_memory_gb=2.0)\n        assert manager.max_memory_gb == 2.0\n\n        # Should work with any value, even unreasonable ones\n        manager = MemoryManager(max_memory_gb=100.0)\n        assert manager.max_memory_gb == 100.0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSystemConfigurationValidation.test_memory_pressure_detection_accuracy","title":"<code>test_memory_pressure_detection_accuracy()</code>","text":"<p>Test that memory pressure detection works accurately with new thresholds.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_pressure_detection_accuracy(self):\n    \"\"\"Test that memory pressure detection works accurately with new thresholds.\"\"\"\n    manager = MemoryManager(max_memory_gb=1.0)\n\n    with patch.object(manager.process, \"memory_info\") as mock_memory:\n        # Test LOW pressure (below 70%)\n        mock_memory.return_value.rss = int(0.5 * manager.max_memory_bytes)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.LOW\n\n        # Test MEDIUM pressure (70-80%)\n        mock_memory.return_value.rss = int(0.75 * manager.max_memory_bytes)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.MEDIUM\n\n        # Test HIGH pressure (80-90%)\n        mock_memory.return_value.rss = int(0.85 * manager.max_memory_bytes)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.HIGH\n\n        # Test CRITICAL pressure (&gt;90%)\n        mock_memory.return_value.rss = int(0.95 * manager.max_memory_bytes)\n        assert manager.get_memory_pressure_level() == MemoryPressureLevel.CRITICAL\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_chunking_optimization.TestSystemConfigurationValidation.test_memory_usage_stays_within_bounds","title":"<code>test_memory_usage_stays_within_bounds()</code>","text":"<p>Test that memory usage stays within auto-detected limits.</p> Source code in <code>testing/performance/test_chunking_optimization.py</code> <pre><code>def test_memory_usage_stays_within_bounds(self):\n    \"\"\"Test that memory usage stays within auto-detected limits.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test 16GB system\n        mock_vm.return_value.total = 16 * 1024**3\n        memory_manager = MemoryManager()\n\n        # Should allocate 30% of 16GB = 4.8GB\n        expected_limit = 4.8\n        assert abs(memory_manager.max_memory_gb - expected_limit) &lt; 0.1\n\n        # Memory usage should not exceed the limit during processing\n        initial_memory = memory_manager.get_current_memory_usage()\n\n        # Simulate some memory usage\n        large_data = [list(range(1000)) for _ in range(100)]\n        current_memory = memory_manager.get_current_memory_usage()\n\n        # Should still be within reasonable bounds\n        assert (\n            current_memory[\"rss_gb\"] &lt;= memory_manager.max_memory_gb * 1.2\n        )  # 20% tolerance\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks","title":"<code>test_enhanced_benchmarks</code>","text":"<p>Enhanced Performance Benchmarking Tests using pytest-benchmark Implements robust, statistics-driven benchmarks with resource-based metrics.</p>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestBenchmarkIntegration","title":"<code>TestBenchmarkIntegration</code>","text":"<p>Tests for benchmark configuration and integration.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>@pytest.mark.performance\n@pytest.mark.benchmark\nclass TestBenchmarkIntegration:\n    \"\"\"Tests for benchmark configuration and integration.\"\"\"\n\n    def test_benchmark_configuration(self, benchmark):\n        \"\"\"Test that benchmark configuration works correctly.\"\"\"\n\n        def simple_operation():\n            return sum(range(10000))\n\n        result = benchmark(simple_operation)\n        assert result == sum(range(10000))\n\n    def test_benchmark_with_setup(self, benchmark):\n        \"\"\"Test benchmark with setup/teardown operations.\"\"\"\n\n        def setup():\n            return list(range(50000))\n\n        def operation(data):\n            return len([x for x in data if x % 2 == 0])\n\n        result = benchmark.pedantic(operation, setup=setup, rounds=3, iterations=1)\n        assert result == 25000  # Half should be even\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestBenchmarkIntegration.test_benchmark_configuration","title":"<code>test_benchmark_configuration(benchmark)</code>","text":"<p>Test that benchmark configuration works correctly.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_benchmark_configuration(self, benchmark):\n    \"\"\"Test that benchmark configuration works correctly.\"\"\"\n\n    def simple_operation():\n        return sum(range(10000))\n\n    result = benchmark(simple_operation)\n    assert result == sum(range(10000))\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestBenchmarkIntegration.test_benchmark_with_setup","title":"<code>test_benchmark_with_setup(benchmark)</code>","text":"<p>Test benchmark with setup/teardown operations.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_benchmark_with_setup(self, benchmark):\n    \"\"\"Test benchmark with setup/teardown operations.\"\"\"\n\n    def setup():\n        return list(range(50000))\n\n    def operation(data):\n        return len([x for x in data if x % 2 == 0])\n\n    result = benchmark.pedantic(operation, setup=setup, rounds=3, iterations=1)\n    assert result == 25000  # Half should be even\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks","title":"<code>TestEnhancedPerformanceBenchmarks</code>","text":"<p>Enhanced performance benchmarking suite using pytest-benchmark.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>@pytest.mark.performance\n@pytest.mark.benchmark\nclass TestEnhancedPerformanceBenchmarks:\n    \"\"\"Enhanced performance benchmarking suite using pytest-benchmark.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment before each test.\"\"\"\n        # Force garbage collection to start with clean state\n        gc.collect()\n\n        # Get baseline memory usage\n        self.initial_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n\n    def teardown_method(self):\n        \"\"\"Clean up after each test.\"\"\"\n        gc.collect()\n\n    def _create_realistic_dataset(\n        self, num_messages: int, avg_tokens_per_message: int = 20\n    ) -&gt; pl.DataFrame:\n        \"\"\"Create a realistic test dataset with variable message lengths.\"\"\"\n        import random\n\n        # Common words for realistic n-gram generation\n        words = [\n            \"the\",\n            \"and\",\n            \"is\",\n            \"in\",\n            \"to\",\n            \"of\",\n            \"a\",\n            \"for\",\n            \"on\",\n            \"with\",\n            \"as\",\n            \"by\",\n            \"be\",\n            \"at\",\n            \"this\",\n            \"that\",\n            \"from\",\n            \"they\",\n            \"we\",\n            \"you\",\n            \"have\",\n            \"has\",\n            \"had\",\n            \"will\",\n            \"would\",\n            \"could\",\n            \"should\",\n            \"can\",\n            \"may\",\n            \"data\",\n            \"analysis\",\n            \"social\",\n            \"media\",\n            \"content\",\n            \"user\",\n            \"post\",\n            \"comment\",\n            \"hashtag\",\n            \"trend\",\n            \"viral\",\n            \"engagement\",\n            \"reach\",\n            \"impression\",\n            \"click\",\n            \"like\",\n            \"share\",\n            \"retweet\",\n            \"follow\",\n            \"followers\",\n            \"following\",\n            \"account\",\n        ]\n\n        messages = []\n        for i in range(num_messages):\n            # Variable message length (10-40 tokens)\n            num_tokens = random.randint(\n                max(5, avg_tokens_per_message - 10), avg_tokens_per_message + 20\n            )\n\n            # Generate message with realistic word distribution\n            message_words = []\n            for _ in range(num_tokens):\n                # Higher probability for common words\n                if random.random() &lt; 0.3:\n                    word = random.choice(words[:10])  # Very common words\n                elif random.random() &lt; 0.6:\n                    word = random.choice(words[:30])  # Common words\n                else:\n                    word = random.choice(words)  # All words\n\n                message_words.append(word)\n\n            messages.append(\n                {\n                    \"message_id\": f\"msg_{i:06d}\",\n                    \"message_text\": \" \".join(message_words),\n                    \"author_id\": f\"user_{i % (num_messages // 10)}\",  # 10% unique users\n                    \"timestamp\": f\"2023-01-{(i % 31) + 1:02d}T{(i % 24):02d}:00:00Z\",\n                }\n            )\n\n        return pl.DataFrame(messages)\n\n    def _process_chunks_old(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate old chunk processing approach.\"\"\"\n        num_chunks = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate processing work (tokenization, basic operations)\n            _ = chunk.select(\n                [\n                    pl.col(\"message_text\").str.split(\" \").alias(\"tokens\"),\n                    pl.col(\"message_id\"),\n                    pl.col(\"author_id\"),\n                ]\n            )\n\n            num_chunks += 1\n\n            # Simulate memory cleanup every few chunks\n            if num_chunks % 5 == 0:\n                gc.collect()\n\n        return num_chunks\n\n    def _process_chunks_new(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate new optimized chunk processing approach.\"\"\"\n        num_chunks = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate processing work (tokenization, basic operations)\n            _ = chunk.select(\n                [\n                    pl.col(\"message_text\").str.split(\" \").alias(\"tokens\"),\n                    pl.col(\"message_id\"),\n                    pl.col(\"author_id\"),\n                ]\n            )\n\n            num_chunks += 1\n\n            # Optimized memory cleanup - less frequent\n            if num_chunks % 10 == 0:\n                gc.collect()\n\n        return num_chunks\n\n    # Phase 2: pytest-benchmark Integration\n\n    def test_chunk_processing_benchmark_small(self, benchmark):\n        \"\"\"Benchmark chunk processing performance on small datasets.\"\"\"\n        dataset = self._create_realistic_dataset(100_000, avg_tokens_per_message=15)\n\n        # Benchmark the new optimized approach\n        result = benchmark(self._process_chunks_new, dataset, 200_000)\n\n        # The benchmark fixture handles statistical analysis automatically\n        # We can still do basic validation\n        assert result &gt; 0, \"Should process at least one chunk\"\n\n    def test_chunk_processing_benchmark_medium(self, benchmark):\n        \"\"\"Benchmark chunk processing performance on medium datasets.\"\"\"\n        dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n        # Benchmark the new optimized approach\n        result = benchmark(self._process_chunks_new, dataset, 150_000)\n\n        assert result &gt; 0, \"Should process at least one chunk\"\n\n    def test_chunk_processing_benchmark_comparison(self):\n        \"\"\"Compare old vs new chunk processing approaches using pytest-benchmark.\"\"\"\n        dataset = self._create_realistic_dataset(300_000, avg_tokens_per_message=16)\n\n        # This test demonstrates how to use benchmark.pedantic for more control\n        # We'll implement this as a property-based test instead\n\n    # Phase 3: Resource-Based Metrics (Deterministic)\n\n    def test_chunk_efficiency_invariant(self):\n        \"\"\"Test that larger chunks always result in fewer I/O operations.\"\"\"\n        dataset = self._create_realistic_dataset(1_000_000, avg_tokens_per_message=20)\n\n        old_chunk_size = 50_000  # ~20 chunks\n        new_chunk_size = 150_000  # ~7 chunks\n\n        old_chunks = self._count_operations(dataset, old_chunk_size)\n        new_chunks = self._count_operations(dataset, new_chunk_size)\n\n        # These assertions will ALWAYS pass regardless of system performance\n        assert (\n            new_chunks &lt; old_chunks\n        ), f\"New chunks ({new_chunks}) should be fewer than old chunks ({old_chunks})\"\n\n        expected_reduction = old_chunks / new_chunks if new_chunks &gt; 0 else old_chunks\n        assert (\n            expected_reduction &gt;= 2.5\n        ), f\"Expected at least 2.5x I/O reduction, got {expected_reduction:.2f}x\"\n\n    def test_memory_efficiency_bounds(self):\n        \"\"\"Validate memory usage stays within acceptable limits.\"\"\"\n        process = psutil.Process()\n\n        initial_memory = process.memory_info().rss\n        dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n        # Process with new chunk size\n        self._process_chunks_new(dataset, 150_000)\n\n        peak_memory = process.memory_info().rss\n        memory_increase = (peak_memory - initial_memory) / 1024**2  # MB\n\n        # Reasonable memory bounds based on dataset size\n        assert (\n            memory_increase &lt; 500\n        ), f\"Memory usage increased by {memory_increase:.1f}MB, should be &lt; 500MB\"\n\n    @pytest.mark.parametrize(\"dataset_size\", [100_000, 500_000, 1_000_000])\n    @pytest.mark.parametrize(\"chunk_factor\", [2, 3, 4])\n    def test_chunk_size_scaling_properties(self, dataset_size, chunk_factor):\n        \"\"\"Test that chunk size scaling behaves predictably.\"\"\"\n        dataset = self._create_realistic_dataset(\n            dataset_size, avg_tokens_per_message=16\n        )\n\n        small_chunk = 50_000\n        large_chunk = small_chunk * chunk_factor\n\n        small_ops = self._count_operations(dataset, small_chunk)\n        large_ops = self._count_operations(dataset, large_chunk)\n\n        # Mathematical relationship should always hold\n        expected_reduction = min(chunk_factor, dataset_size / small_chunk)\n        actual_reduction = small_ops / large_ops if large_ops &gt; 0 else small_ops\n\n        # Allow 20% tolerance for edge cases\n        assert actual_reduction &gt;= expected_reduction * 0.8, (\n            f\"Expected ~{expected_reduction:.1f}x reduction, got {actual_reduction:.2f}x \"\n            f\"(dataset_size={dataset_size}, chunk_factor={chunk_factor})\"\n        )\n\n    def test_io_operation_counting_deterministic(self):\n        \"\"\"Test I/O operation counting produces deterministic results.\"\"\"\n        dataset = self._create_realistic_dataset(750_000, avg_tokens_per_message=15)\n\n        # Multiple runs should produce identical chunk counts\n        chunk_size = 125_000\n\n        run1 = self._count_operations(dataset, chunk_size)\n        run2 = self._count_operations(dataset, chunk_size)\n        run3 = self._count_operations(dataset, chunk_size)\n\n        assert run1 == run2 == run3, \"Chunk counting should be deterministic\"\n\n        # Verify mathematical correctness\n        expected_chunks = (len(dataset) + chunk_size - 1) // chunk_size\n        assert run1 == expected_chunks, f\"Expected {expected_chunks} chunks, got {run1}\"\n\n    def test_memory_usage_scaling_properties(self):\n        \"\"\"Test memory usage scaling properties with different dataset sizes.\"\"\"\n        dataset_sizes = [100_000, 200_000, 400_000]\n        memory_usages = []\n\n        process = psutil.Process()\n\n        for size in dataset_sizes:\n            gc.collect()  # Clean slate\n            initial_memory = process.memory_info().rss\n\n            dataset = self._create_realistic_dataset(size, avg_tokens_per_message=15)\n            self._process_chunks_new(dataset, 150_000)\n\n            peak_memory = process.memory_info().rss\n            memory_increase = (peak_memory - initial_memory) / 1024**2  # MB\n            memory_usages.append(memory_increase)\n\n            # Clean up\n            del dataset\n            gc.collect()\n\n        # Memory usage should scale reasonably with dataset size\n        for i in range(1, len(memory_usages)):\n            size_ratio = dataset_sizes[i] / dataset_sizes[i - 1]\n            memory_ratio = (\n                memory_usages[i] / memory_usages[i - 1]\n                if memory_usages[i - 1] &gt; 0\n                else 1\n            )\n\n            # Memory should not scale worse than linearly with dataset size\n            assert (\n                memory_ratio &lt;= size_ratio * 1.5\n            ), f\"Memory scaling too aggressive: {memory_ratio:.2f}x for {size_ratio:.2f}x data increase\"\n\n    # Phase 4: Enhanced Infrastructure Tests\n\n    def test_chunk_processing_variance_analysis(self):\n        \"\"\"Analyze variance in chunk processing to validate benchmark reliability.\"\"\"\n        dataset = self._create_realistic_dataset(200_000, avg_tokens_per_message=16)\n        chunk_size = 100_000\n\n        # Measure multiple runs\n        times = []\n        for _ in range(5):\n            gc.collect()\n            start_time = time.time()\n            chunks = self._process_chunks_new(dataset, chunk_size)\n            elapsed = time.time() - start_time\n            times.append(elapsed)\n\n        # Calculate coefficient of variation (CV)\n        mean_time = sum(times) / len(times)\n        variance = sum((t - mean_time) ** 2 for t in times) / len(times)\n        std_dev = variance**0.5\n        cv = std_dev / mean_time if mean_time &gt; 0 else 0\n\n        # Coefficient of variation should be reasonable (&lt; 30%)\n        assert cv &lt; 0.3, f\"High variance in processing times: CV = {cv:.2%}\"\n\n        # All runs should produce the same number of chunks\n        chunk_counts = []\n        for _ in range(3):\n            chunks = self._count_operations(dataset, chunk_size)\n            chunk_counts.append(chunks)\n\n        assert len(set(chunk_counts)) == 1, \"Chunk counts should be deterministic\"\n\n    def test_performance_regression_detection(self):\n        \"\"\"Test framework for detecting performance regressions.\"\"\"\n        dataset = self._create_realistic_dataset(400_000, avg_tokens_per_message=17)\n\n        # Baseline performance (optimized)\n        baseline_time = self._time_operation(\n            lambda: self._process_chunks_new(dataset, 150_000)\n        )\n\n        # Simulated regression (using old, slower approach)\n        regression_time = self._time_operation(\n            lambda: self._process_chunks_old(dataset, 50_000)\n        )\n\n        # Should detect significant regression\n        regression_ratio = regression_time / baseline_time if baseline_time &gt; 0 else 1\n\n        # This would fail if we had a real regression &gt; 50%\n        # In test, we expect the old approach to be slower\n        assert (\n            regression_ratio &gt; 1.0\n        ), \"Should detect performance difference between approaches\"\n\n    # Helper Methods\n\n    def _count_operations(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Count I/O operations (chunks) for deterministic testing.\"\"\"\n        dataset_size = len(dataset)\n        return (dataset_size + chunk_size - 1) // chunk_size\n\n    def _time_operation(self, operation) -&gt; float:\n        \"\"\"Time an operation with proper setup/cleanup.\"\"\"\n        gc.collect()\n        start_time = time.time()\n        operation()\n        return time.time() - start_time\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.setup_method","title":"<code>setup_method()</code>","text":"<p>Set up test environment before each test.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def setup_method(self):\n    \"\"\"Set up test environment before each test.\"\"\"\n    # Force garbage collection to start with clean state\n    gc.collect()\n\n    # Get baseline memory usage\n    self.initial_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.teardown_method","title":"<code>teardown_method()</code>","text":"<p>Clean up after each test.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def teardown_method(self):\n    \"\"\"Clean up after each test.\"\"\"\n    gc.collect()\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_efficiency_invariant","title":"<code>test_chunk_efficiency_invariant()</code>","text":"<p>Test that larger chunks always result in fewer I/O operations.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_chunk_efficiency_invariant(self):\n    \"\"\"Test that larger chunks always result in fewer I/O operations.\"\"\"\n    dataset = self._create_realistic_dataset(1_000_000, avg_tokens_per_message=20)\n\n    old_chunk_size = 50_000  # ~20 chunks\n    new_chunk_size = 150_000  # ~7 chunks\n\n    old_chunks = self._count_operations(dataset, old_chunk_size)\n    new_chunks = self._count_operations(dataset, new_chunk_size)\n\n    # These assertions will ALWAYS pass regardless of system performance\n    assert (\n        new_chunks &lt; old_chunks\n    ), f\"New chunks ({new_chunks}) should be fewer than old chunks ({old_chunks})\"\n\n    expected_reduction = old_chunks / new_chunks if new_chunks &gt; 0 else old_chunks\n    assert (\n        expected_reduction &gt;= 2.5\n    ), f\"Expected at least 2.5x I/O reduction, got {expected_reduction:.2f}x\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_processing_benchmark_comparison","title":"<code>test_chunk_processing_benchmark_comparison()</code>","text":"<p>Compare old vs new chunk processing approaches using pytest-benchmark.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_chunk_processing_benchmark_comparison(self):\n    \"\"\"Compare old vs new chunk processing approaches using pytest-benchmark.\"\"\"\n    dataset = self._create_realistic_dataset(300_000, avg_tokens_per_message=16)\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_processing_benchmark_medium","title":"<code>test_chunk_processing_benchmark_medium(benchmark)</code>","text":"<p>Benchmark chunk processing performance on medium datasets.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_chunk_processing_benchmark_medium(self, benchmark):\n    \"\"\"Benchmark chunk processing performance on medium datasets.\"\"\"\n    dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n    # Benchmark the new optimized approach\n    result = benchmark(self._process_chunks_new, dataset, 150_000)\n\n    assert result &gt; 0, \"Should process at least one chunk\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_processing_benchmark_small","title":"<code>test_chunk_processing_benchmark_small(benchmark)</code>","text":"<p>Benchmark chunk processing performance on small datasets.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_chunk_processing_benchmark_small(self, benchmark):\n    \"\"\"Benchmark chunk processing performance on small datasets.\"\"\"\n    dataset = self._create_realistic_dataset(100_000, avg_tokens_per_message=15)\n\n    # Benchmark the new optimized approach\n    result = benchmark(self._process_chunks_new, dataset, 200_000)\n\n    # The benchmark fixture handles statistical analysis automatically\n    # We can still do basic validation\n    assert result &gt; 0, \"Should process at least one chunk\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_processing_variance_analysis","title":"<code>test_chunk_processing_variance_analysis()</code>","text":"<p>Analyze variance in chunk processing to validate benchmark reliability.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_chunk_processing_variance_analysis(self):\n    \"\"\"Analyze variance in chunk processing to validate benchmark reliability.\"\"\"\n    dataset = self._create_realistic_dataset(200_000, avg_tokens_per_message=16)\n    chunk_size = 100_000\n\n    # Measure multiple runs\n    times = []\n    for _ in range(5):\n        gc.collect()\n        start_time = time.time()\n        chunks = self._process_chunks_new(dataset, chunk_size)\n        elapsed = time.time() - start_time\n        times.append(elapsed)\n\n    # Calculate coefficient of variation (CV)\n    mean_time = sum(times) / len(times)\n    variance = sum((t - mean_time) ** 2 for t in times) / len(times)\n    std_dev = variance**0.5\n    cv = std_dev / mean_time if mean_time &gt; 0 else 0\n\n    # Coefficient of variation should be reasonable (&lt; 30%)\n    assert cv &lt; 0.3, f\"High variance in processing times: CV = {cv:.2%}\"\n\n    # All runs should produce the same number of chunks\n    chunk_counts = []\n    for _ in range(3):\n        chunks = self._count_operations(dataset, chunk_size)\n        chunk_counts.append(chunks)\n\n    assert len(set(chunk_counts)) == 1, \"Chunk counts should be deterministic\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_chunk_size_scaling_properties","title":"<code>test_chunk_size_scaling_properties(dataset_size, chunk_factor)</code>","text":"<p>Test that chunk size scaling behaves predictably.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>@pytest.mark.parametrize(\"dataset_size\", [100_000, 500_000, 1_000_000])\n@pytest.mark.parametrize(\"chunk_factor\", [2, 3, 4])\ndef test_chunk_size_scaling_properties(self, dataset_size, chunk_factor):\n    \"\"\"Test that chunk size scaling behaves predictably.\"\"\"\n    dataset = self._create_realistic_dataset(\n        dataset_size, avg_tokens_per_message=16\n    )\n\n    small_chunk = 50_000\n    large_chunk = small_chunk * chunk_factor\n\n    small_ops = self._count_operations(dataset, small_chunk)\n    large_ops = self._count_operations(dataset, large_chunk)\n\n    # Mathematical relationship should always hold\n    expected_reduction = min(chunk_factor, dataset_size / small_chunk)\n    actual_reduction = small_ops / large_ops if large_ops &gt; 0 else small_ops\n\n    # Allow 20% tolerance for edge cases\n    assert actual_reduction &gt;= expected_reduction * 0.8, (\n        f\"Expected ~{expected_reduction:.1f}x reduction, got {actual_reduction:.2f}x \"\n        f\"(dataset_size={dataset_size}, chunk_factor={chunk_factor})\"\n    )\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_io_operation_counting_deterministic","title":"<code>test_io_operation_counting_deterministic()</code>","text":"<p>Test I/O operation counting produces deterministic results.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_io_operation_counting_deterministic(self):\n    \"\"\"Test I/O operation counting produces deterministic results.\"\"\"\n    dataset = self._create_realistic_dataset(750_000, avg_tokens_per_message=15)\n\n    # Multiple runs should produce identical chunk counts\n    chunk_size = 125_000\n\n    run1 = self._count_operations(dataset, chunk_size)\n    run2 = self._count_operations(dataset, chunk_size)\n    run3 = self._count_operations(dataset, chunk_size)\n\n    assert run1 == run2 == run3, \"Chunk counting should be deterministic\"\n\n    # Verify mathematical correctness\n    expected_chunks = (len(dataset) + chunk_size - 1) // chunk_size\n    assert run1 == expected_chunks, f\"Expected {expected_chunks} chunks, got {run1}\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_memory_efficiency_bounds","title":"<code>test_memory_efficiency_bounds()</code>","text":"<p>Validate memory usage stays within acceptable limits.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_memory_efficiency_bounds(self):\n    \"\"\"Validate memory usage stays within acceptable limits.\"\"\"\n    process = psutil.Process()\n\n    initial_memory = process.memory_info().rss\n    dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n    # Process with new chunk size\n    self._process_chunks_new(dataset, 150_000)\n\n    peak_memory = process.memory_info().rss\n    memory_increase = (peak_memory - initial_memory) / 1024**2  # MB\n\n    # Reasonable memory bounds based on dataset size\n    assert (\n        memory_increase &lt; 500\n    ), f\"Memory usage increased by {memory_increase:.1f}MB, should be &lt; 500MB\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_memory_usage_scaling_properties","title":"<code>test_memory_usage_scaling_properties()</code>","text":"<p>Test memory usage scaling properties with different dataset sizes.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_memory_usage_scaling_properties(self):\n    \"\"\"Test memory usage scaling properties with different dataset sizes.\"\"\"\n    dataset_sizes = [100_000, 200_000, 400_000]\n    memory_usages = []\n\n    process = psutil.Process()\n\n    for size in dataset_sizes:\n        gc.collect()  # Clean slate\n        initial_memory = process.memory_info().rss\n\n        dataset = self._create_realistic_dataset(size, avg_tokens_per_message=15)\n        self._process_chunks_new(dataset, 150_000)\n\n        peak_memory = process.memory_info().rss\n        memory_increase = (peak_memory - initial_memory) / 1024**2  # MB\n        memory_usages.append(memory_increase)\n\n        # Clean up\n        del dataset\n        gc.collect()\n\n    # Memory usage should scale reasonably with dataset size\n    for i in range(1, len(memory_usages)):\n        size_ratio = dataset_sizes[i] / dataset_sizes[i - 1]\n        memory_ratio = (\n            memory_usages[i] / memory_usages[i - 1]\n            if memory_usages[i - 1] &gt; 0\n            else 1\n        )\n\n        # Memory should not scale worse than linearly with dataset size\n        assert (\n            memory_ratio &lt;= size_ratio * 1.5\n        ), f\"Memory scaling too aggressive: {memory_ratio:.2f}x for {size_ratio:.2f}x data increase\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_enhanced_benchmarks.TestEnhancedPerformanceBenchmarks.test_performance_regression_detection","title":"<code>test_performance_regression_detection()</code>","text":"<p>Test framework for detecting performance regressions.</p> Source code in <code>testing/performance/test_enhanced_benchmarks.py</code> <pre><code>def test_performance_regression_detection(self):\n    \"\"\"Test framework for detecting performance regressions.\"\"\"\n    dataset = self._create_realistic_dataset(400_000, avg_tokens_per_message=17)\n\n    # Baseline performance (optimized)\n    baseline_time = self._time_operation(\n        lambda: self._process_chunks_new(dataset, 150_000)\n    )\n\n    # Simulated regression (using old, slower approach)\n    regression_time = self._time_operation(\n        lambda: self._process_chunks_old(dataset, 50_000)\n    )\n\n    # Should detect significant regression\n    regression_ratio = regression_time / baseline_time if baseline_time &gt; 0 else 1\n\n    # This would fail if we had a real regression &gt; 50%\n    # In test, we expect the old approach to be slower\n    assert (\n        regression_ratio &gt; 1.0\n    ), \"Should detect performance difference between approaches\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation","title":"<code>test_integration_validation</code>","text":"<p>Integration Validation Tests for Chunking Optimization</p> <p>Tests that validate the complete chunking optimization implementation works end-to-end and meets the performance targets specified in the optimization spec.</p>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration","title":"<code>TestChunkingOptimizationIntegration</code>","text":"<p>Integration tests for complete chunking optimization.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>class TestChunkingOptimizationIntegration:\n    \"\"\"Integration tests for complete chunking optimization.\"\"\"\n\n    def test_memory_manager_auto_detection_integration(self):\n        \"\"\"Test that MemoryManager auto-detection works end-to-end.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test 16GB system detection\n            mock_vm.return_value.total = 16 * 1024**3\n\n            # Should auto-detect 4.8GB (30% of 16GB)\n            manager = MemoryManager()\n            assert abs(manager.max_memory_gb - 4.8) &lt; 0.1\n\n            # Should log the auto-detection\n            assert manager.max_memory_bytes == manager.max_memory_gb * 1024**3\n\n    def test_adaptive_chunk_calculation_integration(self):\n        \"\"\"Test that adaptive chunk calculation integrates with memory detection.\"\"\"\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Test different system configurations\n            test_systems = [\n                (8 * 1024**3, 1.0),  # 8GB system, 1.0x factor\n                (16 * 1024**3, 1.5),  # 16GB system, 1.5x factor\n                (32 * 1024**3, 2.0),  # 32GB system, 2.0x factor\n            ]\n\n            for total_memory, expected_factor in test_systems:\n                mock_vm.return_value.total = total_memory\n\n                manager = MemoryManager()\n\n                # Test chunk size calculation with the memory manager\n                base_chunk = 100_000\n                adaptive_chunk = manager.calculate_adaptive_chunk_size(\n                    base_chunk, \"ngram_generation\"\n                )\n\n                # Should be within reasonable bounds\n                assert adaptive_chunk &gt; 0\n                assert (\n                    adaptive_chunk &gt;= base_chunk * 0.3\n                )  # Allow for pressure reduction\n\n                # For low pressure, should be at or below base chunk\n                with patch.object(manager.process, \"memory_info\") as mock_memory:\n                    # Simulate low memory usage (50% of max) for LOW pressure\n                    mock_memory.return_value.rss = int(0.5 * manager.max_memory_bytes)\n\n                    low_pressure_chunk = manager.calculate_adaptive_chunk_size(\n                        base_chunk, \"ngram_generation\"\n                    )\n\n                    # Should use operation-specific adjustment\n                    # N-gram generation typically gets reduced chunk size\n                    assert low_pressure_chunk &lt;= base_chunk\n\n    def test_chunking_optimization_phase_integration(self):\n        \"\"\"Test that all optimization phases work together correctly.\"\"\"\n        # Test the complete integration of all phases\n\n        # Phase 1: Memory auto-detection\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 16 * 1024**3\n            manager = MemoryManager()\n\n            # Should detect 16GB system and allocate 4.8GB\n            assert abs(manager.max_memory_gb - 4.8) &lt; 0.1\n\n            # Phase 2: Adaptive chunking should use memory manager\n            # Simulate the calculate_optimal_chunk_size function from ngrams_base\n            def calculate_optimal_chunk_size(\n                dataset_size: int, memory_manager=None\n            ) -&gt; int:\n                if memory_manager:\n                    total_gb = psutil.virtual_memory().total / 1024**3\n                    if total_gb &gt;= 32:\n                        memory_factor = 2.0\n                    elif total_gb &gt;= 16:\n                        memory_factor = 1.5\n                    elif total_gb &gt;= 8:\n                        memory_factor = 1.0\n                    else:\n                        memory_factor = 0.5\n                else:\n                    memory_factor = 1.0\n\n                if dataset_size &lt;= 500_000:\n                    base_chunk = int(200_000 * memory_factor)\n                elif dataset_size &lt;= 2_000_000:\n                    base_chunk = int(150_000 * memory_factor)\n                else:\n                    base_chunk = int(100_000 * memory_factor)\n\n                return max(10_000, min(base_chunk, 500_000))\n\n            # Test medium dataset on 16GB system\n            chunk_size = calculate_optimal_chunk_size(1_000_000, manager)\n            assert chunk_size == 225_000  # 150K * 1.5\n\n            # Phase 3: Fallback thresholds should be memory-aware\n            total_gb = psutil.virtual_memory().total / 1024**3\n            if total_gb &gt;= 32:\n                fallback_threshold = 3_000_000\n            elif total_gb &gt;= 16:\n                fallback_threshold = 1_500_000\n            else:\n                fallback_threshold = 500_000\n\n            # 16GB system should get 1.5M threshold\n            assert fallback_threshold == 1_500_000\n\n            # Phase 4: Secondary analyzer chunks should be larger\n            def calculate_ngram_stats_chunk(\n                message_ngram_count: int, ngram_count: int\n            ) -&gt; int:\n                base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n                return max(5_000, min(50_000, base_calc))\n\n            # Should use new larger bounds\n            stats_chunk = calculate_ngram_stats_chunk(100_000, 10_000)\n            assert stats_chunk &gt;= 5_000  # New minimum\n            assert stats_chunk &lt;= 50_000  # New maximum\n\n    def test_performance_improvements_validation(self):\n        \"\"\"Test that performance improvements are measurable.\"\"\"\n        # Create test datasets to measure performance differences\n        small_dataset = self._create_test_dataset(50_000)\n        medium_dataset = self._create_test_dataset(200_000)\n\n        # Test old vs new chunk processing using medium dataset for meaningful comparison\n        old_chunk_size = 50_000  # Original base\n        new_chunk_size = 150_000  # New base\n\n        # Measure old approach\n        start_time = time.time()\n        old_chunks = self._simulate_processing(medium_dataset, old_chunk_size)\n        old_time = time.time() - start_time\n\n        # Measure new approach\n        start_time = time.time()\n        new_chunks = self._simulate_processing(medium_dataset, new_chunk_size)\n        new_time = time.time() - start_time\n\n        # Should have fewer chunks (better I/O efficiency)\n        assert new_chunks &lt; old_chunks\n\n        # Should be faster (allowing for test variability)\n        if new_time &gt; 0:\n            improvement = old_time / new_time\n            assert improvement &gt;= 1.0  # At least no regression\n\n        # Test chunk count reduction\n        expected_reduction = old_chunk_size / new_chunk_size\n        if expected_reduction &gt; 1:\n            actual_reduction = old_chunks / new_chunks if new_chunks &gt; 0 else 1\n            assert actual_reduction &gt;= expected_reduction * 0.8  # Allow 20% tolerance\n\n    def test_memory_bounds_validation(self):\n        \"\"\"Test that memory usage stays within auto-detected bounds.\"\"\"\n        manager = MemoryManager()\n\n        # Get initial memory usage\n        initial_memory = manager.get_current_memory_usage()\n        initial_rss_gb = initial_memory[\"rss_gb\"]\n\n        # Should be well below the limit initially\n        assert initial_rss_gb &lt; manager.max_memory_gb\n\n        # Simulate memory usage with adaptive chunk sizing\n        base_chunk = 100_000\n        for operation in [\"tokenization\", \"ngram_generation\", \"unique_extraction\"]:\n            adaptive_chunk = manager.calculate_adaptive_chunk_size(\n                base_chunk, operation\n            )\n\n            # Should be positive and reasonable\n            assert adaptive_chunk &gt; 0\n            assert (\n                adaptive_chunk &lt;= base_chunk * 2\n            )  # Allow some scaling up for certain operations\n\n        # Memory should still be reasonable\n        current_memory = manager.get_current_memory_usage()\n        current_rss_gb = current_memory[\"rss_gb\"]\n\n        # Should not have exceeded reasonable bounds\n        assert (\n            current_rss_gb &lt;= manager.max_memory_gb * 1.5\n        )  # 50% tolerance for test overhead\n\n    def test_backward_compatibility_validation(self):\n        \"\"\"Test that backward compatibility is preserved.\"\"\"\n        # Manual override should still work exactly as before\n        manual_manager = MemoryManager(max_memory_gb=2.0)\n        assert manual_manager.max_memory_gb == 2.0\n\n        # All existing API methods should still work\n        assert hasattr(manual_manager, \"get_current_memory_usage\")\n        assert hasattr(manual_manager, \"get_memory_pressure_level\")\n        assert hasattr(manual_manager, \"calculate_adaptive_chunk_size\")\n        assert hasattr(manual_manager, \"enhanced_gc_cleanup\")\n\n        # Methods should return expected types\n        usage = manual_manager.get_current_memory_usage()\n        assert isinstance(usage, dict)\n\n        pressure = manual_manager.get_memory_pressure_level()\n        assert hasattr(pressure, \"name\")  # Should be an enum\n\n        chunk_size = manual_manager.calculate_adaptive_chunk_size(10000, \"tokenization\")\n        assert isinstance(chunk_size, int)\n        assert chunk_size &gt; 0\n\n    def test_system_specific_optimization_validation(self):\n        \"\"\"Test that optimizations are appropriate for different system types.\"\"\"\n        test_systems = [\n            (4 * 1024**3, \"constrained\", 0.8, 0.5),  # 4GB: 20% allocation, 0.5x chunks\n            (8 * 1024**3, \"lower\", 2.0, 1.0),  # 8GB: 25% allocation, 1.0x chunks\n            (16 * 1024**3, \"standard\", 4.8, 1.5),  # 16GB: 30% allocation, 1.5x chunks\n            (32 * 1024**3, \"high\", 12.8, 2.0),  # 32GB: 40% allocation, 2.0x chunks\n        ]\n\n        for total_memory, system_type, expected_limit, expected_factor in test_systems:\n            with patch(\"psutil.virtual_memory\") as mock_vm:\n                mock_vm.return_value.total = total_memory\n\n                manager = MemoryManager()\n\n                # Should detect appropriate memory limit\n                assert (\n                    abs(manager.max_memory_gb - expected_limit) &lt; 0.1\n                ), f\"{system_type} system should allocate {expected_limit}GB\"\n\n                # Should use appropriate chunk scaling\n                total_gb = total_memory / 1024**3\n                if total_gb &gt;= 32:\n                    chunk_factor = 2.0\n                elif total_gb &gt;= 16:\n                    chunk_factor = 1.5\n                elif total_gb &gt;= 8:\n                    chunk_factor = 1.0\n                else:\n                    chunk_factor = 0.5\n\n                assert (\n                    abs(chunk_factor - expected_factor) &lt; 0.1\n                ), f\"{system_type} system should use {expected_factor}x chunk factor\"\n\n    def test_error_handling_integration(self):\n        \"\"\"Test that error handling works correctly in integration scenarios.\"\"\"\n        # Test with very low memory limit\n        constrained_manager = MemoryManager(max_memory_gb=0.1)  # 100MB\n\n        # Should still provide reasonable chunk sizes\n        chunk_size = constrained_manager.calculate_adaptive_chunk_size(\n            10000, \"tokenization\"\n        )\n        assert chunk_size &gt; 0\n        assert chunk_size &gt;= 1000  # Should enforce some minimum\n\n        # Test with extreme memory pressure\n        with patch.object(constrained_manager.process, \"memory_info\") as mock_memory:\n            # Simulate critical memory usage (95% of max)\n            mock_memory.return_value.rss = int(\n                0.95 * constrained_manager.max_memory_bytes\n            )\n\n            critical_chunk = constrained_manager.calculate_adaptive_chunk_size(\n                100000, \"ngram_generation\"\n            )\n\n            # Should drastically reduce chunk size but still be usable\n            assert critical_chunk &gt; 0\n            assert critical_chunk &lt; 100000 * 0.5  # Should be significantly reduced\n\n    def _create_test_dataset(self, size: int) -&gt; pl.DataFrame:\n        \"\"\"Create a test dataset for benchmarking.\"\"\"\n        return pl.DataFrame(\n            {\n                \"message_id\": [f\"msg_{i}\" for i in range(size)],\n                \"message_text\": [\n                    f\"test message {i} with some content\" for i in range(size)\n                ],\n                \"author_id\": [f\"user_{i % 100}\" for i in range(size)],\n                \"timestamp\": [\"2023-01-01T00:00:00Z\"] * size,\n            }\n        )\n\n    def _simulate_processing(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate chunk processing and return number of chunks.\"\"\"\n        num_chunks = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate some processing work\n            _ = chunk.select(\n                [\n                    pl.col(\"message_text\").str.len_chars().alias(\"length\"),\n                    pl.col(\"message_id\"),\n                ]\n            )\n\n            num_chunks += 1\n\n        return num_chunks\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_adaptive_chunk_calculation_integration","title":"<code>test_adaptive_chunk_calculation_integration()</code>","text":"<p>Test that adaptive chunk calculation integrates with memory detection.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_adaptive_chunk_calculation_integration(self):\n    \"\"\"Test that adaptive chunk calculation integrates with memory detection.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test different system configurations\n        test_systems = [\n            (8 * 1024**3, 1.0),  # 8GB system, 1.0x factor\n            (16 * 1024**3, 1.5),  # 16GB system, 1.5x factor\n            (32 * 1024**3, 2.0),  # 32GB system, 2.0x factor\n        ]\n\n        for total_memory, expected_factor in test_systems:\n            mock_vm.return_value.total = total_memory\n\n            manager = MemoryManager()\n\n            # Test chunk size calculation with the memory manager\n            base_chunk = 100_000\n            adaptive_chunk = manager.calculate_adaptive_chunk_size(\n                base_chunk, \"ngram_generation\"\n            )\n\n            # Should be within reasonable bounds\n            assert adaptive_chunk &gt; 0\n            assert (\n                adaptive_chunk &gt;= base_chunk * 0.3\n            )  # Allow for pressure reduction\n\n            # For low pressure, should be at or below base chunk\n            with patch.object(manager.process, \"memory_info\") as mock_memory:\n                # Simulate low memory usage (50% of max) for LOW pressure\n                mock_memory.return_value.rss = int(0.5 * manager.max_memory_bytes)\n\n                low_pressure_chunk = manager.calculate_adaptive_chunk_size(\n                    base_chunk, \"ngram_generation\"\n                )\n\n                # Should use operation-specific adjustment\n                # N-gram generation typically gets reduced chunk size\n                assert low_pressure_chunk &lt;= base_chunk\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_backward_compatibility_validation","title":"<code>test_backward_compatibility_validation()</code>","text":"<p>Test that backward compatibility is preserved.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_backward_compatibility_validation(self):\n    \"\"\"Test that backward compatibility is preserved.\"\"\"\n    # Manual override should still work exactly as before\n    manual_manager = MemoryManager(max_memory_gb=2.0)\n    assert manual_manager.max_memory_gb == 2.0\n\n    # All existing API methods should still work\n    assert hasattr(manual_manager, \"get_current_memory_usage\")\n    assert hasattr(manual_manager, \"get_memory_pressure_level\")\n    assert hasattr(manual_manager, \"calculate_adaptive_chunk_size\")\n    assert hasattr(manual_manager, \"enhanced_gc_cleanup\")\n\n    # Methods should return expected types\n    usage = manual_manager.get_current_memory_usage()\n    assert isinstance(usage, dict)\n\n    pressure = manual_manager.get_memory_pressure_level()\n    assert hasattr(pressure, \"name\")  # Should be an enum\n\n    chunk_size = manual_manager.calculate_adaptive_chunk_size(10000, \"tokenization\")\n    assert isinstance(chunk_size, int)\n    assert chunk_size &gt; 0\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_chunking_optimization_phase_integration","title":"<code>test_chunking_optimization_phase_integration()</code>","text":"<p>Test that all optimization phases work together correctly.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_chunking_optimization_phase_integration(self):\n    \"\"\"Test that all optimization phases work together correctly.\"\"\"\n    # Test the complete integration of all phases\n\n    # Phase 1: Memory auto-detection\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 16 * 1024**3\n        manager = MemoryManager()\n\n        # Should detect 16GB system and allocate 4.8GB\n        assert abs(manager.max_memory_gb - 4.8) &lt; 0.1\n\n        # Phase 2: Adaptive chunking should use memory manager\n        # Simulate the calculate_optimal_chunk_size function from ngrams_base\n        def calculate_optimal_chunk_size(\n            dataset_size: int, memory_manager=None\n        ) -&gt; int:\n            if memory_manager:\n                total_gb = psutil.virtual_memory().total / 1024**3\n                if total_gb &gt;= 32:\n                    memory_factor = 2.0\n                elif total_gb &gt;= 16:\n                    memory_factor = 1.5\n                elif total_gb &gt;= 8:\n                    memory_factor = 1.0\n                else:\n                    memory_factor = 0.5\n            else:\n                memory_factor = 1.0\n\n            if dataset_size &lt;= 500_000:\n                base_chunk = int(200_000 * memory_factor)\n            elif dataset_size &lt;= 2_000_000:\n                base_chunk = int(150_000 * memory_factor)\n            else:\n                base_chunk = int(100_000 * memory_factor)\n\n            return max(10_000, min(base_chunk, 500_000))\n\n        # Test medium dataset on 16GB system\n        chunk_size = calculate_optimal_chunk_size(1_000_000, manager)\n        assert chunk_size == 225_000  # 150K * 1.5\n\n        # Phase 3: Fallback thresholds should be memory-aware\n        total_gb = psutil.virtual_memory().total / 1024**3\n        if total_gb &gt;= 32:\n            fallback_threshold = 3_000_000\n        elif total_gb &gt;= 16:\n            fallback_threshold = 1_500_000\n        else:\n            fallback_threshold = 500_000\n\n        # 16GB system should get 1.5M threshold\n        assert fallback_threshold == 1_500_000\n\n        # Phase 4: Secondary analyzer chunks should be larger\n        def calculate_ngram_stats_chunk(\n            message_ngram_count: int, ngram_count: int\n        ) -&gt; int:\n            base_calc = 500_000 // max(1, message_ngram_count // ngram_count)\n            return max(5_000, min(50_000, base_calc))\n\n        # Should use new larger bounds\n        stats_chunk = calculate_ngram_stats_chunk(100_000, 10_000)\n        assert stats_chunk &gt;= 5_000  # New minimum\n        assert stats_chunk &lt;= 50_000  # New maximum\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_error_handling_integration","title":"<code>test_error_handling_integration()</code>","text":"<p>Test that error handling works correctly in integration scenarios.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_error_handling_integration(self):\n    \"\"\"Test that error handling works correctly in integration scenarios.\"\"\"\n    # Test with very low memory limit\n    constrained_manager = MemoryManager(max_memory_gb=0.1)  # 100MB\n\n    # Should still provide reasonable chunk sizes\n    chunk_size = constrained_manager.calculate_adaptive_chunk_size(\n        10000, \"tokenization\"\n    )\n    assert chunk_size &gt; 0\n    assert chunk_size &gt;= 1000  # Should enforce some minimum\n\n    # Test with extreme memory pressure\n    with patch.object(constrained_manager.process, \"memory_info\") as mock_memory:\n        # Simulate critical memory usage (95% of max)\n        mock_memory.return_value.rss = int(\n            0.95 * constrained_manager.max_memory_bytes\n        )\n\n        critical_chunk = constrained_manager.calculate_adaptive_chunk_size(\n            100000, \"ngram_generation\"\n        )\n\n        # Should drastically reduce chunk size but still be usable\n        assert critical_chunk &gt; 0\n        assert critical_chunk &lt; 100000 * 0.5  # Should be significantly reduced\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_memory_bounds_validation","title":"<code>test_memory_bounds_validation()</code>","text":"<p>Test that memory usage stays within auto-detected bounds.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_memory_bounds_validation(self):\n    \"\"\"Test that memory usage stays within auto-detected bounds.\"\"\"\n    manager = MemoryManager()\n\n    # Get initial memory usage\n    initial_memory = manager.get_current_memory_usage()\n    initial_rss_gb = initial_memory[\"rss_gb\"]\n\n    # Should be well below the limit initially\n    assert initial_rss_gb &lt; manager.max_memory_gb\n\n    # Simulate memory usage with adaptive chunk sizing\n    base_chunk = 100_000\n    for operation in [\"tokenization\", \"ngram_generation\", \"unique_extraction\"]:\n        adaptive_chunk = manager.calculate_adaptive_chunk_size(\n            base_chunk, operation\n        )\n\n        # Should be positive and reasonable\n        assert adaptive_chunk &gt; 0\n        assert (\n            adaptive_chunk &lt;= base_chunk * 2\n        )  # Allow some scaling up for certain operations\n\n    # Memory should still be reasonable\n    current_memory = manager.get_current_memory_usage()\n    current_rss_gb = current_memory[\"rss_gb\"]\n\n    # Should not have exceeded reasonable bounds\n    assert (\n        current_rss_gb &lt;= manager.max_memory_gb * 1.5\n    )  # 50% tolerance for test overhead\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_memory_manager_auto_detection_integration","title":"<code>test_memory_manager_auto_detection_integration()</code>","text":"<p>Test that MemoryManager auto-detection works end-to-end.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_memory_manager_auto_detection_integration(self):\n    \"\"\"Test that MemoryManager auto-detection works end-to-end.\"\"\"\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        # Test 16GB system detection\n        mock_vm.return_value.total = 16 * 1024**3\n\n        # Should auto-detect 4.8GB (30% of 16GB)\n        manager = MemoryManager()\n        assert abs(manager.max_memory_gb - 4.8) &lt; 0.1\n\n        # Should log the auto-detection\n        assert manager.max_memory_bytes == manager.max_memory_gb * 1024**3\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_performance_improvements_validation","title":"<code>test_performance_improvements_validation()</code>","text":"<p>Test that performance improvements are measurable.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_performance_improvements_validation(self):\n    \"\"\"Test that performance improvements are measurable.\"\"\"\n    # Create test datasets to measure performance differences\n    small_dataset = self._create_test_dataset(50_000)\n    medium_dataset = self._create_test_dataset(200_000)\n\n    # Test old vs new chunk processing using medium dataset for meaningful comparison\n    old_chunk_size = 50_000  # Original base\n    new_chunk_size = 150_000  # New base\n\n    # Measure old approach\n    start_time = time.time()\n    old_chunks = self._simulate_processing(medium_dataset, old_chunk_size)\n    old_time = time.time() - start_time\n\n    # Measure new approach\n    start_time = time.time()\n    new_chunks = self._simulate_processing(medium_dataset, new_chunk_size)\n    new_time = time.time() - start_time\n\n    # Should have fewer chunks (better I/O efficiency)\n    assert new_chunks &lt; old_chunks\n\n    # Should be faster (allowing for test variability)\n    if new_time &gt; 0:\n        improvement = old_time / new_time\n        assert improvement &gt;= 1.0  # At least no regression\n\n    # Test chunk count reduction\n    expected_reduction = old_chunk_size / new_chunk_size\n    if expected_reduction &gt; 1:\n        actual_reduction = old_chunks / new_chunks if new_chunks &gt; 0 else 1\n        assert actual_reduction &gt;= expected_reduction * 0.8  # Allow 20% tolerance\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestChunkingOptimizationIntegration.test_system_specific_optimization_validation","title":"<code>test_system_specific_optimization_validation()</code>","text":"<p>Test that optimizations are appropriate for different system types.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_system_specific_optimization_validation(self):\n    \"\"\"Test that optimizations are appropriate for different system types.\"\"\"\n    test_systems = [\n        (4 * 1024**3, \"constrained\", 0.8, 0.5),  # 4GB: 20% allocation, 0.5x chunks\n        (8 * 1024**3, \"lower\", 2.0, 1.0),  # 8GB: 25% allocation, 1.0x chunks\n        (16 * 1024**3, \"standard\", 4.8, 1.5),  # 16GB: 30% allocation, 1.5x chunks\n        (32 * 1024**3, \"high\", 12.8, 2.0),  # 32GB: 40% allocation, 2.0x chunks\n    ]\n\n    for total_memory, system_type, expected_limit, expected_factor in test_systems:\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = total_memory\n\n            manager = MemoryManager()\n\n            # Should detect appropriate memory limit\n            assert (\n                abs(manager.max_memory_gb - expected_limit) &lt; 0.1\n            ), f\"{system_type} system should allocate {expected_limit}GB\"\n\n            # Should use appropriate chunk scaling\n            total_gb = total_memory / 1024**3\n            if total_gb &gt;= 32:\n                chunk_factor = 2.0\n            elif total_gb &gt;= 16:\n                chunk_factor = 1.5\n            elif total_gb &gt;= 8:\n                chunk_factor = 1.0\n            else:\n                chunk_factor = 0.5\n\n            assert (\n                abs(chunk_factor - expected_factor) &lt; 0.1\n            ), f\"{system_type} system should use {expected_factor}x chunk factor\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestRealWorldScenarios","title":"<code>TestRealWorldScenarios</code>","text":"<p>Test real-world scenarios with chunking optimization.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>class TestRealWorldScenarios:\n    \"\"\"Test real-world scenarios with chunking optimization.\"\"\"\n\n    def test_typical_social_media_dataset_scenario(self):\n        \"\"\"Test with a dataset that simulates typical social media analysis.\"\"\"\n        # Create realistic dataset\n        dataset = self._create_social_media_dataset(100_000)\n\n        # Test with auto-detected memory manager\n        manager = MemoryManager()\n\n        # Simulate n-gram analysis workflow\n        base_chunk_size = 50_000  # Old default\n\n        # Calculate adaptive chunk size\n        adaptive_chunk = manager.calculate_adaptive_chunk_size(\n            base_chunk_size, \"ngram_generation\"\n        )\n\n        # Should be reasonable for the dataset\n        assert adaptive_chunk &gt; 0\n        assert adaptive_chunk &lt;= base_chunk_size * 2  # Reasonable scaling\n\n        # Test processing with adaptive chunk size\n        start_time = time.time()\n        chunks_processed = self._simulate_ngram_processing(dataset, adaptive_chunk)\n        processing_time = time.time() - start_time\n\n        # Should complete in reasonable time\n        assert processing_time &lt; 30  # Should be fast for test dataset\n        assert chunks_processed &gt; 0\n\n        # Memory usage should be reasonable\n        memory_stats = manager.get_current_memory_usage()\n        assert memory_stats[\"rss_gb\"] &lt;= manager.max_memory_gb * 1.2  # 20% tolerance\n\n    def test_large_dataset_fallback_scenario(self):\n        \"\"\"Test fallback behavior with large datasets.\"\"\"\n        # Test the fallback threshold logic\n        manager = MemoryManager()\n\n        # Determine fallback threshold based on system memory\n        system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n        if system_memory_gb &gt;= 32:\n            expected_threshold = 3_000_000\n        elif system_memory_gb &gt;= 16:\n            expected_threshold = 1_500_000\n        else:\n            expected_threshold = 500_000\n\n        # Test datasets around the threshold\n        test_sizes = [\n            expected_threshold // 2,  # Below threshold\n            expected_threshold,  # At threshold\n            expected_threshold * 2,  # Above threshold\n        ]\n\n        for dataset_size in test_sizes:\n            uses_fallback = dataset_size &gt; expected_threshold\n\n            # Fallback behavior should be consistent\n            if uses_fallback:\n                # Should use more conservative chunking\n                pass  # Fallback logic is complex, just verify it doesn't crash\n            else:\n                # Should use regular optimized chunking\n                pass\n\n    def test_memory_constrained_system_scenario(self):\n        \"\"\"Test behavior on memory-constrained systems.\"\"\"\n        # Simulate a 4GB system\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 4 * 1024**3\n\n            manager = MemoryManager()\n\n            # Should allocate only 20% (0.8GB) on constrained system\n            assert abs(manager.max_memory_gb - 0.8) &lt; 0.1\n\n            # Should use conservative chunk sizes\n            conservative_chunk = manager.calculate_adaptive_chunk_size(\n                100_000, \"ngram_generation\"\n            )\n\n            # Should be reduced due to system constraints\n            assert conservative_chunk &lt;= 100_000\n\n            # Should still be usable\n            assert conservative_chunk &gt;= 1000\n\n    def test_high_memory_system_scenario(self):\n        \"\"\"Test behavior on high-memory systems.\"\"\"\n        # Simulate a 32GB system\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            mock_vm.return_value.total = 32 * 1024**3\n\n            manager = MemoryManager()\n\n            # Should allocate 40% (12.8GB) on high-memory system\n            assert abs(manager.max_memory_gb - 12.8) &lt; 0.1\n\n            # Should use larger chunk sizes\n            large_chunk = manager.calculate_adaptive_chunk_size(100_000, \"tokenization\")\n\n            # Should be able to scale up for some operations\n            assert large_chunk &gt;= 50_000  # Should be reasonable sized\n\n    def _create_social_media_dataset(self, size: int) -&gt; pl.DataFrame:\n        \"\"\"Create a realistic social media dataset.\"\"\"\n        import random\n\n        # Sample social media content patterns\n        content_templates = [\n            \"Just finished watching {movie}! Amazing {adjective}!\",\n            \"Can't believe {celebrity} said that about {topic}\",\n            \"Weather is {weather_adj} today in {city}\",\n            \"Check out this {adjective} {noun} I found!\",\n            \"Happy {day} everyone! Hope you have a {adjective} day!\",\n            \"Anyone else think {opinion}? Just me? #thoughts\",\n        ]\n\n        substitutions = {\n            \"movie\": [\"Avatar\", \"Inception\", \"The Matrix\", \"Frozen\", \"Avengers\"],\n            \"adjective\": [\"amazing\", \"terrible\", \"incredible\", \"boring\", \"fantastic\"],\n            \"celebrity\": [\"@celebrity1\", \"@celebrity2\", \"@celebrity3\"],\n            \"topic\": [\"climate change\", \"politics\", \"technology\", \"sports\"],\n            \"weather_adj\": [\"sunny\", \"rainy\", \"cloudy\", \"snowy\", \"windy\"],\n            \"city\": [\"NYC\", \"LA\", \"Chicago\", \"Miami\", \"Seattle\"],\n            \"noun\": [\"gadget\", \"recipe\", \"book\", \"song\", \"photo\"],\n            \"day\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"],\n            \"opinion\": [\"pineapple belongs on pizza\", \"cats are better than dogs\"],\n        }\n\n        messages = []\n        for i in range(size):\n            template = random.choice(content_templates)\n            message = template\n\n            # Apply substitutions\n            for key, values in substitutions.items():\n                if f\"{{{key}}}\" in message:\n                    message = message.replace(f\"{{{key}}}\", random.choice(values))\n\n            messages.append(\n                {\n                    \"message_id\": f\"msg_{i:06d}\",\n                    \"message_text\": message,\n                    \"author_id\": f\"user_{i % (size // 10)}\",  # 10% unique users\n                    \"timestamp\": f\"2023-{random.randint(1,12):02d}-{random.randint(1,28):02d}T{random.randint(0,23):02d}:00:00Z\",\n                }\n            )\n\n        return pl.DataFrame(messages)\n\n    def _simulate_ngram_processing(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate n-gram processing with chunking.\"\"\"\n        chunks_processed = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate tokenization and n-gram generation\n            processed_chunk = chunk.select(\n                [\n                    pl.col(\"message_text\").str.split(\" \").alias(\"tokens\"),\n                    pl.col(\"message_id\"),\n                    pl.col(\"author_id\"),\n                ]\n            ).with_columns(\n                [\n                    # Simulate n-gram generation (just count tokens for simplicity)\n                    pl.col(\"tokens\")\n                    .list.len()\n                    .alias(\"token_count\")\n                ]\n            )\n\n            chunks_processed += 1\n\n        return chunks_processed\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestRealWorldScenarios.test_high_memory_system_scenario","title":"<code>test_high_memory_system_scenario()</code>","text":"<p>Test behavior on high-memory systems.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_high_memory_system_scenario(self):\n    \"\"\"Test behavior on high-memory systems.\"\"\"\n    # Simulate a 32GB system\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 32 * 1024**3\n\n        manager = MemoryManager()\n\n        # Should allocate 40% (12.8GB) on high-memory system\n        assert abs(manager.max_memory_gb - 12.8) &lt; 0.1\n\n        # Should use larger chunk sizes\n        large_chunk = manager.calculate_adaptive_chunk_size(100_000, \"tokenization\")\n\n        # Should be able to scale up for some operations\n        assert large_chunk &gt;= 50_000  # Should be reasonable sized\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestRealWorldScenarios.test_large_dataset_fallback_scenario","title":"<code>test_large_dataset_fallback_scenario()</code>","text":"<p>Test fallback behavior with large datasets.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_large_dataset_fallback_scenario(self):\n    \"\"\"Test fallback behavior with large datasets.\"\"\"\n    # Test the fallback threshold logic\n    manager = MemoryManager()\n\n    # Determine fallback threshold based on system memory\n    system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n    if system_memory_gb &gt;= 32:\n        expected_threshold = 3_000_000\n    elif system_memory_gb &gt;= 16:\n        expected_threshold = 1_500_000\n    else:\n        expected_threshold = 500_000\n\n    # Test datasets around the threshold\n    test_sizes = [\n        expected_threshold // 2,  # Below threshold\n        expected_threshold,  # At threshold\n        expected_threshold * 2,  # Above threshold\n    ]\n\n    for dataset_size in test_sizes:\n        uses_fallback = dataset_size &gt; expected_threshold\n\n        # Fallback behavior should be consistent\n        if uses_fallback:\n            # Should use more conservative chunking\n            pass  # Fallback logic is complex, just verify it doesn't crash\n        else:\n            # Should use regular optimized chunking\n            pass\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestRealWorldScenarios.test_memory_constrained_system_scenario","title":"<code>test_memory_constrained_system_scenario()</code>","text":"<p>Test behavior on memory-constrained systems.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_memory_constrained_system_scenario(self):\n    \"\"\"Test behavior on memory-constrained systems.\"\"\"\n    # Simulate a 4GB system\n    with patch(\"psutil.virtual_memory\") as mock_vm:\n        mock_vm.return_value.total = 4 * 1024**3\n\n        manager = MemoryManager()\n\n        # Should allocate only 20% (0.8GB) on constrained system\n        assert abs(manager.max_memory_gb - 0.8) &lt; 0.1\n\n        # Should use conservative chunk sizes\n        conservative_chunk = manager.calculate_adaptive_chunk_size(\n            100_000, \"ngram_generation\"\n        )\n\n        # Should be reduced due to system constraints\n        assert conservative_chunk &lt;= 100_000\n\n        # Should still be usable\n        assert conservative_chunk &gt;= 1000\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_integration_validation.TestRealWorldScenarios.test_typical_social_media_dataset_scenario","title":"<code>test_typical_social_media_dataset_scenario()</code>","text":"<p>Test with a dataset that simulates typical social media analysis.</p> Source code in <code>testing/performance/test_integration_validation.py</code> <pre><code>def test_typical_social_media_dataset_scenario(self):\n    \"\"\"Test with a dataset that simulates typical social media analysis.\"\"\"\n    # Create realistic dataset\n    dataset = self._create_social_media_dataset(100_000)\n\n    # Test with auto-detected memory manager\n    manager = MemoryManager()\n\n    # Simulate n-gram analysis workflow\n    base_chunk_size = 50_000  # Old default\n\n    # Calculate adaptive chunk size\n    adaptive_chunk = manager.calculate_adaptive_chunk_size(\n        base_chunk_size, \"ngram_generation\"\n    )\n\n    # Should be reasonable for the dataset\n    assert adaptive_chunk &gt; 0\n    assert adaptive_chunk &lt;= base_chunk_size * 2  # Reasonable scaling\n\n    # Test processing with adaptive chunk size\n    start_time = time.time()\n    chunks_processed = self._simulate_ngram_processing(dataset, adaptive_chunk)\n    processing_time = time.time() - start_time\n\n    # Should complete in reasonable time\n    assert processing_time &lt; 30  # Should be fast for test dataset\n    assert chunks_processed &gt; 0\n\n    # Memory usage should be reasonable\n    memory_stats = manager.get_current_memory_usage()\n    assert memory_stats[\"rss_gb\"] &lt;= manager.max_memory_gb * 1.2  # 20% tolerance\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks","title":"<code>test_performance_benchmarks</code>","text":"<p>Performance Benchmarking Tests for Chunking Optimization Measures actual performance improvements and validates 2-4x performance gains.</p>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks","title":"<code>TestPerformanceBenchmarks</code>","text":"<p>Comprehensive performance benchmarking suite.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>@pytest.mark.performance\n@pytest.mark.slow\nclass TestPerformanceBenchmarks:\n    \"\"\"Comprehensive performance benchmarking suite.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment before each test.\"\"\"\n        # Force garbage collection to start with clean state\n        gc.collect()\n\n        # Get baseline memory usage\n        self.initial_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n\n    def teardown_method(self):\n        \"\"\"Clean up after each test.\"\"\"\n        gc.collect()\n\n    def _create_realistic_dataset(\n        self, num_messages: int, avg_tokens_per_message: int = 20\n    ) -&gt; pl.DataFrame:\n        \"\"\"Create a realistic test dataset with variable message lengths.\"\"\"\n        import random\n\n        # Common words for realistic n-gram generation\n        words = [\n            \"the\",\n            \"and\",\n            \"is\",\n            \"in\",\n            \"to\",\n            \"of\",\n            \"a\",\n            \"for\",\n            \"on\",\n            \"with\",\n            \"as\",\n            \"by\",\n            \"be\",\n            \"at\",\n            \"this\",\n            \"that\",\n            \"from\",\n            \"they\",\n            \"we\",\n            \"you\",\n            \"have\",\n            \"has\",\n            \"had\",\n            \"will\",\n            \"would\",\n            \"could\",\n            \"should\",\n            \"can\",\n            \"may\",\n            \"data\",\n            \"analysis\",\n            \"social\",\n            \"media\",\n            \"content\",\n            \"user\",\n            \"post\",\n            \"comment\",\n            \"hashtag\",\n            \"trend\",\n            \"viral\",\n            \"engagement\",\n            \"reach\",\n            \"impression\",\n            \"click\",\n            \"like\",\n            \"share\",\n            \"retweet\",\n            \"follow\",\n            \"followers\",\n            \"following\",\n            \"account\",\n        ]\n\n        messages = []\n        for i in range(num_messages):\n            # Variable message length (10-40 tokens)\n            num_tokens = random.randint(\n                max(5, avg_tokens_per_message - 10), avg_tokens_per_message + 20\n            )\n\n            # Generate message with realistic word distribution\n            message_words = []\n            for _ in range(num_tokens):\n                # Higher probability for common words\n                if random.random() &lt; 0.3:\n                    word = random.choice(words[:10])  # Very common words\n                elif random.random() &lt; 0.6:\n                    word = random.choice(words[:30])  # Common words\n                else:\n                    word = random.choice(words)  # All words\n\n                message_words.append(word)\n\n            messages.append(\n                {\n                    \"message_id\": f\"msg_{i:06d}\",\n                    \"message_text\": \" \".join(message_words),\n                    \"author_id\": f\"user_{i % (num_messages // 10)}\",  # 10% unique users\n                    \"timestamp\": f\"2023-01-{(i % 31) + 1:02d}T{(i % 24):02d}:00:00Z\",\n                }\n            )\n\n        return pl.DataFrame(messages)\n\n    def _benchmark_chunk_processing(\n        self, dataset: pl.DataFrame, old_chunk_size: int, new_chunk_size: int\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark chunk processing with different chunk sizes.\"\"\"\n        results = {}\n\n        # Benchmark old chunk size\n        start_time = time.time()\n        old_chunks = self._simulate_chunk_processing(dataset, old_chunk_size)\n        old_time = time.time() - start_time\n        results[\"old_time\"] = old_time\n        results[\"old_chunks\"] = old_chunks\n\n        # Clear memory between tests\n        gc.collect()\n\n        # Benchmark new chunk size\n        start_time = time.time()\n        new_chunks = self._simulate_chunk_processing(dataset, new_chunk_size)\n        new_time = time.time() - start_time\n        results[\"new_time\"] = new_time\n        results[\"new_chunks\"] = new_chunks\n\n        # Calculate improvements\n        results[\"time_improvement\"] = old_time / new_time if new_time &gt; 0 else 1.0\n        results[\"io_reduction\"] = old_chunks / new_chunks if new_chunks &gt; 0 else 1.0\n\n        return results\n\n    def _simulate_chunk_processing(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate chunk processing and return number of chunks processed.\"\"\"\n        num_chunks = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate processing work (tokenization, basic operations)\n            _ = chunk.select(\n                [\n                    pl.col(\"message_text\").str.split(\" \").alias(\"tokens\"),\n                    pl.col(\"message_id\"),\n                    pl.col(\"author_id\"),\n                ]\n            )\n\n            num_chunks += 1\n\n            # Simulate memory cleanup every few chunks\n            if num_chunks % 5 == 0:\n                gc.collect()\n\n        return num_chunks\n\n    def test_small_dataset_performance(self):\n        \"\"\"Test performance improvements on small datasets (100K messages).\"\"\"\n        dataset = self._create_realistic_dataset(100_000, avg_tokens_per_message=15)\n\n        # Old vs new chunk sizes for small datasets\n        old_chunk_size = 50_000  # Original base\n        new_chunk_size = 200_000  # New base for small datasets\n\n        results = self._benchmark_chunk_processing(\n            dataset, old_chunk_size, new_chunk_size\n        )\n\n        # Should have fewer chunks with new size\n        assert (\n            results[\"io_reduction\"] &gt;= 2.0\n        ), f\"Expected at least 2x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n        # Should be faster (allowing for test variability)\n        assert (\n            results[\"time_improvement\"] &gt;= 1.02\n        ), f\"Expected at least 1.02x time improvement, got {results['time_improvement']:.2f}x\"\n\n        # Memory usage should be reasonable\n        current_memory = psutil.Process().memory_info().rss / 1024**2\n        memory_increase = current_memory - self.initial_memory\n        assert (\n            memory_increase &lt; 500\n        ), f\"Memory usage increased by {memory_increase:.1f}MB, should be &lt; 500MB\"\n\n    def test_medium_dataset_performance(self):\n        \"\"\"Test performance improvements on medium datasets (500K messages).\"\"\"\n        dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n        # Old vs new chunk sizes for medium datasets\n        old_chunk_size = 50_000  # Original base\n        new_chunk_size = 150_000  # New base for medium datasets\n\n        results = self._benchmark_chunk_processing(\n            dataset, old_chunk_size, new_chunk_size\n        )\n\n        # Should have significant I/O reduction\n        assert (\n            results[\"io_reduction\"] &gt;= 2.5\n        ), f\"Expected at least 2.5x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n        # Should be noticeably faster\n        assert (\n            results[\"time_improvement\"] &gt;= 1.3\n        ), f\"Expected at least 1.3x time improvement, got {results['time_improvement']:.2f}x\"\n\n        # Validate chunk counts make sense\n        expected_old_chunks = (500_000 + old_chunk_size - 1) // old_chunk_size\n        expected_new_chunks = (500_000 + new_chunk_size - 1) // new_chunk_size\n\n        assert abs(results[\"old_chunks\"] - expected_old_chunks) &lt;= 1\n        assert abs(results[\"new_chunks\"] - expected_new_chunks) &lt;= 1\n\n    def test_large_dataset_performance(self):\n        \"\"\"Test performance improvements on large datasets (1M messages).\"\"\"\n        dataset = self._create_realistic_dataset(1_000_000, avg_tokens_per_message=20)\n\n        # Test with different chunk sizes based on system memory\n        memory_manager = MemoryManager()\n        system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n        if system_memory_gb &gt;= 16:\n            memory_factor = 1.5\n        elif system_memory_gb &gt;= 8:\n            memory_factor = 1.0\n        else:\n            memory_factor = 0.5\n\n        old_chunk_size = 50_000\n        new_chunk_size = int(150_000 * memory_factor)  # Adaptive based on system\n\n        results = self._benchmark_chunk_processing(\n            dataset, old_chunk_size, new_chunk_size\n        )\n\n        # Should have substantial improvements\n        expected_io_reduction = new_chunk_size / old_chunk_size\n        assert (\n            results[\"io_reduction\"] &gt;= expected_io_reduction * 0.8\n        ), f\"Expected ~{expected_io_reduction:.1f}x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n        # Time improvement should be significant for large datasets\n        assert (\n            results[\"time_improvement\"] &gt;= 1.15\n        ), f\"Expected at least 1.15x time improvement, got {results['time_improvement']:.2f}x\"\n\n    def test_memory_adaptive_chunk_sizing_performance(self):\n        \"\"\"Test that memory-adaptive chunk sizing provides better performance.\"\"\"\n        dataset = self._create_realistic_dataset(300_000, avg_tokens_per_message=15)\n\n        # Test with different memory configurations\n        test_configs = [\n            (4.0, 1.0),  # 4GB limit, 1.0x factor (old config)\n            (8.0, 1.5),  # 8GB limit, 1.5x factor (16GB system)\n            (12.0, 2.0),  # 12GB limit, 2.0x factor (32GB system)\n        ]\n\n        performance_results = []\n\n        for memory_limit, expected_factor in test_configs:\n            with patch(\"psutil.virtual_memory\") as mock_vm:\n                # Set up system memory to match expected factor\n                if expected_factor == 1.0:\n                    mock_vm.return_value.total = 8 * 1024**3  # 8GB system\n                elif expected_factor == 1.5:\n                    mock_vm.return_value.total = 16 * 1024**3  # 16GB system\n                else:\n                    mock_vm.return_value.total = 32 * 1024**3  # 32GB system\n\n                memory_manager = MemoryManager()\n\n                # Calculate chunk size with adaptive scaling\n                base_chunk = 150_000\n                adaptive_chunk = int(base_chunk * expected_factor)\n                adaptive_chunk = max(10_000, min(adaptive_chunk, 500_000))\n\n                # Benchmark this configuration\n                start_time = time.time()\n                chunks = self._simulate_chunk_processing(dataset, adaptive_chunk)\n                elapsed = time.time() - start_time\n\n                performance_results.append(\n                    {\n                        \"memory_limit\": memory_limit,\n                        \"factor\": expected_factor,\n                        \"chunk_size\": adaptive_chunk,\n                        \"time\": elapsed,\n                        \"chunks\": chunks,\n                    }\n                )\n\n                gc.collect()\n\n        # Higher memory configurations should be faster\n        for i in range(1, len(performance_results)):\n            current = performance_results[i]\n            previous = performance_results[i - 1]\n\n            # Should have larger chunks\n            assert current[\"chunk_size\"] &gt;= previous[\"chunk_size\"]\n\n            # Should have fewer chunks (better I/O efficiency)\n            assert current[\"chunks\"] &lt;= previous[\"chunks\"]\n\n    def test_vectorized_ngram_generation_performance(self):\n        \"\"\"Test performance of vectorized n-gram generation with larger chunks.\"\"\"\n        # Create dataset with pre-tokenized data\n        dataset_size = 50_000\n        tokens_data = []\n\n        for i in range(dataset_size):\n            tokens = [\n                f\"word_{j}\" for j in range(i % 10 + 5)\n            ]  # Variable length 5-14 tokens\n            tokens_data.append({\"message_surrogate_id\": i, \"tokens\": tokens})\n\n        df = pl.DataFrame(tokens_data)\n\n        # Test old vs new chunk sizes\n        old_chunk_size = 10_000\n        new_chunk_size = 30_000\n\n        # Benchmark old chunk size\n        start_time = time.time()\n        old_result = self._benchmark_vectorized_ngram_generation(\n            df, old_chunk_size, min_n=2, max_n=3\n        )\n        old_time = time.time() - start_time\n\n        gc.collect()\n\n        # Benchmark new chunk size\n        start_time = time.time()\n        new_result = self._benchmark_vectorized_ngram_generation(\n            df, new_chunk_size, min_n=2, max_n=3\n        )\n        new_time = time.time() - start_time\n\n        # Should produce same results\n        assert len(old_result) == len(new_result), \"Results should be identical\"\n\n        # Should be faster with larger chunks\n        time_improvement = old_time / new_time if new_time &gt; 0 else 1.0\n        assert (\n            time_improvement &gt;= 0.95\n        ), f\"Expected at least 0.95x improvement, got {time_improvement:.2f}x\"\n\n    def _benchmark_vectorized_ngram_generation(\n        self, df: pl.DataFrame, chunk_size: int, min_n: int, max_n: int\n    ) -&gt; pl.DataFrame:\n        \"\"\"Benchmark vectorized n-gram generation with specified chunk size.\"\"\"\n        results = []\n\n        for start_idx in range(0, len(df), chunk_size):\n            end_idx = min(start_idx + chunk_size, len(df))\n            chunk = df.slice(start_idx, end_idx - start_idx)\n\n            # Simulate vectorized n-gram generation\n            chunk_result = (\n                chunk.select([pl.col(\"message_surrogate_id\"), pl.col(\"tokens\")])\n                .with_columns(\n                    [\n                        # Simulate n-gram generation\n                        pl.col(\"tokens\")\n                        .map_elements(\n                            lambda tokens: self._generate_ngrams_for_tokens(\n                                tokens, min_n, max_n\n                            ),\n                            return_dtype=pl.List(pl.String),\n                        )\n                        .alias(\"ngrams\")\n                    ]\n                )\n                .explode(\"ngrams\")\n                .filter(pl.col(\"ngrams\").is_not_null())\n                .select(\n                    [\n                        pl.col(\"message_surrogate_id\"),\n                        pl.col(\"ngrams\").alias(\"ngram_text\"),\n                    ]\n                )\n            )\n\n            results.append(chunk_result)\n\n        # Combine all results\n        if results:\n            return pl.concat(results)\n        else:\n            return pl.DataFrame(\n                schema={\"message_surrogate_id\": pl.Int64, \"ngram_text\": pl.String}\n            )\n\n    def _generate_ngrams_for_tokens(\n        self, tokens: List[str], min_n: int, max_n: int\n    ) -&gt; List[str]:\n        \"\"\"Generate n-grams from a list of tokens.\"\"\"\n        if len(tokens) == 0 or len(tokens) &lt; min_n:\n            return []\n\n        ngrams = []\n        for n in range(min_n, max_n + 1):\n            for i in range(len(tokens) - n + 1):\n                ngram = \" \".join(tokens[i : i + n])\n                ngrams.append(ngram)\n\n        return ngrams\n\n    def test_fallback_threshold_performance(self):\n        \"\"\"Test performance improvements with updated fallback thresholds.\"\"\"\n        # Test datasets of different sizes around fallback thresholds\n        test_sizes = [\n            400_000,  # Below old threshold (500K)\n            800_000,  # Above old threshold, below new 16GB threshold (1.5M)\n            1_200_000,  # Above old threshold, below new 16GB threshold\n            2_000_000,  # Above new 16GB threshold\n        ]\n\n        memory_manager = MemoryManager()\n        system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n        # Determine expected threshold based on system memory\n        if system_memory_gb &gt;= 32:\n            new_threshold = 3_000_000\n        elif system_memory_gb &gt;= 16:\n            new_threshold = 1_500_000\n        else:\n            new_threshold = 500_000\n\n        old_threshold = 500_000\n\n        for dataset_size in test_sizes:\n            # Check which processing method would be used\n            uses_old_fallback = dataset_size &gt; old_threshold\n            uses_new_fallback = dataset_size &gt; new_threshold\n\n            # With new thresholds, more datasets should avoid fallback processing\n            if dataset_size &lt;= new_threshold and dataset_size &gt; old_threshold:\n                # This dataset would have used fallback with old threshold\n                # but uses regular processing with new threshold\n                assert (\n                    not uses_new_fallback\n                ), f\"Dataset size {dataset_size} should not use fallback with new threshold {new_threshold}\"\n                assert (\n                    uses_old_fallback\n                ), f\"Dataset size {dataset_size} would have used fallback with old threshold {old_threshold}\"\n\n    def test_memory_usage_efficiency(self):\n        \"\"\"Test that memory usage is more efficient with new chunking.\"\"\"\n        dataset = self._create_realistic_dataset(200_000, avg_tokens_per_message=12)\n\n        # Test memory usage with different chunk sizes\n        old_chunk_size = 25_000  # Old fallback chunk size\n        new_chunk_size = 100_000  # New fallback chunk size\n\n        # Measure memory usage with old chunk size\n        gc.collect()\n        initial_memory = psutil.Process().memory_info().rss\n\n        self._simulate_chunk_processing(dataset, old_chunk_size)\n        old_peak_memory = psutil.Process().memory_info().rss\n        old_memory_usage = (old_peak_memory - initial_memory) / 1024**2  # MB\n\n        gc.collect()\n\n        # Measure memory usage with new chunk size\n        initial_memory = psutil.Process().memory_info().rss\n\n        self._simulate_chunk_processing(dataset, new_chunk_size)\n        new_peak_memory = psutil.Process().memory_info().rss\n        new_memory_usage = (new_peak_memory - initial_memory) / 1024**2  # MB\n\n        # Memory usage should be reasonable for both\n        # Larger chunks may use more memory but should be more efficient\n        assert (\n            new_memory_usage &lt; old_memory_usage * 5\n        ), f\"New memory usage ({new_memory_usage:.1f}MB) should not be more than 5x old usage ({old_memory_usage:.1f}MB)\"\n\n        # Both should use reasonable amounts of memory\n        assert (\n            old_memory_usage &lt; 1000\n        ), f\"Old chunking should use &lt; 1GB, used {old_memory_usage:.1f}MB\"\n        assert (\n            new_memory_usage &lt; 1000\n        ), f\"New chunking should use &lt; 1GB, used {new_memory_usage:.1f}MB\"\n\n    @pytest.mark.skipif(\n        psutil.virtual_memory().total &lt; 8 * 1024**3,\n        reason=\"Requires at least 8GB RAM for comprehensive performance testing\",\n    )\n    def test_comprehensive_performance_validation(self):\n        \"\"\"Comprehensive performance validation on systems with adequate memory.\"\"\"\n        system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n        # Test with appropriately sized dataset\n        if system_memory_gb &gt;= 16:\n            dataset_size = 1_000_000\n            expected_min_improvement = 1.25\n        else:\n            dataset_size = 500_000\n            expected_min_improvement = 1.25\n\n        dataset = self._create_realistic_dataset(\n            dataset_size, avg_tokens_per_message=18\n        )\n\n        # Compare old conservative approach vs new adaptive approach\n        old_chunk_size = 50_000\n\n        # Calculate new chunk size based on system\n        if system_memory_gb &gt;= 32:\n            memory_factor = 2.0\n        elif system_memory_gb &gt;= 16:\n            memory_factor = 1.5\n        elif system_memory_gb &gt;= 8:\n            memory_factor = 1.0\n        else:\n            memory_factor = 0.5\n\n        new_chunk_size = int(150_000 * memory_factor)\n        new_chunk_size = max(10_000, min(new_chunk_size, 500_000))\n\n        results = self._benchmark_chunk_processing(\n            dataset, old_chunk_size, new_chunk_size\n        )\n\n        # Should meet performance improvement targets\n        assert (\n            results[\"time_improvement\"] &gt;= expected_min_improvement\n        ), f\"Expected at least {expected_min_improvement}x improvement, got {results['time_improvement']:.2f}x\"\n\n        # Should have substantial I/O reduction\n        expected_io_reduction = new_chunk_size / old_chunk_size\n        assert (\n            results[\"io_reduction\"] &gt;= expected_io_reduction * 0.8\n        ), f\"Expected ~{expected_io_reduction:.1f}x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n        # Log results for documentation\n        print(f\"\\nPerformance Results for {system_memory_gb:.1f}GB system:\")\n        print(f\"  Dataset size: {dataset_size:,} messages\")\n        print(f\"  Old chunk size: {old_chunk_size:,}\")\n        print(f\"  New chunk size: {new_chunk_size:,}\")\n        print(f\"  Time improvement: {results['time_improvement']:.2f}x\")\n        print(f\"  I/O reduction: {results['io_reduction']:.2f}x\")\n        print(f\"  Memory factor: {memory_factor}x\")\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.setup_method","title":"<code>setup_method()</code>","text":"<p>Set up test environment before each test.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def setup_method(self):\n    \"\"\"Set up test environment before each test.\"\"\"\n    # Force garbage collection to start with clean state\n    gc.collect()\n\n    # Get baseline memory usage\n    self.initial_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.teardown_method","title":"<code>teardown_method()</code>","text":"<p>Clean up after each test.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def teardown_method(self):\n    \"\"\"Clean up after each test.\"\"\"\n    gc.collect()\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_comprehensive_performance_validation","title":"<code>test_comprehensive_performance_validation()</code>","text":"<p>Comprehensive performance validation on systems with adequate memory.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>@pytest.mark.skipif(\n    psutil.virtual_memory().total &lt; 8 * 1024**3,\n    reason=\"Requires at least 8GB RAM for comprehensive performance testing\",\n)\ndef test_comprehensive_performance_validation(self):\n    \"\"\"Comprehensive performance validation on systems with adequate memory.\"\"\"\n    system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n    # Test with appropriately sized dataset\n    if system_memory_gb &gt;= 16:\n        dataset_size = 1_000_000\n        expected_min_improvement = 1.25\n    else:\n        dataset_size = 500_000\n        expected_min_improvement = 1.25\n\n    dataset = self._create_realistic_dataset(\n        dataset_size, avg_tokens_per_message=18\n    )\n\n    # Compare old conservative approach vs new adaptive approach\n    old_chunk_size = 50_000\n\n    # Calculate new chunk size based on system\n    if system_memory_gb &gt;= 32:\n        memory_factor = 2.0\n    elif system_memory_gb &gt;= 16:\n        memory_factor = 1.5\n    elif system_memory_gb &gt;= 8:\n        memory_factor = 1.0\n    else:\n        memory_factor = 0.5\n\n    new_chunk_size = int(150_000 * memory_factor)\n    new_chunk_size = max(10_000, min(new_chunk_size, 500_000))\n\n    results = self._benchmark_chunk_processing(\n        dataset, old_chunk_size, new_chunk_size\n    )\n\n    # Should meet performance improvement targets\n    assert (\n        results[\"time_improvement\"] &gt;= expected_min_improvement\n    ), f\"Expected at least {expected_min_improvement}x improvement, got {results['time_improvement']:.2f}x\"\n\n    # Should have substantial I/O reduction\n    expected_io_reduction = new_chunk_size / old_chunk_size\n    assert (\n        results[\"io_reduction\"] &gt;= expected_io_reduction * 0.8\n    ), f\"Expected ~{expected_io_reduction:.1f}x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n    # Log results for documentation\n    print(f\"\\nPerformance Results for {system_memory_gb:.1f}GB system:\")\n    print(f\"  Dataset size: {dataset_size:,} messages\")\n    print(f\"  Old chunk size: {old_chunk_size:,}\")\n    print(f\"  New chunk size: {new_chunk_size:,}\")\n    print(f\"  Time improvement: {results['time_improvement']:.2f}x\")\n    print(f\"  I/O reduction: {results['io_reduction']:.2f}x\")\n    print(f\"  Memory factor: {memory_factor}x\")\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_fallback_threshold_performance","title":"<code>test_fallback_threshold_performance()</code>","text":"<p>Test performance improvements with updated fallback thresholds.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_fallback_threshold_performance(self):\n    \"\"\"Test performance improvements with updated fallback thresholds.\"\"\"\n    # Test datasets of different sizes around fallback thresholds\n    test_sizes = [\n        400_000,  # Below old threshold (500K)\n        800_000,  # Above old threshold, below new 16GB threshold (1.5M)\n        1_200_000,  # Above old threshold, below new 16GB threshold\n        2_000_000,  # Above new 16GB threshold\n    ]\n\n    memory_manager = MemoryManager()\n    system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n    # Determine expected threshold based on system memory\n    if system_memory_gb &gt;= 32:\n        new_threshold = 3_000_000\n    elif system_memory_gb &gt;= 16:\n        new_threshold = 1_500_000\n    else:\n        new_threshold = 500_000\n\n    old_threshold = 500_000\n\n    for dataset_size in test_sizes:\n        # Check which processing method would be used\n        uses_old_fallback = dataset_size &gt; old_threshold\n        uses_new_fallback = dataset_size &gt; new_threshold\n\n        # With new thresholds, more datasets should avoid fallback processing\n        if dataset_size &lt;= new_threshold and dataset_size &gt; old_threshold:\n            # This dataset would have used fallback with old threshold\n            # but uses regular processing with new threshold\n            assert (\n                not uses_new_fallback\n            ), f\"Dataset size {dataset_size} should not use fallback with new threshold {new_threshold}\"\n            assert (\n                uses_old_fallback\n            ), f\"Dataset size {dataset_size} would have used fallback with old threshold {old_threshold}\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_large_dataset_performance","title":"<code>test_large_dataset_performance()</code>","text":"<p>Test performance improvements on large datasets (1M messages).</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_large_dataset_performance(self):\n    \"\"\"Test performance improvements on large datasets (1M messages).\"\"\"\n    dataset = self._create_realistic_dataset(1_000_000, avg_tokens_per_message=20)\n\n    # Test with different chunk sizes based on system memory\n    memory_manager = MemoryManager()\n    system_memory_gb = psutil.virtual_memory().total / 1024**3\n\n    if system_memory_gb &gt;= 16:\n        memory_factor = 1.5\n    elif system_memory_gb &gt;= 8:\n        memory_factor = 1.0\n    else:\n        memory_factor = 0.5\n\n    old_chunk_size = 50_000\n    new_chunk_size = int(150_000 * memory_factor)  # Adaptive based on system\n\n    results = self._benchmark_chunk_processing(\n        dataset, old_chunk_size, new_chunk_size\n    )\n\n    # Should have substantial improvements\n    expected_io_reduction = new_chunk_size / old_chunk_size\n    assert (\n        results[\"io_reduction\"] &gt;= expected_io_reduction * 0.8\n    ), f\"Expected ~{expected_io_reduction:.1f}x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n    # Time improvement should be significant for large datasets\n    assert (\n        results[\"time_improvement\"] &gt;= 1.15\n    ), f\"Expected at least 1.15x time improvement, got {results['time_improvement']:.2f}x\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_medium_dataset_performance","title":"<code>test_medium_dataset_performance()</code>","text":"<p>Test performance improvements on medium datasets (500K messages).</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_medium_dataset_performance(self):\n    \"\"\"Test performance improvements on medium datasets (500K messages).\"\"\"\n    dataset = self._create_realistic_dataset(500_000, avg_tokens_per_message=18)\n\n    # Old vs new chunk sizes for medium datasets\n    old_chunk_size = 50_000  # Original base\n    new_chunk_size = 150_000  # New base for medium datasets\n\n    results = self._benchmark_chunk_processing(\n        dataset, old_chunk_size, new_chunk_size\n    )\n\n    # Should have significant I/O reduction\n    assert (\n        results[\"io_reduction\"] &gt;= 2.5\n    ), f\"Expected at least 2.5x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n    # Should be noticeably faster\n    assert (\n        results[\"time_improvement\"] &gt;= 1.3\n    ), f\"Expected at least 1.3x time improvement, got {results['time_improvement']:.2f}x\"\n\n    # Validate chunk counts make sense\n    expected_old_chunks = (500_000 + old_chunk_size - 1) // old_chunk_size\n    expected_new_chunks = (500_000 + new_chunk_size - 1) // new_chunk_size\n\n    assert abs(results[\"old_chunks\"] - expected_old_chunks) &lt;= 1\n    assert abs(results[\"new_chunks\"] - expected_new_chunks) &lt;= 1\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_memory_adaptive_chunk_sizing_performance","title":"<code>test_memory_adaptive_chunk_sizing_performance()</code>","text":"<p>Test that memory-adaptive chunk sizing provides better performance.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_memory_adaptive_chunk_sizing_performance(self):\n    \"\"\"Test that memory-adaptive chunk sizing provides better performance.\"\"\"\n    dataset = self._create_realistic_dataset(300_000, avg_tokens_per_message=15)\n\n    # Test with different memory configurations\n    test_configs = [\n        (4.0, 1.0),  # 4GB limit, 1.0x factor (old config)\n        (8.0, 1.5),  # 8GB limit, 1.5x factor (16GB system)\n        (12.0, 2.0),  # 12GB limit, 2.0x factor (32GB system)\n    ]\n\n    performance_results = []\n\n    for memory_limit, expected_factor in test_configs:\n        with patch(\"psutil.virtual_memory\") as mock_vm:\n            # Set up system memory to match expected factor\n            if expected_factor == 1.0:\n                mock_vm.return_value.total = 8 * 1024**3  # 8GB system\n            elif expected_factor == 1.5:\n                mock_vm.return_value.total = 16 * 1024**3  # 16GB system\n            else:\n                mock_vm.return_value.total = 32 * 1024**3  # 32GB system\n\n            memory_manager = MemoryManager()\n\n            # Calculate chunk size with adaptive scaling\n            base_chunk = 150_000\n            adaptive_chunk = int(base_chunk * expected_factor)\n            adaptive_chunk = max(10_000, min(adaptive_chunk, 500_000))\n\n            # Benchmark this configuration\n            start_time = time.time()\n            chunks = self._simulate_chunk_processing(dataset, adaptive_chunk)\n            elapsed = time.time() - start_time\n\n            performance_results.append(\n                {\n                    \"memory_limit\": memory_limit,\n                    \"factor\": expected_factor,\n                    \"chunk_size\": adaptive_chunk,\n                    \"time\": elapsed,\n                    \"chunks\": chunks,\n                }\n            )\n\n            gc.collect()\n\n    # Higher memory configurations should be faster\n    for i in range(1, len(performance_results)):\n        current = performance_results[i]\n        previous = performance_results[i - 1]\n\n        # Should have larger chunks\n        assert current[\"chunk_size\"] &gt;= previous[\"chunk_size\"]\n\n        # Should have fewer chunks (better I/O efficiency)\n        assert current[\"chunks\"] &lt;= previous[\"chunks\"]\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_memory_usage_efficiency","title":"<code>test_memory_usage_efficiency()</code>","text":"<p>Test that memory usage is more efficient with new chunking.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_memory_usage_efficiency(self):\n    \"\"\"Test that memory usage is more efficient with new chunking.\"\"\"\n    dataset = self._create_realistic_dataset(200_000, avg_tokens_per_message=12)\n\n    # Test memory usage with different chunk sizes\n    old_chunk_size = 25_000  # Old fallback chunk size\n    new_chunk_size = 100_000  # New fallback chunk size\n\n    # Measure memory usage with old chunk size\n    gc.collect()\n    initial_memory = psutil.Process().memory_info().rss\n\n    self._simulate_chunk_processing(dataset, old_chunk_size)\n    old_peak_memory = psutil.Process().memory_info().rss\n    old_memory_usage = (old_peak_memory - initial_memory) / 1024**2  # MB\n\n    gc.collect()\n\n    # Measure memory usage with new chunk size\n    initial_memory = psutil.Process().memory_info().rss\n\n    self._simulate_chunk_processing(dataset, new_chunk_size)\n    new_peak_memory = psutil.Process().memory_info().rss\n    new_memory_usage = (new_peak_memory - initial_memory) / 1024**2  # MB\n\n    # Memory usage should be reasonable for both\n    # Larger chunks may use more memory but should be more efficient\n    assert (\n        new_memory_usage &lt; old_memory_usage * 5\n    ), f\"New memory usage ({new_memory_usage:.1f}MB) should not be more than 5x old usage ({old_memory_usage:.1f}MB)\"\n\n    # Both should use reasonable amounts of memory\n    assert (\n        old_memory_usage &lt; 1000\n    ), f\"Old chunking should use &lt; 1GB, used {old_memory_usage:.1f}MB\"\n    assert (\n        new_memory_usage &lt; 1000\n    ), f\"New chunking should use &lt; 1GB, used {new_memory_usage:.1f}MB\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_small_dataset_performance","title":"<code>test_small_dataset_performance()</code>","text":"<p>Test performance improvements on small datasets (100K messages).</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_small_dataset_performance(self):\n    \"\"\"Test performance improvements on small datasets (100K messages).\"\"\"\n    dataset = self._create_realistic_dataset(100_000, avg_tokens_per_message=15)\n\n    # Old vs new chunk sizes for small datasets\n    old_chunk_size = 50_000  # Original base\n    new_chunk_size = 200_000  # New base for small datasets\n\n    results = self._benchmark_chunk_processing(\n        dataset, old_chunk_size, new_chunk_size\n    )\n\n    # Should have fewer chunks with new size\n    assert (\n        results[\"io_reduction\"] &gt;= 2.0\n    ), f\"Expected at least 2x I/O reduction, got {results['io_reduction']:.2f}x\"\n\n    # Should be faster (allowing for test variability)\n    assert (\n        results[\"time_improvement\"] &gt;= 1.02\n    ), f\"Expected at least 1.02x time improvement, got {results['time_improvement']:.2f}x\"\n\n    # Memory usage should be reasonable\n    current_memory = psutil.Process().memory_info().rss / 1024**2\n    memory_increase = current_memory - self.initial_memory\n    assert (\n        memory_increase &lt; 500\n    ), f\"Memory usage increased by {memory_increase:.1f}MB, should be &lt; 500MB\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestPerformanceBenchmarks.test_vectorized_ngram_generation_performance","title":"<code>test_vectorized_ngram_generation_performance()</code>","text":"<p>Test performance of vectorized n-gram generation with larger chunks.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_vectorized_ngram_generation_performance(self):\n    \"\"\"Test performance of vectorized n-gram generation with larger chunks.\"\"\"\n    # Create dataset with pre-tokenized data\n    dataset_size = 50_000\n    tokens_data = []\n\n    for i in range(dataset_size):\n        tokens = [\n            f\"word_{j}\" for j in range(i % 10 + 5)\n        ]  # Variable length 5-14 tokens\n        tokens_data.append({\"message_surrogate_id\": i, \"tokens\": tokens})\n\n    df = pl.DataFrame(tokens_data)\n\n    # Test old vs new chunk sizes\n    old_chunk_size = 10_000\n    new_chunk_size = 30_000\n\n    # Benchmark old chunk size\n    start_time = time.time()\n    old_result = self._benchmark_vectorized_ngram_generation(\n        df, old_chunk_size, min_n=2, max_n=3\n    )\n    old_time = time.time() - start_time\n\n    gc.collect()\n\n    # Benchmark new chunk size\n    start_time = time.time()\n    new_result = self._benchmark_vectorized_ngram_generation(\n        df, new_chunk_size, min_n=2, max_n=3\n    )\n    new_time = time.time() - start_time\n\n    # Should produce same results\n    assert len(old_result) == len(new_result), \"Results should be identical\"\n\n    # Should be faster with larger chunks\n    time_improvement = old_time / new_time if new_time &gt; 0 else 1.0\n    assert (\n        time_improvement &gt;= 0.95\n    ), f\"Expected at least 0.95x improvement, got {time_improvement:.2f}x\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestStressTests","title":"<code>TestStressTests</code>","text":"<p>Stress tests for extreme conditions.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>@pytest.mark.performance\n@pytest.mark.slow\nclass TestStressTests:\n    \"\"\"Stress tests for extreme conditions.\"\"\"\n\n    def test_large_chunk_memory_stability(self):\n        \"\"\"Test that large chunks don't cause memory issues.\"\"\"\n        # Test with largest possible chunk size\n        large_chunk_size = 500_000  # Maximum allowed\n        dataset = self._create_test_dataset(100_000)  # Smaller than chunk\n\n        memory_manager = MemoryManager()\n        initial_memory = psutil.Process().memory_info().rss / 1024**2\n\n        # Process with large chunk\n        start_time = time.time()\n        self._simulate_chunk_processing(dataset, large_chunk_size)\n        processing_time = time.time() - start_time\n\n        peak_memory = psutil.Process().memory_info().rss / 1024**2\n        memory_increase = peak_memory - initial_memory\n\n        # Should complete successfully\n        assert processing_time &gt; 0\n\n        # Memory usage should be reasonable\n        assert (\n            memory_increase &lt; memory_manager.max_memory_gb * 1024 * 0.8\n        ), f\"Memory usage ({memory_increase:.1f}MB) should be within 80% of limit\"\n\n    def test_many_small_chunks_efficiency(self):\n        \"\"\"Test efficiency with many small chunks.\"\"\"\n        dataset = self._create_test_dataset(500_000)\n        small_chunk_size = 10_000  # Many small chunks\n\n        start_time = time.time()\n        num_chunks = self._simulate_chunk_processing(dataset, small_chunk_size)\n        processing_time = time.time() - start_time\n\n        # Should complete in reasonable time\n        assert (\n            processing_time &lt; 60\n        ), f\"Processing took {processing_time:.1f}s, should be &lt; 60s\"\n\n        # Should have expected number of chunks\n        expected_chunks = (len(dataset) + small_chunk_size - 1) // small_chunk_size\n        assert abs(num_chunks - expected_chunks) &lt;= 1\n\n    def _create_test_dataset(self, size: int) -&gt; pl.DataFrame:\n        \"\"\"Create a simple test dataset.\"\"\"\n        return pl.DataFrame(\n            {\n                \"message_id\": [f\"msg_{i}\" for i in range(size)],\n                \"message_text\": [f\"test message {i} content\" for i in range(size)],\n                \"author_id\": [f\"user_{i % 1000}\" for i in range(size)],\n                \"timestamp\": [\"2023-01-01T00:00:00Z\"] * size,\n            }\n        )\n\n    def _simulate_chunk_processing(self, dataset: pl.DataFrame, chunk_size: int) -&gt; int:\n        \"\"\"Simulate chunk processing and return number of chunks.\"\"\"\n        num_chunks = 0\n        dataset_size = len(dataset)\n\n        for start_idx in range(0, dataset_size, chunk_size):\n            end_idx = min(start_idx + chunk_size, dataset_size)\n            chunk = dataset.slice(start_idx, end_idx - start_idx)\n\n            # Simulate basic processing\n            _ = chunk.select(\n                [\n                    pl.col(\"message_text\").str.len_chars().alias(\"text_length\"),\n                    pl.col(\"message_id\"),\n                    pl.col(\"author_id\"),\n                ]\n            )\n\n            num_chunks += 1\n\n            # Periodic cleanup\n            if num_chunks % 10 == 0:\n                gc.collect()\n\n        return num_chunks\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestStressTests.test_large_chunk_memory_stability","title":"<code>test_large_chunk_memory_stability()</code>","text":"<p>Test that large chunks don't cause memory issues.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_large_chunk_memory_stability(self):\n    \"\"\"Test that large chunks don't cause memory issues.\"\"\"\n    # Test with largest possible chunk size\n    large_chunk_size = 500_000  # Maximum allowed\n    dataset = self._create_test_dataset(100_000)  # Smaller than chunk\n\n    memory_manager = MemoryManager()\n    initial_memory = psutil.Process().memory_info().rss / 1024**2\n\n    # Process with large chunk\n    start_time = time.time()\n    self._simulate_chunk_processing(dataset, large_chunk_size)\n    processing_time = time.time() - start_time\n\n    peak_memory = psutil.Process().memory_info().rss / 1024**2\n    memory_increase = peak_memory - initial_memory\n\n    # Should complete successfully\n    assert processing_time &gt; 0\n\n    # Memory usage should be reasonable\n    assert (\n        memory_increase &lt; memory_manager.max_memory_gb * 1024 * 0.8\n    ), f\"Memory usage ({memory_increase:.1f}MB) should be within 80% of limit\"\n</code></pre>"},{"location":"reference/testing/#testing.performance.test_performance_benchmarks.TestStressTests.test_many_small_chunks_efficiency","title":"<code>test_many_small_chunks_efficiency()</code>","text":"<p>Test efficiency with many small chunks.</p> Source code in <code>testing/performance/test_performance_benchmarks.py</code> <pre><code>def test_many_small_chunks_efficiency(self):\n    \"\"\"Test efficiency with many small chunks.\"\"\"\n    dataset = self._create_test_dataset(500_000)\n    small_chunk_size = 10_000  # Many small chunks\n\n    start_time = time.time()\n    num_chunks = self._simulate_chunk_processing(dataset, small_chunk_size)\n    processing_time = time.time() - start_time\n\n    # Should complete in reasonable time\n    assert (\n        processing_time &lt; 60\n    ), f\"Processing took {processing_time:.1f}s, should be &lt; 60s\"\n\n    # Should have expected number of chunks\n    expected_chunks = (len(dataset) + small_chunk_size - 1) // small_chunk_size\n    assert abs(num_chunks - expected_chunks) &lt;= 1\n</code></pre>"},{"location":"reference/testing/#testing.testers","title":"<code>testers</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer","title":"<code>test_primary_analyzer(interface, main, *, input, outputs, params=dict())</code>","text":"<p>Runs the primary analyzer test.</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The interface of the analyzer.</p> required <code>Callable[[PrimaryAnalyzerContext], None]</code> <p>The main function of the analyzer.</p> required <code>TestData</code> <p>The input data.</p> required <code>dict[str, ParamValue]</code> <p>(Optional) The analysis parameters.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The output data, keyed by output ID.</p> required Source code in <code>testing/testers.py</code> <pre><code>@pytest.mark.skip()\ndef test_primary_analyzer(\n    interface: AnalyzerInterface,\n    main: Callable[[PrimaryAnalyzerContext], None],\n    *,\n    input: TestData,\n    outputs: dict[str, TestData],\n    params: dict[str, ParamValue] = dict(),\n):\n    \"\"\"\n    Runs the primary analyzer test.\n\n    Args:\n        interface (AnalyzerInterface): The interface of the analyzer.\n        main (Callable[[PrimaryAnalyzerContext], None]): The main function of the analyzer.\n        input (TestData): The input data.\n        params (dict[str, ParamValue]): (Optional) The analysis parameters.\n        outputs (dict[str, TestData]): The output data, keyed by output ID.\n    \"\"\"\n    with ExitStack() as exit_stack:\n        temp_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_output_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_input_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n\n        input_path = os.path.join(actual_input_dir, \"input.parquet\")\n        input.convert_to_parquet(input_path)\n\n        # Create input_columns mapping from interface and test data semantics\n        from testing.context import TestInputColumnProvider\n\n        input_columns = {}\n        for column_spec in interface.input.columns:\n            analyzer_column_name = column_spec.name\n            # For testing, we assume the user column name matches the analyzer column name\n            user_column_name = analyzer_column_name\n            semantic = input.semantics.get(analyzer_column_name)\n            if semantic:\n                input_columns[analyzer_column_name] = TestInputColumnProvider(\n                    user_column_name=user_column_name, semantic=semantic\n                )\n\n        context = TestPrimaryAnalyzerContext(\n            temp_dir=temp_dir,\n            input_parquet_path=input_path,\n            param_values=params,\n            output_parquet_root_path=actual_output_dir,\n            input_columns=input_columns,\n        )\n        main(context)\n\n        specified_outputs = [output_spec.id for output_spec in interface.outputs]\n        unused_outputs = [\n            output_id\n            for output_id in outputs.keys()\n            if output_id not in specified_outputs\n        ]\n        if unused_outputs:\n            raise ValueError(\n                f\"The test case provided outputs that are not specified in the interface: {unused_outputs}\"\n            )\n\n        has_compared_output = any(\n            outputs.get(output_spec.id) is not None for output_spec in interface.outputs\n        )\n        if not has_compared_output:\n            raise ValueError(\"The test case did not compare any outputs.\")\n\n        for output_spec in interface.outputs:\n            expected_output_data = outputs.get(output_spec.id)\n            if expected_output_data is None:\n                continue\n\n            actual_output_path = context.output_path(output_spec.id)\n\n            expected_output = expected_output_data.load()\n            actual_output = pl.read_parquet(actual_output_path)\n            compare_dfs(actual_output, expected_output)\n</code></pre>"},{"location":"reference/testing/#testing.testers.test_primary_analyzer(interface)","title":"<code>interface</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(main)","title":"<code>main</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(input)","title":"<code>input</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(params)","title":"<code>params</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(outputs)","title":"<code>outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer","title":"<code>test_secondary_analyzer(interface, main, *, primary_params=dict(), primary_outputs, dependency_outputs=dict(), expected_outputs)</code>","text":"<p>Runs the secondary analyzer test.</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The interface of the analyzer.</p> required <code>Callable[[SecondaryAnalyzerInterface], None]</code> <p>The main function of the analyzer.</p> required <code>dict[str, ParamValue]</code> <p>(Optional) The primary analysis parameters.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The primary output data, keyed by output ID.</p> required <code>dict[str, dict[str, TestData]]</code> <p>The dependency output data, keyed by dependency ID and then by output ID.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The expected output data, keyed by output ID.</p> required Source code in <code>testing/testers.py</code> <pre><code>@pytest.mark.skip()\ndef test_secondary_analyzer(\n    interface: AnalyzerInterface,\n    main: Callable[[SecondaryAnalyzerContext], None],\n    *,\n    primary_params: dict[str, ParamValue] = dict(),\n    primary_outputs: dict[str, TestData],\n    dependency_outputs: dict[str, dict[str, TestData]] = dict(),\n    expected_outputs: dict[str, TestData],\n):\n    \"\"\"\n    Runs the secondary analyzer test.\n\n    Args:\n        interface (AnalyzerInterface): The interface of the analyzer.\n        main (Callable[[SecondaryAnalyzerInterface], None]): The main function of the analyzer.\n        primary_params (dict[str, ParamValue]): (Optional) The primary analysis parameters.\n        primary_outputs (dict[str, TestData]): The primary output data, keyed by output ID.\n        dependency_outputs (dict[str, dict[str, TestData]]): The dependency output data, keyed by dependency ID and then by output ID.\n        expected_outputs (dict[str, TestData]): The expected output data, keyed by output ID.\n    \"\"\"\n    with ExitStack() as exit_stack:\n        temp_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_output_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_base_output_dir = exit_stack.enter_context(\n            TemporaryDirectory(delete=True)\n        )\n        actual_dependency_output_dirs = {\n            dependency_id: exit_stack.enter_context(TemporaryDirectory(delete=True))\n            for dependency_id in dependency_outputs.keys()\n        }\n\n        for output_id, output_data in primary_outputs.items():\n            output_data.convert_to_parquet(\n                os.path.join(actual_base_output_dir, f\"{output_id}.parquet\")\n            )\n\n        for dependency_id, dependency_output in dependency_outputs.items():\n            for output_id, output_data in dependency_output.items():\n                output_data.convert_to_parquet(\n                    os.path.join(\n                        actual_dependency_output_dirs[dependency_id],\n                        f\"{output_id}.parquet\",\n                    )\n                )\n\n        context = TestSecondaryAnalyzerContext(\n            temp_dir=temp_dir,\n            primary_param_values=primary_params,\n            primary_output_parquet_paths={\n                output_id: os.path.join(actual_base_output_dir, f\"{output_id}.parquet\")\n                for output_id in primary_outputs.keys()\n            },\n            dependency_output_parquet_paths={\n                dependency_id: {\n                    output_id: os.path.join(\n                        actual_dependency_output_dirs[dependency_id],\n                        f\"{output_id}.parquet\",\n                    )\n                    for output_id in dependency_output.keys()\n                }\n                for dependency_id, dependency_output in dependency_outputs.items()\n            },\n            output_parquet_root_path=actual_output_dir,\n        )\n        main(context)\n\n        specified_outputs = [output_spec.id for output_spec in interface.outputs]\n        unused_outputs = [\n            output_id\n            for output_id in expected_outputs.keys()\n            if output_id not in specified_outputs\n        ]\n        if unused_outputs:\n            raise ValueError(\n                f\"The test case provided outputs that are not specified in the interface: {unused_outputs}\"\n            )\n\n        has_compared_output = any(\n            expected_outputs.get(output_spec.id) is not None\n            for output_spec in interface.outputs\n        )\n        if not has_compared_output:\n            raise ValueError(\"The test case did not compare any outputs.\")\n\n        for output_spec in interface.outputs:\n            expected_output_data = expected_outputs.get(output_spec.id)\n            if expected_output_data is None:\n                continue\n\n            actual_output_path = context.output_path(output_spec.id)\n\n            expected_output = expected_output_data.load()\n            actual_output = pl.read_parquet(actual_output_path)\n            compare_dfs(actual_output, expected_output)\n</code></pre>"},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(interface)","title":"<code>interface</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(main)","title":"<code>main</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(primary_params)","title":"<code>primary_params</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(primary_outputs)","title":"<code>primary_outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(dependency_outputs)","title":"<code>dependency_outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(expected_outputs)","title":"<code>expected_outputs</code>","text":""}]}
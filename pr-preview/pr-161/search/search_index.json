{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#cib-mango-tree-technical-documentation","title":"CIB Mango Tree Technical documentation","text":"<p>Welcome to the technical documentation website of \ud83e\udd6d CIB Mango Tree \ud83e\udd6d, a collaborative and open-source project to develop software that tests for coordinated inauthentic behavior (CIB) in datasets of online activity. </p> <p>This is the technical documentation, for a user-based perspective and project story see: www.cibmangotree.org</p>"},{"location":"license/","title":"License","text":"<pre><code># PolyForm Noncommercial License 1.0.0\n\n&lt;https://polyformproject.org/licenses/noncommercial/1.0.0&gt;\n\n## Acceptance\n\nIn order to get any license under these terms, you must agree\nto them as both strict obligations and conditions to all\nyour licenses.\n\n## Copyright License\n\nThe licensor grants you a copyright license for the\nsoftware to do everything you might do with the software\nthat would otherwise infringe the licensor's copyright\nin it for any permitted purpose.  However, you may\nonly distribute the software according to [Distribution\nLicense](#distribution-license) and make changes or new works\nbased on the software according to [Changes and New Works\nLicense](#changes-and-new-works-license).\n\n## Distribution License\n\nThe licensor grants you an additional copyright license\nto distribute copies of the software.  Your license\nto distribute covers distributing the software with\nchanges and new works permitted by [Changes and New Works\nLicense](#changes-and-new-works-license).\n\n## Notices\n\nYou must ensure that anyone who gets a copy of any part of\nthe software from you also gets a copy of these terms or the\nURL for them above, as well as copies of any plain-text lines\nbeginning with `Required Notice:` that the licensor provided\nwith the software.  For example:\n\n&gt; Required Notice: Copyright Yoyodyne, Inc. (http://example.com)\n\n## Changes and New Works License\n\nThe licensor grants you an additional copyright license to\nmake changes and new works based on the software for any\npermitted purpose.\n\n## Patent License\n\nThe licensor grants you a patent license for the software that\ncovers patent claims the licensor can license, or becomes able\nto license, that you would infringe by using the software.\n\n## Noncommercial Purposes\n\nAny noncommercial purpose is a permitted purpose.\n\n## Personal Uses\n\nPersonal use for research, experiment, and testing for\nthe benefit of public knowledge, personal study, private\nentertainment, hobby projects, amateur pursuits, or religious\nobservance, without any anticipated commercial application,\nis use for a permitted purpose.\n\n## Noncommercial Organizations\n\nUse by any charitable organization, educational institution,\npublic research organization, public safety or health\norganization, environmental protection organization,\nor government institution is use for a permitted purpose\nregardless of the source of funding or obligations resulting\nfrom the funding.\n\n## Fair Use\n\nYou may have \"fair use\" rights for the software under the\nlaw. These terms do not limit them.\n\n## No Other Rights\n\nThese terms do not allow you to sublicense or transfer any of\nyour licenses to anyone else, or prevent the licensor from\ngranting licenses to anyone else.  These terms do not imply\nany other licenses.\n\n## Patent Defense\n\nIf you make any written claim that the software infringes or\ncontributes to infringement of any patent, your patent license\nfor the software granted under these terms ends immediately. If\nyour company makes such a claim, your patent license ends\nimmediately for work on behalf of your company.\n\n## Violations\n\nThe first time you are notified in writing that you have\nviolated any of these terms, or done anything with the software\nnot covered by your licenses, your licenses can nonetheless\ncontinue if you come into full compliance with these terms,\nand take practical steps to correct past violations, within\n32 days of receiving notice.  Otherwise, all your licenses\nend immediately.\n\n## No Liability\n\n***As far as the law allows, the software comes as is, without\nany warranty or condition, and the licensor will not be liable\nto you for any damages arising out of these terms or the use\nor nature of the software, under any kind of legal claim.***\n\n## Definitions\n\nThe **licensor** is the individual or entity offering these\nterms, and the **software** is the software the licensor makes\navailable under these terms.\n\n**You** refers to the individual or entity agreeing to these\nterms.\n\n**Your company** is any legal entity, sole proprietorship,\nor other kind of organization that you work for, plus all\norganizations that have control over, are under the control of,\nor are under common control with that organization.  **Control**\nmeans ownership of substantially all the assets of an entity,\nor the power to direct its management and policies by vote,\ncontract, or otherwise.  Control can be direct or indirect.\n\n**Your licenses** are all the licenses granted to you for the\nsoftware under these terms.\n\n**Use** means anything you do with the software requiring one\nof your licenses.\n</code></pre>"},{"location":"guides/analyzers/","title":"Implementing Analyzers","text":""},{"location":"guides/analyzers/#analyzers-guide","title":"Analyzers Guide","text":"<p>Analyzers are the core data processing components of the platform. They follow a three-tier architecture that separates data processing, analysis, and presentation concerns.</p>"},{"location":"guides/analyzers/#architecture-overview","title":"Architecture Overview","text":"<p>The analyzer system consists of three types of components:</p> <ol> <li>Primary Analyzer: Performs the core data analysis and outputs structured results</li> <li>Secondary Analyzer: Processes primary analyzer outputs for specific use cases or exports</li> <li>Web Presenter: Creates interactive dashboards and visualizations</li> </ol> <p>This separation allows for:</p> <ul> <li>Reusable analysis logic</li> <li>Multiple presentation formats for the same analysis</li> <li>Collaborative development where different contributors can focus on different layers</li> </ul>"},{"location":"guides/analyzers/#primary-analyzers","title":"Primary Analyzers","text":"<p>Primary analyzers perform the core data analysis. They read user input data, process it according to their algorithm, and output structured results in parquet format.</p>"},{"location":"guides/analyzers/#interface-definition","title":"Interface Definition","text":"<p>Every primary analyzer must define an interface that specifies:</p> <ul> <li>Input columns required from the user</li> <li>Parameters the analyzer accepts</li> <li>Output tables the analyzer produces</li> </ul> <pre><code>from analyzer_interface import (\n    AnalyzerInput,\n    AnalyzerInterface, \n    AnalyzerOutput,\n    AnalyzerParam,\n    InputColumn,\n    OutputColumn,\n    IntegerParam\n)\n\ninterface = AnalyzerInterface(\n    id=\"example_analyzer\",  # Must be globally unique\n    version=\"0.1.0\",\n    name=\"Example Analyzer\",\n    short_description=\"Counts characters in messages\",\n    long_description=\"\"\"\nThis analyzer demonstrates the basic structure by counting \ncharacters in each message and marking long messages.\n    \"\"\",\n    input=AnalyzerInput(\n        columns=[\n            InputColumn(\n                name=\"message_id\",\n                human_readable_name=\"Unique Message ID\", \n                data_type=\"identifier\",\n                description=\"The unique identifier of the message\",\n                name_hints=[\"post\", \"message\", \"tweet\", \"id\"]\n            ),\n            InputColumn(\n                name=\"message_text\",\n                human_readable_name=\"Message Text\",\n                data_type=\"text\", \n                description=\"The text content of the message\",\n                name_hints=[\"message\", \"text\", \"content\", \"body\"]\n            )\n        ]\n    ),\n    params=[\n        AnalyzerParam(\n            id=\"fudge_factor\",\n            human_readable_name=\"Character Count Adjustment\",\n            description=\"Adds to the character count for testing purposes\",\n            type=IntegerParam(min=-1000, max=1000),\n            default=0\n        )\n    ],\n    outputs=[\n        AnalyzerOutput(\n            id=\"character_count\",\n            name=\"Character Count Per Message\", \n            internal=True,  # Not shown in export list\n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\")\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/analyzers/#implementation","title":"Implementation","text":"<p>The main function receives a context object with access to input data and output paths:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import PrimaryAnalyzerContext\nfrom terminal_tools import ProgressReporter\n\ndef main(context: PrimaryAnalyzerContext):\n    # Read and preprocess input data\n    input_reader = context.input()\n    df_input = input_reader.preprocess(pl.read_parquet(input_reader.parquet_path))\n\n    # Access parameters\n    fudge_factor = context.params.get(\"fudge_factor\")\n    assert isinstance(fudge_factor, int), \"Fudge factor must be an integer\"\n\n    # Perform analysis with progress reporting\n    with ProgressReporter(\"Counting characters\") as progress:\n        df_count = df_input.select(\n            pl.col(\"message_id\"),\n            pl.col(\"message_text\")\n            .str.len_chars()\n            .add(fudge_factor)\n            .alias(\"character_count\")\n        )\n        progress.update(1.0)\n\n    # Write output to specified path\n    df_count.write_parquet(context.output(\"character_count\").parquet_path)\n</code></pre>"},{"location":"guides/analyzers/#declaration","title":"Declaration","text":"<p>Finally, create the analyzer declaration:</p> <pre><code>from analyzer_interface import AnalyzerDeclaration\nfrom .interface import interface\nfrom .main import main\n\nexample_analyzer = AnalyzerDeclaration(\n    interface=interface,\n    main=main,\n    is_distributed=False  # Set to True for production analyzers\n)\n</code></pre>"},{"location":"guides/analyzers/#secondary-analyzers","title":"Secondary Analyzers","text":"<p>Secondary analyzers process the output of primary analyzers to create user-friendly exports or perform additional analysis.</p>"},{"location":"guides/analyzers/#interface-definition_1","title":"Interface Definition","text":"<p>Secondary analyzers specify their base primary analyzer and their own outputs:</p> <pre><code>from analyzer_interface import AnalyzerOutput, OutputColumn, SecondaryAnalyzerInterface\nfrom ..example_base.interface import interface as example_base\n\ninterface = SecondaryAnalyzerInterface(\n    id=\"example_report\",\n    version=\"0.1.0\", \n    name=\"Example Report\",\n    short_description=\"Adds 'is_long' flag to character count analysis\",\n    base_analyzer=example_base,  # Reference to primary analyzer\n    outputs=[\n        AnalyzerOutput(\n            id=\"example_report\",\n            name=\"Example Report\", \n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\"),\n                OutputColumn(name=\"is_long\", data_type=\"boolean\")  # New column\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/analyzers/#implementation_1","title":"Implementation","text":"<p>Secondary analyzers read primary outputs and create enhanced results:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import SecondaryAnalyzerContext\n\ndef main(context: SecondaryAnalyzerContext):\n    # Read primary analyzer output\n    df_character_count = pl.read_parquet(\n        context.base.table(\"character_count\").parquet_path\n    )\n\n    # Add derived columns\n    df_export = df_character_count.with_columns(\n        pl.col(\"character_count\").gt(100).alias(\"is_long\")\n    )\n\n    # Access primary analyzer parameters if needed\n    fudge_factor = context.base_params.get(\"fudge_factor\")\n\n    # Write enhanced output\n    df_export.write_parquet(context.output(\"example_report\").parquet_path)\n</code></pre>"},{"location":"guides/analyzers/#web-presenters","title":"Web Presenters","text":"<p>Web presenters create interactive dashboards using either Dash or Shiny frameworks.</p>"},{"location":"guides/analyzers/#interface-definition_2","title":"Interface Definition","text":"<pre><code>from analyzer_interface import WebPresenterInterface\nfrom ..example_base import interface as example_base\nfrom ..example_report import interface as example_report\n\ninterface = WebPresenterInterface(\n    id=\"example_web\", \n    version=\"0.1.0\",\n    name=\"Message Length Histogram\",\n    short_description=\"Shows distribution of message lengths\",\n    base_analyzer=example_base,\n    depends_on=[example_report]  # Secondary analyzers used\n)\n</code></pre>"},{"location":"guides/analyzers/#shiny-implementation","title":"Shiny Implementation","text":"<p>For more interactive dashboards:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Define UI components\n    analysis_panel = ui.card(\n        ui.card_header(\"Character Count Analysis\"),\n        ui.input_checkbox(\"show_details\", \"Show detailed view\", value=False),\n        output_widget(\"histogram\", height=\"400px\")\n    )\n\n    def server(input, output, session):\n        @render_widget\n        def histogram():\n            # Create interactive plot based on inputs\n            show_details = input.show_details()\n            # ... create plotly figure ...\n            return fig\n\n        @render.text\n        def summary():\n            return f\"Total messages: {len(df)}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", analysis_panel)\n        )\n    )\n</code></pre>"},{"location":"guides/analyzers/#api-factory-for-react-dashboards","title":"API Factory for React Dashboards","text":"<p>Web presenters can also implement an <code>api_factory</code> function to provide structured data for React-based frontends through REST API endpoints:</p> <pre><code>from ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Provides structured data for React dashboards via API endpoints.\n\n    Args:\n        context: WebPresenterContext with access to analyzer outputs\n        options: Optional parameters from API requests (filters, etc.)\n\n    Returns:\n        Dict with presenter metadata and processed data arrays\n    \"\"\"\n    # Extract API options/filters\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load data\n    data_frame = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Apply filters if provided\n    if filter_value:\n        # Apply filtering logic based on the filter_value\n        data_frame = data_frame.filter(pl.col(\"message_text\").str.contains(filter_value))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model[\"figure_type\"] = \"histogram\"\n    presenter_model[\"axis\"] = {\n        \"x\": {\"label\": \"Message Character Count\", \"value\": \"message_character_count\"},\n        \"y\": {\"label\": \"Number of Messages\", \"value\": \"number_of_messages\"}\n    }\n\n    # Add data arrays for the frontend\n    presenter_model[\"x\"] = data_frame[\"character_count\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/analyzers/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple outputs, return a dictionary with different data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load different data sources\n    df_stats = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_STATS).parquet_path\n    )\n    df_full = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_FULL).parquet_path\n    )\n\n    # Apply filtering to both datasets\n    if filter_value:\n        matcher = create_word_matcher(filter_value, pl.col(COL_NGRAM_WORDS))\n        if matcher is not None:\n            df_stats = df_stats.filter(matcher)\n            df_full = df_full.filter(matcher)\n\n    # Create separate presenter models for each output\n    stats_model = context.web_presenter.model_dump()\n    full_model = context.web_presenter.model_dump()\n\n    # Configure stats view\n    stats_model.update({\n        \"figure_type\": \"scatter\",\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"amplification_factor\": \"N-grams higher up are repeated more times...\"\n        },\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"x\": df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT].to_list(),\n        \"y\": {\n            \"total_repetition\": df_stats[COL_NGRAM_TOTAL_REPS].to_list(),\n            \"amplification_factor\": (\n                df_stats[COL_NGRAM_TOTAL_REPS] / \n                df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT]\n            ).to_list()\n        },\n        \"ngrams\": df_stats[COL_NGRAM_WORDS].to_list()\n    })\n\n    # Configure full data view  \n    full_model.update({\n        \"figure_type\": \"scatter\",\n        \"ids\": df_full[COL_NGRAM_ID].to_list(),\n        \"timestamps\": df_full[COL_MESSAGE_TIMESTAMP].to_list(),\n        \"messages\": df_full[COL_MESSAGE_TEXT].to_list(),\n        \"users\": df_full[COL_AUTHOR_ID].to_list(),\n        # ... additional fields for detailed view\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": OUTPUT_NGRAM_STATS,\n            OUTPUT_NGRAM_STATS: pop_unnecessary_fields(stats_model),\n            OUTPUT_NGRAM_FULL: pop_unnecessary_fields(full_model)\n        }\n    )\n</code></pre>"},{"location":"guides/analyzers/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through REST endpoints:</p> <ul> <li><code>GET /api/presenters</code> - List all presenter data</li> <li><code>GET /api/presenters/{id}</code> - Get specific presenter data</li> <li><code>GET /api/presenters/{id}/download/{format}</code> - Download data as CSV/JSON/Excel</li> </ul> <p>Query parameters:</p> <ul> <li><code>output</code> - Specify which output to return (for multi-output presenters)</li> <li><code>filter_field</code> &amp; <code>filter_value</code> - Apply filtering</li> <li><code>matcher</code> - Text matching filter (passed to api_factory options)</li> </ul> <p>Example API usage:</p> <pre><code># Get basic presenter data\ncurl \"/api/presenters/ngram_repetition_by_poster\"\n\n# Get filtered data\ncurl \"/api/presenters/ngram_repetition_by_poster?filter_value=climate&amp;matcher=climate\"\n\n# Get specific output\ncurl \"/api/presenters/ngram_repetition_by_poster?output=ngram_full\"\n\n# Download as CSV\ncurl \"/api/presenters/ngram_repetition_by_poster/download/csv\"\n</code></pre>"},{"location":"guides/analyzers/#testing-analyzers","title":"Testing Analyzers","text":""},{"location":"guides/analyzers/#testing-primary-analyzers","title":"Testing Primary Analyzers","text":"<pre><code>from testing import CsvTestData, test_primary_analyzer\nfrom .interface import interface\nfrom .main import main\n\ndef test_example_analyzer():\n    test_primary_analyzer(\n        interface=interface,\n        main=main,\n        input=CsvTestData(\n            \"test_input.csv\",\n            semantics={\"message_id\": identifier}\n        ),\n        params={\"fudge_factor\": 10},\n        outputs={\n            \"character_count\": CsvTestData(\"expected_output.csv\")\n        }\n    )\n</code></pre>"},{"location":"guides/analyzers/#testing-secondary-analyzers","title":"Testing Secondary Analyzers","text":"<pre><code>from testing import test_secondary_analyzer, ParquetTestData\n\ndef test_example_report():\n    test_secondary_analyzer(\n        interface=interface,\n        main=main,\n        primary_params={\"fudge_factor\": 10},\n        primary_outputs={\n            \"character_count\": ParquetTestData(\"primary_output.parquet\")\n        },\n        expected_outputs={\n            \"example_report\": ParquetTestData(\"expected_report.parquet\")\n        }\n    )\n</code></pre>"},{"location":"guides/analyzers/#best-practices","title":"Best Practices","text":""},{"location":"guides/analyzers/#data-processing","title":"Data Processing","text":"<ul> <li>Always call <code>input_reader.preprocess()</code> on input data in primary analyzers</li> <li>Use <code>ProgressReporter</code> for long-running operations</li> <li>Handle missing or null data gracefully</li> <li>Use appropriate data types (avoid defaulting to small integer types)</li> </ul>"},{"location":"guides/analyzers/#interface-design","title":"Interface Design","text":"<ul> <li>Choose descriptive, globally unique IDs</li> <li>Provide comprehensive <code>name_hints</code> for better column matching</li> <li>Mark internal outputs that users shouldn't see directly</li> <li>Include helpful parameter descriptions and validation</li> </ul>"},{"location":"guides/analyzers/#performance","title":"Performance","text":"<ul> <li>Use lazy evaluation with Polars when possible</li> <li>Process data in chunks for large datasets</li> <li>Consider memory usage when designing algorithms</li> <li>Use appropriate file formats (parquet for structured data)</li> </ul>"},{"location":"guides/analyzers/#error-handling","title":"Error Handling","text":"<ul> <li>Validate parameters and input data</li> <li>Provide meaningful error messages</li> <li>Handle edge cases (empty datasets, missing columns)</li> <li>Use assertions for internal consistency checks</li> </ul>"},{"location":"guides/analyzers/#adding-to-the-suite","title":"Adding to the Suite","text":"<p>Register all analyzers in <code>analyzers/__init__.py</code>:</p> <pre><code>from analyzer_interface import AnalyzerSuite\nfrom .example.example_base import example_base\nfrom .example.example_report import example_report  \nfrom .example.example_web import example_web\n\nsuite = AnalyzerSuite(\n    all_analyzers=[\n        example_base,\n        example_report, \n        example_web,\n        # ... other analyzers\n    ]\n)\n</code></pre> <p>This creates a complete analysis pipeline that users can run through the application interface, from data input through interactive visualization.</p>"},{"location":"guides/analyzers/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it would be a good idea to review the sections for each domain. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/dashboards/react/","title":"React","text":""},{"location":"guides/dashboards/react/#react-dashboards-guide","title":"React Dashboards Guide","text":"<p>Web presenters can create modern, client-side dashboards using React for rich interactivity and responsive user experiences. React dashboards consume data through REST APIs and provide smooth, app-like interfaces.</p>"},{"location":"guides/dashboards/react/#overview","title":"Overview","text":"<p>React dashboards in this platform provide:</p> <ul> <li>Client-side rendering: Fast, responsive user interfaces</li> <li>API-driven: Clean separation between data and presentation</li> <li>Modern UI components: Built with shadcn/ui and Tailwind CSS</li> <li>Rich visualizations: Interactive charts with Deck.gl and Visx</li> <li>Real-time interactions: Immediate feedback and smooth animations</li> </ul>"},{"location":"guides/dashboards/react/#architecture","title":"Architecture","text":"<p>React dashboards follow a three-tier architecture:</p> <ol> <li>Web Presenter (Python): Implements <code>api_factory</code> to serve structured data via REST API</li> <li>API Layer: Automatically generated endpoints that serve presenter data as JSON</li> <li>React Frontend: TypeScript components that consume the API and render interactive UI</li> </ol> <pre><code>graph TB\n    A[Analyzer Data] --&gt; B[Web Presenter api_factory]\n    B --&gt; C[REST API Endpoints]\n    C --&gt; D[React Components]\n    D --&gt; E[Interactive Dashboard]\n</code></pre>"},{"location":"guides/dashboards/react/#setting-up-api-factory","title":"Setting Up API Factory","text":"<p>The <code>api_factory</code> function transforms analyzer outputs into structured data for React consumption:</p>"},{"location":"guides/dashboards/react/#basic-api-factory","title":"Basic API Factory","text":"<pre><code>from typing import Optional, Any\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext\nfrom ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Transform analyzer data for React dashboard consumption.\n\n    Args:\n        context: Access to analyzer outputs and metadata\n        options: Query parameters from API requests (filters, pagination, etc.)\n\n    Returns:\n        FactoryOutputContext with API-formatted data\n    \"\"\"\n    # Extract API options\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    matcher = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"main_output\").parquet_path)\n\n    # Apply filtering based on API parameters\n    if filter_value:\n        df = df.filter(pl.col(\"category\") == filter_value)\n\n    if matcher:\n        df = df.filter(pl.col(\"text_field\").str.contains(matcher, literal=False))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model.update({\n        \"figure_type\": \"scatter\",  # histogram, bar, scatter\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Message Count\", \"value\": \"message_count\"}\n        },\n        \"explanation\": {\n            \"main_view\": \"This chart shows the relationship between users and messages...\"\n        }\n    })\n\n    # Add data arrays for visualization\n    presenter_model[\"x\"] = df[\"x_column\"].to_list()\n    presenter_model[\"y\"] = df[\"y_column\"].to_list()\n    presenter_model[\"labels\"] = df[\"label_column\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/dashboards/react/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Determine which output to return\n    output_type = options.get(\"output\", \"default\") if options else \"default\"\n\n    # Load different datasets\n    df_summary = pl.read_parquet(\n        context.base.table(\"summary_stats\").parquet_path\n    )\n    df_details = pl.read_parquet(\n        context.dependency(detail_analyzer).table(\"full_details\").parquet_path\n    )\n\n    # Apply common filtering\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    if filter_value:\n        df_summary = df_summary.filter(pl.col(\"category\") == filter_value)\n        df_details = df_details.filter(pl.col(\"category\") == filter_value)\n\n    # Create different presenter models for each output\n    base_model = context.web_presenter.model_dump()\n\n    summary_model = base_model.copy()\n    summary_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"Users\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Messages\", \"value\": \"message_count\"}\n        },\n        \"x\": df_summary[\"user_count\"].to_list(),\n        \"y\": df_summary[\"message_count\"].to_list(),\n        \"labels\": df_summary[\"category\"].to_list()\n    })\n\n    details_model = base_model.copy()\n    details_model.update({\n        \"figure_type\": \"table\",\n        \"columns\": [\"user_id\", \"message_text\", \"timestamp\", \"category\"],\n        \"data\": df_details.to_dicts()\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": \"summary\",\n            \"summary\": pop_unnecessary_fields(summary_model),\n            \"details\": pop_unnecessary_fields(details_model)\n        }\n    )\n</code></pre>"},{"location":"guides/dashboards/react/#complex-filtering-and-search","title":"Complex Filtering and Search","text":"<pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Load base data\n    df = pl.read_parquet(context.base.table(\"ngram_analysis\").parquet_path)\n\n    # Extract search/filter parameters\n    search_term = options.get(\"matcher\", \"\") if options else \"\"\n    date_range = options.get(\"date_range\") if options else None\n    min_frequency = options.get(\"min_frequency\", 0) if options else 0\n\n    # Apply filters progressively\n    if search_term:\n        # Create word matcher for ngram search\n        search_filter = pl.col(\"ngram_text\").str.contains(\n            search_term.lower(), literal=False\n        )\n        df = df.filter(search_filter)\n\n    if date_range:\n        start_date, end_date = date_range.split(\",\")\n        df = df.filter(\n            pl.col(\"timestamp\").is_between(\n                pl.datetime(start_date), \n                pl.datetime(end_date)\n            )\n        )\n\n    if min_frequency &gt; 0:\n        df = df.filter(pl.col(\"frequency\") &gt;= min_frequency)\n\n    # Sort and limit results for performance\n    df = df.sort(\"frequency\", descending=True).head(1000)\n\n    # Build API response\n    presenter_model = context.web_presenter.model_dump()\n    presenter_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"User Repetition\", \"value\": \"user_repetition\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"user_repetition\": \"N-grams higher up show higher amplification...\"\n        },\n        # Data for visualization\n        \"x\": df[\"user_count\"].to_list(),\n        \"y\": df[\"total_count\"].to_list(),\n        \"ngrams\": df[\"ngram_text\"].to_list(),\n        \"frequencies\": df[\"frequency\"].to_list(),\n        \"rankings\": list(range(1, len(df) + 1))\n    })\n\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/dashboards/react/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through these REST endpoints:</p>"},{"location":"guides/dashboards/react/#standard-endpoints","title":"Standard Endpoints","text":"<pre><code># List all presenters\nGET /api/presenters\n\n# Get specific presenter data  \nGET /api/presenters/{presenter_id}\n\n# Get specific output (for multi-output presenters)\nGET /api/presenters/{presenter_id}?output=details\n\n# Apply filters\nGET /api/presenters/{presenter_id}?filter_field=category&amp;filter_value=news\n\n# Search/match text\nGET /api/presenters/{presenter_id}?matcher=climate\n\n# Combine parameters\nGET /api/presenters/{presenter_id}?output=summary&amp;matcher=election&amp;min_frequency=5\n</code></pre>"},{"location":"guides/dashboards/react/#download-endpoints","title":"Download Endpoints","text":"<pre><code># Download as CSV\nGET /api/presenters/{presenter_id}/download/csv\n\n# Download as JSON\nGET /api/presenters/{presenter_id}/download/json\n\n# Download as Excel\nGET /api/presenters/{presenter_id}/download/excel\n\n# Download with filters applied\nGET /api/presenters/{presenter_id}/download/csv?filter_value=news&amp;matcher=climate \n</code></pre>"},{"location":"guides/dashboards/react/#react-component-architecture","title":"React Component Architecture","text":"<p>The React frontend is organized into reusable components that work together to create cohesive dashboards:</p>"},{"location":"guides/dashboards/react/#core-component-structure","title":"Core Component Structure","text":"<pre><code>// Main presenter component\nexport default function NgramScatterPlot({ presenter }: ChartContainerProps&lt;NgramPresenterStats&gt;): ReactElement&lt;FC&gt; {\n    const [searchValue, setSearchValue] = useState&lt;string&gt;('');\n    const [selectedItem, setSelectedItem] = useState&lt;string&gt;('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n\n    // Data fetching and state management\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchValue,\n        output: currentTab\n    });\n\n    // Event handlers\n    const handleSearch = (value: string) =&gt; setSearchValue(value);\n    const handleItemSelect = (item: DataPoint) =&gt; setSelectedItem(item.id);\n    const handleTabChange = (tab: string) =&gt; setCurrentTab(tab);\n\n    return (\n        &lt;div className=\"space-y-6\"&gt;\n            {/* Controls */}\n            &lt;div className=\"flex justify-between items-center\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} onClear={() =&gt; setSearchValue('')} /&gt;\n                &lt;DownloadButton presenterID={presenter.id} /&gt;\n            &lt;/div&gt;\n\n            {/* Tabs for different views */}\n            &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                &lt;TabsList&gt;\n                    &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                    &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification&lt;/TabsTrigger&gt;\n                &lt;/TabsList&gt;\n\n                &lt;TabsContent value=\"total_repetition\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('total_repetition')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n\n                &lt;TabsContent value=\"amplification_factor\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('amplification_factor')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n            &lt;/Tabs&gt;\n\n            {/* Data table */}\n            &lt;DataTable \n                data={data}\n                columns={tableColumns}\n                onRowSelect={handleItemSelect}\n                selectedRows={selectedItem ? [selectedItem] : []}\n            /&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#data-fetching-hooks","title":"Data Fetching Hooks","text":"<p>Custom hooks manage API communication and state:</p> <pre><code>// hooks/usePresenterData.ts\nimport { useState, useEffect } from 'react';\nimport { fetchPresenter } from '@/lib/data/presenters';\n\ninterface UsePresenterDataOptions {\n    matcher?: string;\n    output?: string;\n    filter_field?: string;\n    filter_value?: string;\n    enabled?: boolean;\n}\n\nexport function usePresenterData&lt;T extends Presenter&gt;(\n    presenterId: string, \n    options: UsePresenterDataOptions = {}\n) {\n    const [data, setData] = useState&lt;T | null&gt;(null);\n    const [isLoading, setIsLoading] = useState(false);\n    const [error, setError] = useState&lt;string | null&gt;(null);\n\n    useEffect(() =&gt; {\n        if (!options.enabled &amp;&amp; options.enabled !== undefined) return;\n\n        const controller = new AbortController();\n\n        const loadData = async () =&gt; {\n            setIsLoading(true);\n            setError(null);\n\n            try {\n                const result = await fetchPresenter(\n                    presenterId, \n                    controller.signal, \n                    options\n                );\n\n                if (result) {\n                    setData(result);\n                } else {\n                    setError('Failed to load data');\n                }\n            } catch (err) {\n                if (!controller.signal.aborted) {\n                    setError(err instanceof Error ? err.message : 'Unknown error');\n                }\n            } finally {\n                setIsLoading(false);\n            }\n        };\n\n        loadData();\n\n        return () =&gt; controller.abort();\n    }, [presenterId, options.matcher, options.output, options.filter_value, options.enabled]);\n\n    return { data, isLoading, error };\n}\n</code></pre>"},{"location":"guides/dashboards/react/#chart-components","title":"Chart Components","text":"<p>Interactive visualizations using Deck.gl:</p> <pre><code>// components/charts/scatter.tsx\nimport { useMemo, useRef, useState } from 'react';\nimport { ScatterplotLayer } from '@deck.gl/layers';\nimport { COORDINATE_SYSTEM } from '@deck.gl/core';\nimport DeckGL from '@deck.gl/react';\nimport { AxisLeft, AxisBottom } from '@visx/axis';\nimport useChart from '@/lib/hooks/chart';\n\ninterface ScatterPlotProps {\n    data: Array&lt;DataPoint&gt;;\n    onItemClick?: (item: PickingInfo&lt;DataPoint&gt;) =&gt; void;\n    tooltip: TooltipFunction&lt;DataPoint&gt;;\n    darkMode?: boolean;\n    dimensions?: Dimensions;\n}\n\nexport default function ScatterPlot({\n    data,\n    onItemClick,\n    tooltip,\n    darkMode = false,\n    dimensions = { width: 800, height: 600, margins: { top: 20, right: 40, bottom: 40, left: 60 }}\n}: ScatterPlotProps) {\n    const deckRef = useRef&lt;DeckGLRef | null&gt;(null);\n    const [deckInstance, setDeckInstance] = useState&lt;Deck | null&gt;(null);\n\n    // Custom hook handles coordinate transformation and scaling\n    const { data: plotData, deckProps, axis, viewport } = useChart(\n        data,\n        tooltip,\n        deckInstance,\n        true, // resetZoomOnChange\n        { x: { type: 'log', show: true }, y: { type: 'log', show: true } },\n        dimensions\n    );\n\n    // Create Deck.gl layers\n    const layers = useMemo(() =&gt; [\n        new ScatterplotLayer({\n            id: 'scatter-points',\n            data: plotData,\n            pickable: true,\n            opacity: 0.8,\n            stroked: false,\n            filled: true,\n            radiusScale: 6,\n            radiusMinPixels: 2,\n            radiusMaxPixels: 8,\n            coordinateSystem: COORDINATE_SYSTEM.CARTESIAN,\n            getPosition: (d: any) =&gt; d.position,\n            getFillColor: (d: any) =&gt; d.color,\n            updateTriggers: {\n                getFillColor: [darkMode, viewport.viewState.zoom]\n            },\n            transitions: {\n                getPosition: { duration: 300, type: 'spring' },\n                getFillColor: { duration: 200 }\n            }\n        })\n    ], [plotData, darkMode, viewport.viewState.zoom]);\n\n    return (\n        &lt;div className=\"relative\"&gt;\n            {/* Zoom controls */}\n            &lt;div className=\"absolute top-4 right-4 z-10\"&gt;\n                &lt;ToolBox \n                    features={['zoom', 'restore']}\n                    zoomIncrement={viewport.hooks.increment}\n                    zoomDecrement={viewport.hooks.decrement}\n                    zoomReset={viewport.hooks.reset}\n                /&gt;\n            &lt;/div&gt;\n\n            {/* Main chart area */}\n            &lt;div style={{ position: 'relative', width: dimensions.width, height: dimensions.height }}&gt;\n                &lt;DeckGL\n                    ref={deckRef}\n                    {...deckProps}\n                    layers={layers}\n                    onClick={onItemClick}\n                    onAfterRender={() =&gt; {\n                        if (deckRef.current?.deck &amp;&amp; !deckInstance) {\n                            setDeckInstance(deckRef.current.deck);\n                        }\n                    }}\n                /&gt;\n\n                {/* Axes overlay */}\n                &lt;svg width={dimensions.width} height={dimensions.height}&gt;\n                    &lt;AxisBottom\n                        scale={axis.x.scale}\n                        top={dimensions.height - dimensions.margins.bottom}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'middle'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                    &lt;AxisLeft\n                        scale={axis.y.scale}\n                        left={dimensions.margins.left}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'end'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                &lt;/svg&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#interactive-data-tables","title":"Interactive Data Tables","text":"<p>Rich data tables with selection and filtering:</p> <pre><code>// components/data_table.tsx\nimport { useMemo, useCallback, useState, useEffect } from 'react';\nimport { DataEditor, GridCellKind, CompactSelection } from '@glideapps/glide-data-grid';\n\ninterface DataTableProps&lt;T extends BaseRow&gt; {\n    data: Array&lt;T&gt;;\n    columns: Array&lt;GridColumn&gt;;\n    onRowSelect?: (item: T | null, selection?: GridSelection) =&gt; void;\n    selectedRows?: CompactSelection;\n    darkMode?: boolean;\n    theme?: Partial&lt;Theme&gt;;\n}\n\nexport default function DataTable&lt;T extends BaseRow&gt;({\n    data,\n    columns,\n    onRowSelect,\n    selectedRows,\n    darkMode = false,\n    theme\n}: DataTableProps&lt;T&gt;) {\n    const [gridSelection, setGridSelection] = useState&lt;GridSelection&gt;({\n        columns: CompactSelection.empty(),\n        rows: selectedRows ?? CompactSelection.empty()\n    });\n\n    // Column mapping for data access\n    const columnIds = useMemo(() =&gt; \n        columns.map(col =&gt; col.id).filter(Boolean) as string[]\n    , [columns]);\n\n    // Cell content renderer\n    const getCellContent = useCallback(([col, row]: Item): GridCell =&gt; {\n        const item = data[row];\n        const columnId = columnIds[col];\n        const value = item[columnId];\n\n        // Determine cell type based on value\n        let cellType = GridCellKind.Text;\n        if (typeof value === 'number') cellType = GridCellKind.Number;\n        if (typeof value === 'boolean') cellType = GridCellKind.Boolean;\n\n        return {\n            kind: cellType,\n            allowOverlay: false,\n            displayData: String(value ?? ''),\n            data: value\n        };\n    }, [data, columnIds]);\n\n    // Selection handler\n    const handleSelectionChange = useCallback((selection: GridSelection) =&gt; {\n        setGridSelection(selection);\n\n        const selectedRow = selection.rows.first();\n        const item = selectedRow !== undefined ? data[selectedRow] : null;\n\n        onRowSelect?.(item, selection);\n    }, [data, onRowSelect]);\n\n    // Theme configuration\n    const tableTheme = useMemo(() =&gt; {\n        if (theme) return theme;\n\n        return darkMode ? {\n            accentColor: '#8c96ff',\n            textDark: '#ffffff',\n            textMedium: '#b8b8b8',\n            bgCell: '#16161b',\n            bgHeader: '#212121',\n            borderColor: 'rgba(225,225,225,0.2)',\n            fontFamily: 'Inter, sans-serif'\n        } : {};\n    }, [darkMode, theme]);\n\n    // Sync external selection changes\n    useEffect(() =&gt; {\n        if (selectedRows &amp;&amp; !selectedRows.equals(gridSelection.rows)) {\n            setGridSelection(prev =&gt; ({ ...prev, rows: selectedRows }));\n        }\n    }, [selectedRows]);\n\n    return (\n        &lt;DataEditor\n            width=\"100%\"\n            height=\"50rem\"\n            className=\"rounded-md border shadow-md\"\n            columns={columns}\n            rows={data.length}\n            getCellContent={getCellContent}\n            gridSelection={gridSelection}\n            onGridSelectionChange={handleSelectionChange}\n            theme={tableTheme}\n            rowSelect=\"single\"\n            rowMarkers=\"checkbox-visible\"\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#search-and-filtering-components","title":"Search and Filtering Components","text":"<p>Reusable search components with autocomplete:</p> <pre><code>// components/search.tsx\nimport { useState, useEffect, useMemo } from 'react';\nimport { useVirtualizer } from '@tanstack/react-virtual';\nimport { Input } from '@/components/ui/input';\nimport { Button } from '@/components/ui/button';\nimport { X } from 'lucide-react';\n\ninterface SearchBarProps {\n    searchList: Array&lt;string&gt;;\n    onSubmit: (value: string) =&gt; void;\n    onClear?: () =&gt; void;\n    placeholder?: string;\n    maxSuggestions?: number;\n}\n\nexport default function SearchBar({ \n    searchList, \n    onSubmit, \n    onClear, \n    placeholder = \"Search...\",\n    maxSuggestions = 100\n}: SearchBarProps) {\n    const [value, setValue] = useState('');\n    const [showSuggestions, setShowSuggestions] = useState(false);\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    // Filter suggestions based on input\n    const suggestions = useMemo(() =&gt; {\n        if (!value) return [];\n\n        return searchList\n            .filter(item =&gt; item.toLowerCase().includes(value.toLowerCase()))\n            .slice(0, maxSuggestions);\n    }, [value, searchList, maxSuggestions]);\n\n    // Virtual scrolling for large suggestion lists\n    const virtualizer = useVirtualizer({\n        count: suggestions.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 35\n    });\n\n    const handleSubmit = (e: FormEvent) =&gt; {\n        e.preventDefault();\n        onSubmit(value);\n        setShowSuggestions(false);\n    };\n\n    const handleSuggestionClick = (suggestion: string) =&gt; {\n        setValue(suggestion);\n        onSubmit(suggestion);\n        setShowSuggestions(false);\n    };\n\n    const handleClear = () =&gt; {\n        setValue('');\n        onClear?.();\n    };\n\n    // Show/hide suggestions based on focus and value\n    useEffect(() =&gt; {\n        setShowSuggestions(suggestions.length &gt; 0 &amp;&amp; value.length &gt; 0);\n    }, [suggestions.length, value.length]);\n\n    return (\n        &lt;form onSubmit={handleSubmit} className=\"relative\"&gt;\n            &lt;div className=\"flex items-center space-x-2\"&gt;\n                &lt;Input\n                    type=\"text\"\n                    value={value}\n                    onChange={(e) =&gt; setValue(e.target.value)}\n                    placeholder={placeholder}\n                    className=\"w-80\"\n                    onFocus={() =&gt; suggestions.length &gt; 0 &amp;&amp; setShowSuggestions(true)}\n                    onBlur={() =&gt; setTimeout(() =&gt; setShowSuggestions(false), 150)}\n                /&gt;\n                {value &amp;&amp; (\n                    &lt;Button\n                        type=\"button\"\n                        variant=\"ghost\"\n                        size=\"icon\"\n                        onClick={handleClear}\n                    &gt;\n                        &lt;X className=\"h-4 w-4\" /&gt;\n                    &lt;/Button&gt;\n                )}\n            &lt;/div&gt;\n\n            {/* Suggestions dropdown */}\n            {showSuggestions &amp;&amp; (\n                &lt;div\n                    ref={containerRef}\n                    className=\"absolute z-50 w-80 max-h-96 mt-1 bg-white border rounded-md shadow-lg overflow-auto dark:bg-zinc-950 dark:border-zinc-800\"\n                &gt;\n                    &lt;div\n                        style={{ height: virtualizer.getTotalSize() }}\n                        className=\"relative\"\n                    &gt;\n                        {virtualizer.getVirtualItems().map((item) =&gt; (\n                            &lt;div\n                                key={item.key}\n                                className=\"absolute w-full px-3 py-2 cursor-pointer hover:bg-zinc-100 dark:hover:bg-zinc-800\"\n                                style={{\n                                    height: item.size,\n                                    transform: `translateY(${item.start}px)`\n                                }}\n                                onMouseDown={() =&gt; handleSuggestionClick(suggestions[item.index])}\n                            &gt;\n                                {suggestions[item.index]}\n                            &lt;/div&gt;\n                        ))}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            )}\n        &lt;/form&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#download-components","title":"Download Components","text":"<p>Export functionality for data:</p> <pre><code>// components/download.tsx\nimport { useState } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { DropdownMenu, DropdownMenuContent, DropdownMenuItem, DropdownMenuTrigger } from '@/components/ui/dropdown-menu';\nimport { Sheet, Braces, Table, ChevronDown } from 'lucide-react';\nimport { createDownloadLink } from '@/lib/data/download';\n\ninterface DownloadButtonProps {\n    presenterID: string;\n    queryParams?: PresenterQueryParams;\n    label?: string;\n}\n\nexport function DownloadButton({ \n    presenterID, \n    queryParams, \n    label = \"Export\" \n}: DownloadButtonProps) {\n    const [isOpen, setIsOpen] = useState(false);\n\n    const downloadFormats = [\n        { type: 'excel', icon: Sheet, label: 'Excel', extension: '.xlsx' },\n        { type: 'csv', icon: Table, label: 'CSV', extension: '.csv' },\n        { type: 'json', icon: Braces, label: 'JSON', extension: '.json' }\n    ] as const;\n\n    return (\n        &lt;DropdownMenu open={isOpen} onOpenChange={setIsOpen}&gt;\n            &lt;DropdownMenuTrigger asChild&gt;\n                &lt;Button variant=\"outline\" size=\"sm\"&gt;\n                    {label}\n                    &lt;ChevronDown className={`ml-2 h-4 w-4 transition-transform ${isOpen ? 'rotate-180' : ''}`} /&gt;\n                &lt;/Button&gt;\n            &lt;/DropdownMenuTrigger&gt;\n\n            &lt;DropdownMenuContent align=\"end\"&gt;\n                {downloadFormats.map(({ type, icon: Icon, label: formatLabel }) =&gt; (\n                    &lt;DropdownMenuItem key={type} asChild&gt;\n                        &lt;a\n                            href={createDownloadLink(presenterID, type, queryParams)}\n                            target=\"_blank\"\n                            rel=\"noopener noreferrer\"\n                            className=\"flex items-center w-full\"\n                        &gt;\n                            &lt;Icon className=\"mr-2 h-4 w-4\" /&gt;\n                            {formatLabel}\n                        &lt;/a&gt;\n                    &lt;/DropdownMenuItem&gt;\n                ))}\n            &lt;/DropdownMenuContent&gt;\n        &lt;/DropdownMenu&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#state-management","title":"State Management","text":""},{"location":"guides/dashboards/react/#local-component-state","title":"Local Component State","text":"<p>For simple interactions, use React's built-in state:</p> <pre><code>function NgramDashboard({ presenter }: DashboardProps) {\n    // UI state\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedTab, setSelectedTab] = useState&lt;TabType&gt;('overview');\n    const [selectedItems, setSelectedItems] = useState&lt;string[]&gt;([]);\n\n    // Data state with custom hook\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchTerm,\n        output: selectedTab\n    });\n\n    // Derived state\n    const filteredData = useMemo(() =&gt; {\n        if (!data || !searchTerm) return data;\n        return data.filter(item =&gt; \n            item.text.toLowerCase().includes(searchTerm.toLowerCase())\n        );\n    }, [data, searchTerm]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedItems([]); // Clear selection on new search\n    }, []);\n\n    return (\n        // Component JSX\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#global-state-with-zustand","title":"Global State with Zustand","text":"<p>For complex dashboards with shared state:</p> <pre><code>// stores/dashboardStore.ts\nimport { create } from 'zustand';\n\ninterface DashboardState {\n    // Data\n    presenters: PresenterCollection;\n    currentPresenter: Presenter | null;\n\n    // UI state\n    sidebarOpen: boolean;\n    theme: 'light' | 'dark' | 'system';\n\n    // Filters\n    globalFilters: {\n        dateRange?: [Date, Date];\n        categories: string[];\n        searchTerm: string;\n    };\n\n    // Actions\n    setCurrentPresenter: (presenter: Presenter) =&gt; void;\n    updateFilters: (filters: Partial&lt;DashboardState['globalFilters']&gt;) =&gt; void;\n    toggleSidebar: () =&gt; void;\n}\n\nexport const useDashboardStore = create&lt;DashboardState&gt;((set, get) =&gt; ({\n    presenters: [],\n    currentPresenter: null,\n    sidebarOpen: true,\n    theme: 'system',\n    globalFilters: {\n        categories: [],\n        searchTerm: ''\n    },\n\n    setCurrentPresenter: (presenter) =&gt; set({ currentPresenter: presenter }),\n\n    updateFilters: (newFilters) =&gt; set(state =&gt; ({\n        globalFilters: { ...state.globalFilters, ...newFilters }\n    })),\n\n    toggleSidebar: () =&gt; set(state =&gt; ({ sidebarOpen: !state.sidebarOpen }))\n}));\n\n// Usage in components\nfunction Dashboard() {\n    const { currentPresenter, globalFilters, updateFilters } = useDashboardStore();\n\n    const handleSearch = (searchTerm: string) =&gt; {\n        updateFilters({ searchTerm });\n    };\n\n    return (\n        // Dashboard JSX\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#styling-and-theming","title":"Styling and Theming","text":""},{"location":"guides/dashboards/react/#tailwind-css-classes","title":"Tailwind CSS Classes","text":"<p>The project uses Tailwind CSS for utility-first styling:</p> <pre><code>// Layout utilities\n&lt;div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\"&gt;\n    &lt;div className=\"col-span-full lg:col-span-2\"&gt;\n        {/* Main content */}\n    &lt;/div&gt;\n    &lt;div className=\"lg:col-span-1\"&gt;\n        {/* Sidebar */}\n    &lt;/div&gt;\n&lt;/div&gt;\n\n// Component styling\n&lt;Card className=\"p-6 shadow-lg border-zinc-200 dark:border-zinc-800\"&gt;\n    &lt;CardHeader className=\"pb-4\"&gt;\n        &lt;CardTitle className=\"text-lg font-semibold text-zinc-900 dark:text-zinc-100\"&gt;\n            Chart Title\n        &lt;/CardTitle&gt;\n    &lt;/CardHeader&gt;\n    &lt;CardContent&gt;\n        {/* Chart content */}\n    &lt;/CardContent&gt;\n&lt;/Card&gt;\n\n// Interactive states\n&lt;Button \n    variant=\"outline\" \n    className=\"hover:bg-zinc-100 dark:hover:bg-zinc-800 transition-colors\"\n    disabled={isLoading}\n&gt;\n    {isLoading ? &lt;Spinner className=\"mr-2\" /&gt; : null}\n    Load Data\n&lt;/Button&gt;\n</code></pre>"},{"location":"guides/dashboards/react/#dark-mode-support","title":"Dark Mode Support","text":"<p>Dark mode is handled through CSS custom properties and Tailwind's dark variant:</p> <pre><code>// Theme provider context\nexport function ThemeProvider({ children, defaultTheme = \"system\" }) {\n    const [theme, setTheme] = useState&lt;Theme&gt;(defaultTheme);\n\n    useEffect(() =&gt; {\n        const root = window.document.documentElement;\n        root.classList.remove(\"light\", \"dark\");\n\n        if (theme === \"system\") {\n            const systemTheme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches \n                ? \"dark\" : \"light\";\n            root.classList.add(systemTheme);\n        } else {\n            root.classList.add(theme);\n        }\n    }, [theme]);\n\n    return (\n        &lt;ThemeContext.Provider value={{ theme, setTheme }}&gt;\n            {children}\n        &lt;/ThemeContext.Provider&gt;\n    );\n}\n\n// Usage in components\nfunction Chart({ data }: ChartProps) {\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    return (\n        &lt;ScatterPlot \n            data={data}\n            darkMode={isDark}\n            // Colors adapt automatically through Tailwind dark: variants\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#responsive-design","title":"Responsive Design","text":"<p>Components adapt to different screen sizes:</p> <pre><code>&lt;div className=\"space-y-6\"&gt;\n    {/* Mobile-first responsive grid */}\n    &lt;div className=\"grid grid-cols-1 lg:grid-cols-4 gap-4\"&gt;\n        &lt;div className=\"lg:col-span-3\"&gt;\n            {/* Chart takes full width on mobile, 3/4 on desktop */}\n            &lt;ScatterPlot data={data} /&gt;\n        &lt;/div&gt;\n        &lt;div className=\"lg:col-span-1\"&gt;\n            {/* Controls stack below chart on mobile, sidebar on desktop */}\n            &lt;div className=\"space-y-4\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} /&gt;\n                &lt;FilterControls /&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    {/* Table with horizontal scroll on mobile */}\n    &lt;div className=\"overflow-x-auto\"&gt;\n        &lt;DataTable \n            data={data}\n            className=\"min-w-[600px]\" \n        /&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"guides/dashboards/react/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/dashboards/react/#memoization-and-optimization","title":"Memoization and Optimization","text":"<pre><code>// Memoize expensive calculations\nconst processedData = useMemo(() =&gt; {\n    if (!rawData) return [];\n\n    return rawData\n        .filter(item =&gt; item.value &gt; threshold)\n        .sort((a, b) =&gt; b.value - a.value)\n        .slice(0, maxItems);\n}, [rawData, threshold, maxItems]);\n\n// Memoize callback functions\nconst handleItemClick = useCallback((item: DataPoint) =&gt; {\n    setSelectedItem(item);\n    onItemSelect?.(item);\n}, [onItemSelect]);\n\n// Memoize complex components\nconst ChartComponent = memo(({ data, options }: ChartProps) =&gt; {\n    return &lt;ExpensiveChart data={data} options={options} /&gt;;\n});\n</code></pre>"},{"location":"guides/dashboards/react/#virtual-scrolling","title":"Virtual Scrolling","text":"<p>For large datasets, implement virtual scrolling:</p> <pre><code>import { useVirtualizer } from '@tanstack/react-virtual';\n\nfunction VirtualTable({ data }: { data: Array&lt;any&gt; }) {\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    const virtualizer = useVirtualizer({\n        count: data.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 50, // Row height\n        overscan: 10 // Render extra items for smooth scrolling\n    });\n\n    return (\n        &lt;div ref={containerRef} className=\"h-96 overflow-auto\"&gt;\n            &lt;div style={{ height: virtualizer.getTotalSize() }}&gt;\n                {virtualizer.getVirtualItems().map((item) =&gt; (\n                    &lt;div\n                        key={item.key}\n                        style={{\n                            position: 'absolute',\n                            top: 0,\n                            left: 0,\n                            width: '100%',\n                            height: item.size,\n                            transform: `translateY(${item.start}px)`\n                        }}\n                    &gt;\n                        &lt;TableRow data={data[item.index]} /&gt;\n                    &lt;/div&gt;\n                ))}\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#code-splitting","title":"Code Splitting","text":"<p>Split large components with lazy loading:</p> <pre><code>// Lazy load heavy visualization components\nconst AdvancedChart = lazy(() =&gt; import('@/components/charts/advanced-chart'));\nconst ComplexTable = lazy(() =&gt; import('@/components/tables/complex-table'));\n\nfunction Dashboard() {\n    return (\n        &lt;Suspense fallback={&lt;div&gt;Loading chart...&lt;/div&gt;}&gt;\n            &lt;AdvancedChart data={data} /&gt;\n        &lt;/Suspense&gt;\n    );\n}\n</code></pre>"},{"location":"guides/dashboards/react/#testing-react-dashboards","title":"Testing React Dashboards","text":""},{"location":"guides/dashboards/react/#component-testing","title":"Component Testing","text":"<pre><code>// __tests__/components/SearchBar.test.tsx\nimport { render, screen, fireEvent, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport SearchBar from '@/components/search';\n\ndescribe('SearchBar', () =&gt; {\n    const mockSubmit = jest.fn();\n    const mockClear = jest.fn();\n    const searchList = ['apple', 'banana', 'cherry', 'date'];\n\n    beforeEach(() =&gt; {\n        jest.clearAllMocks();\n    });\n\n    test('renders with placeholder', () =&gt; {\n        render(\n            &lt;SearchBar \n                searchList={searchList}\n                onSubmit={mockSubmit}\n                placeholder=\"Search fruits...\"\n            /&gt;\n        );\n\n        expect(screen.getByPlaceholderText('Search fruits...')).toBeInTheDocument();\n    });\n\n    test('shows suggestions when typing', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'a');\n\n        await waitFor(() =&gt; {\n            expect(screen.getByText('apple')).toBeInTheDocument();\n            expect(screen.getByText('banana')).toBeInTheDocument();\n        });\n    });\n\n    test('calls onSubmit when form submitted', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'apple');\n        await user.keyboard('{Enter}');\n\n        expect(mockSubmit).toHaveBeenCalledWith('apple');\n    });\n});\n</code></pre>"},{"location":"guides/dashboards/react/#integration-testing","title":"Integration Testing","text":"<pre><code>// __tests__/integration/Dashboard.test.tsx\nimport { render, screen, waitFor } from '@testing-library/react';\nimport { rest } from 'msw';\nimport { setupServer } from 'msw/node';\nimport Dashboard from '@/components/dashboard';\n\n// Mock API server\nconst server = setupServer(\n    rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n        return res(ctx.json({\n            id: 'test-presenter',\n            name: 'Test Presenter',\n            data: [\n                { x: 1, y: 2, label: 'Point 1' },\n                { x: 3, y: 4, label: 'Point 2' }\n            ]\n        }));\n    })\n);\n\nbeforeAll(() =&gt; server.listen());\nafterEach(() =&gt; server.resetHandlers());\nafterAll(() =&gt; server.close());\n\ntest('loads and displays data', async () =&gt; {\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    // Initially shows loading\n    expect(screen.getByText(/loading/i)).toBeInTheDocument();\n\n    // After API call, shows data\n    await waitFor(() =&gt; {\n        expect(screen.getByText('Test Presenter')).toBeInTheDocument();\n    });\n\n    // Chart renders with data points\n    expect(screen.getByText('Point 1')).toBeInTheDocument();\n    expect(screen.getByText('Point 2')).toBeInTheDocument();\n});\n\ntest('handles API errors gracefully', async () =&gt; {\n    server.use(\n        rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n            return res(ctx.status(500), ctx.json({ error: 'Server error' }));\n        })\n    );\n\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    await waitFor(() =&gt; {\n        expect(screen.getByText(/error loading data/i)).toBeInTheDocument();\n    });\n});\n</code></pre>"},{"location":"guides/dashboards/react/#e2e-testing-with-cypress","title":"E2E Testing with Cypress","text":"<pre><code>// cypress/integration/dashboard.spec.ts\ndescribe('Dashboard Interaction', () =&gt; {\n    beforeEach(() =&gt; {\n        cy.intercept('GET', '/api/presenters/ngram-analysis', { \n            fixture: 'ngram-data.json' \n        }).as('getNgramData');\n\n        cy.visit('/dashboard/ngram-analysis');\n        cy.wait('@getNgramData');\n    });\n\n    it('allows searching and filtering data', () =&gt; {\n        // Search for specific terms\n        cy.get('[data-testid=\"search-input\"]').type('climate');\n        cy.get('[data-testid=\"search-submit\"]').click();\n\n        // Verify results update\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.lessThan', 100);\n        cy.get('[data-testid=\"data-table\"]').should('contain', 'climate');\n\n        // Clear search\n        cy.get('[data-testid=\"search-clear\"]').click();\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.greaterThan', 100);\n    });\n\n    it('supports chart interactions', () =&gt; {\n        // Click on chart point\n        cy.get('[data-testid=\"chart-container\"]').click(300, 200);\n\n        // Verify tooltip appears\n        cy.get('[data-testid=\"tooltip\"]').should('be.visible');\n        cy.get('[data-testid=\"tooltip\"]').should('contain', 'Ranking:');\n\n        // Verify data table selection updates\n        cy.get('[data-testid=\"data-table\"] .selected-row').should('exist');\n    });\n\n    it('downloads data in different formats', () =&gt; {\n        // Open download menu\n        cy.get('[data-testid=\"download-button\"]').click();\n\n        // Download CSV\n        cy.get('[data-testid=\"download-csv\"]').click();\n        cy.readFile('cypress/downloads/data.csv').should('exist');\n\n        // Download JSON\n        cy.get('[data-testid=\"download-button\"]').click();\n        cy.get('[data-testid=\"download-json\"]').click();\n        cy.readFile('cypress/downloads/data.json').should('exist');\n    });\n});\n</code></pre>"},{"location":"guides/dashboards/react/#deployment-and-build-process","title":"Deployment and Build Process","text":""},{"location":"guides/dashboards/react/#production-build","title":"Production Build","text":"<p>The React dashboard builds as static assets:</p> <pre><code># Build for production\nnpm run build\n\n# Outputs to app/web_templates/build/\n# - bundled/ (JS/CSS assets)\n# - manifest.json (asset mapping)\n</code></pre>"},{"location":"guides/dashboards/react/#integration-with-backend","title":"Integration with Backend","text":"<p>The Python backend serves the React app:</p> <pre><code># Backend integration\nfrom pathlib import Path\nimport json\n\n# Load build manifest\nmanifest_path = Path(\"web_templates/build/manifest.json\")\nwith open(manifest_path) as f:\n    manifest = json.load(f)\n\n# Serve React app\n@app.route(\"/\")\ndef dashboard():\n    return render_template(\n        \"index.html\",\n        js_files=get_js_files(manifest),\n        css_files=get_css_files(manifest),\n        project_name=config.PROJECT_NAME\n    )\n</code></pre>"},{"location":"guides/dashboards/react/#environment-configuration","title":"Environment Configuration","text":"<pre><code>// Environment variables for different deployments\nconst config = {\n    API_BASE_URL: process.env.REACT_APP_API_URL || 'http://localhost:8050',\n    ENABLE_DEV_TOOLS: process.env.NODE_ENV === 'development',\n    VERSION: process.env.REACT_APP_VERSION || '1.0.0'\n};\n\n// API client configuration\nconst apiClient = axios.create({\n    baseURL: config.API_BASE_URL,\n    timeout: 30000,\n    headers: {\n        'Content-Type': 'application/json'\n    }\n});\n</code></pre>"},{"location":"guides/dashboards/react/#best-practices","title":"Best Practices","text":""},{"location":"guides/dashboards/react/#component-design","title":"Component Design","text":"<pre><code>// 1. Keep components focused and single-purpose\nfunction ChartControls({ onFilterChange, onExport }: ChartControlsProps): ReactElement&lt;FC&gt; {\n    // Only handle UI controls, delegate data management\n    return (\n        &lt;div className=\"flex gap-4\"&gt;\n            &lt;SearchBar onSubmit={(term) =&gt; onFilterChange({ search: term })} /&gt;\n            &lt;ExportButton onExport={onExport} /&gt;\n        &lt;/div&gt;\n    );\n}\n\n// 2. Use composition over inheritance\nfunction Dashboard({ children }: PropsWithChildren): ReactElement&lt;FC&gt; {\n    return (\n        &lt;div className=\"dashboard-layout\"&gt;\n            &lt;Sidebar /&gt;\n            &lt;main className=\"main-content\"&gt;\n                {children}\n            &lt;/main&gt;\n        &lt;/div&gt;\n    );\n}\n\n// Usage\n&lt;Dashboard&gt;\n    &lt;ChartContainer&gt;\n        &lt;ScatterPlot data={data} /&gt;\n        &lt;DataTable data={data} /&gt;\n    &lt;/ChartContainer&gt;\n&lt;/Dashboard&gt;\n\n// 3. Extract custom hooks for reusable logic\nfunction useChartData(presenterId: string, filters: Filters) {\n    const [data, setData] = useState(null);\n    const [isLoading, setIsLoading] = useState(false);\n\n    useEffect(() =&gt; {\n        // Data fetching logic\n    }, [presenterId, filters]);\n\n    return { data, isLoading, refetch: () =&gt; setData(null) };\n}\n</code></pre>"},{"location":"guides/dashboards/react/#complete-example-ngram-analysis-dashboard","title":"Complete Example: Ngram Analysis Dashboard","text":"<p>Here's a complete example showing all concepts together:</p> <pre><code>// components/ngram-dashboard.tsx\nimport { useState, useEffect, useMemo, useCallback } from 'react';\nimport { useTheme } from '@/components/theme-provider';\nimport { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\nimport { Info } from 'lucide-react';\nimport { Tooltip, TooltipContent, TooltipTrigger } from '@/components/ui/tooltip';\n\nimport ScatterPlot from '@/components/charts/scatter';\nimport DataTable from '@/components/data-table';\nimport SearchBar from '@/components/search';\nimport { DownloadButton } from '@/components/download';\nimport { usePresenterData } from '@/hooks/usePresenterData';\n\ninterface NgramDashboardProps {\n    presenter: NgramPresenter;\n}\n\nexport default function NgramDashboard({ presenter }: NgramDashboardProps) {\n    // State management\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedNgram, setSelectedNgram] = useState('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n    const [selectedRows, setSelectedRows] = useState&lt;CompactSelection&gt;(CompactSelection.empty());\n\n    // Theme\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    // Data fetching\n    const { data: summaryData, isLoading } = usePresenterData(presenter.id, {\n        output: 'summary',\n        matcher: searchTerm\n    });\n\n    const { data: detailData } = usePresenterData(presenter.id, {\n        output: 'details',\n        filter_field: 'ngram',\n        filter_value: selectedNgram\n    }, { enabled: !!selectedNgram });\n\n    // Computed values\n    const currentData = useMemo(() =&gt; {\n        if (!summaryData) return [];\n\n        return summaryData.map((item, index) =&gt; ({\n            ...item,\n            ranking: index + 1,\n            y: currentTab === 'total_repetition' \n                ? item.total_repetition \n                : item.amplification_factor\n        }));\n    }, [summaryData, currentTab]);\n\n    const tableColumns = useMemo(() =&gt; {\n        if (selectedNgram &amp;&amp; detailData) {\n            return [\n                { id: 'ngram', title: 'N-gram', width: 200 },\n                { id: 'user', title: 'User', width: 150 },\n                { id: 'userReps', title: 'User Reps', width: 100 },\n                { id: 'message', title: 'Message', width: 400 },\n                { id: 'timestamp', title: 'Timestamp', width: 200 }\n            ];\n        }\n\n        return [\n            { id: 'ranking', title: 'Rank', width: 80 },\n            { id: 'ngram', title: 'N-gram', width: 300 },\n            { id: 'x', title: 'User Count', width: 120 },\n            { \n                id: 'y', \n                title: currentTab === 'total_repetition' ? 'Total Reps' : 'Amplification',\n                width: 120 \n            }\n        ];\n    }, [selectedNgram, detailData, currentTab]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedNgram(''); // Clear selection when searching\n    }, []);\n\n    const handleSearchClear = useCallback(() =&gt; {\n        setSearchTerm('');\n        setSelectedNgram('');\n    }, []);\n\n    const handleChartClick = useCallback((info: PickingInfo&lt;DataPoint&gt;) =&gt; {\n        if (info.object) {\n            setSelectedNgram(info.object.ngram);\n        }\n    }, []);\n\n    const handleTableSelect = useCallback((item: DataPoint | null, selection?: GridSelection) =&gt; {\n        if (item) {\n            setSelectedNgram(item.ngram);\n        }\n        if (selection) {\n            setSelectedRows(selection.rows);\n        }\n    }, []);\n\n    const handleTabChange = useCallback((tab: string) =&gt; {\n        setCurrentTab(tab as typeof currentTab);\n    }, []);\n\n    // Tooltip formatters\n    const createTooltipFormatter = useCallback((type: string) =&gt; (params: DataPoint) =&gt; `\n        &lt;div class=\"space-y-2\"&gt;\n            &lt;div class=\"font-bold\"&gt;${params.ngram}&lt;/div&gt;\n            &lt;div&gt;Ranking: ${params.ranking}&lt;/div&gt;\n            &lt;div&gt;User Count: ${params.x}&lt;/div&gt;\n            &lt;div&gt;${type === 'total_repetition' ? 'Total Reps' : 'Amplification'}: ${params.y}&lt;/div&gt;\n        &lt;/div&gt;\n    `, []);\n\n    // Loading state\n    if (isLoading) {\n        return (\n            &lt;Card&gt;\n                &lt;CardContent className=\"flex items-center justify-center h-96\"&gt;\n                    &lt;div className=\"text-center\"&gt;\n                        &lt;div className=\"animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4\"&gt;&lt;/div&gt;\n                        &lt;p&gt;Loading dashboard...&lt;/p&gt;\n                    &lt;/div&gt;\n                &lt;/CardContent&gt;\n            &lt;/Card&gt;\n        );\n    }\n\n    return (\n        &lt;Card&gt;\n            &lt;CardContent className=\"space-y-6\"&gt;\n                &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                    &lt;div className=\"flex items-center justify-between\"&gt;\n                        &lt;TabsList&gt;\n                            &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                            &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification Factor&lt;/TabsTrigger&gt;\n                        &lt;/TabsList&gt;\n\n                        &lt;Tooltip&gt;\n                            &lt;TooltipTrigger&gt;\n                                &lt;Info className=\"h-5 w-5 text-zinc-500\" /&gt;\n                            &lt;/TooltipTrigger&gt;\n                            &lt;TooltipContent&gt;\n                                &lt;p className=\"max-w-xs\"&gt;\n                                    {presenter.explanation[currentTab]}\n                                &lt;/p&gt;\n                            &lt;/TooltipContent&gt;\n                        &lt;/Tooltip&gt;\n                    &lt;/div&gt;\n\n                    &lt;TabsContent value=\"total_repetition\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('total_repetition')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n\n                    &lt;TabsContent value=\"amplification_factor\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('amplification_factor')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n                &lt;/Tabs&gt;\n            &lt;/CardContent&gt;\n        &lt;/Card&gt;\n    );\n}\n</code></pre> <p>This comprehensive guide covers all aspects of building React dashboards for the analyzer platform. The combination of TypeScript, modern React patterns, rich UI components, and seamless API integration creates powerful, user-friendly data analysis interfaces that complement the Python-based analyzer pipeline.</p>"},{"location":"guides/dashboards/react/#next-steps","title":"Next Steps","text":"<p>After this section it would be a good idea to review the sections that discuss implementing  Shiny dashboards. Although once you finish reading this it would also be a good idea to review the sections for each domain.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> </ul>"},{"location":"guides/dashboards/shiny/","title":"Shiny","text":""},{"location":"guides/dashboards/shiny/#shiny-dashboards-guide","title":"Shiny Dashboards Guide","text":"<p>Web presenters can create interactive dashboards using Python Shiny for rich server-side interactivity. Shiny dashboards provide immediate reactivity, complex data processing capabilities, and seamless integration with the analyzer pipeline. Which in turn allows developers and data scientists the ability to quickly prototype new analyses.</p>"},{"location":"guides/dashboards/shiny/#overview","title":"Overview","text":"<p>Shiny dashboards are server-rendered applications that provide:</p> <ul> <li>Real-time interactivity: Components update automatically when inputs change</li> <li>Server-side processing: Complex calculations run on the server with full Python ecosystem access</li> <li>Widgets: Built-in components for inputs, outputs, and visualizations</li> <li>Session management: Automatic handling of user sessions and state</li> </ul>"},{"location":"guides/dashboards/shiny/#basic-structure","title":"Basic Structure","text":"<p>Every Shiny web presenter follows this pattern:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\nimport polars as pl\nimport plotly.express as px\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"your_output\").parquet_path)\n\n    # Define UI layout\n    dashboard_ui = ui.card(\n        ui.card_header(\"Your Dashboard Title\"),\n        ui.row(\n            ui.column(4, \n                # Input controls\n                ui.input_selectize(\"category\", \"Select Category\", \n                                 choices=df[\"category\"].unique().to_list()),\n                ui.input_slider(\"threshold\", \"Threshold\", 0, 100, 50)\n            ),\n            ui.column(8,\n                # Output displays\n                output_widget(\"main_plot\", height=\"400px\"),\n                ui.output_text(\"summary_stats\")\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            # Reactive data filtering\n            return df.filter(\n                (pl.col(\"category\") == input.category()) &amp;\n                (pl.col(\"value\") &gt;= input.threshold())\n            )\n\n        @render_widget\n        def main_plot():\n            # Create interactive plot\n            plot_df = filtered_data().to_pandas()\n            fig = px.scatter(plot_df, x=\"x\", y=\"y\", color=\"category\")\n            return fig\n\n        @render.text\n        def summary_stats():\n            data = filtered_data()\n            return f\"Showing {len(data)} items, avg value: {data['value'].mean():.2f}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", dashboard_ui)\n        )\n    )\n</code></pre>"},{"location":"guides/dashboards/shiny/#user-interface-components","title":"User Interface Components","text":""},{"location":"guides/dashboards/shiny/#layout-components","title":"Layout Components","text":"<p>Organize your dashboard with these layout elements:</p> <pre><code># Cards for grouped content\nui.card(\n    ui.card_header(\"Section Title\"),\n    ui.card_body(\"Content goes here\")\n)\n\n# Grid layouts\nui.row(\n    ui.column(6, \"Left column\"),\n    ui.column(6, \"Right column\")\n)\n\n# Navigation\nui.navset_tab(\n    ui.nav_panel(\"Tab 1\", \"Content 1\"),\n    ui.nav_panel(\"Tab 2\", \"Content 2\")\n)\n\n# Sidebars\nui.sidebar(\n    \"Sidebar content\",\n    open=\"open\"  # or \"closed\"\n)\n</code></pre>"},{"location":"guides/dashboards/shiny/#input-controls","title":"Input Controls","text":"<p>Collect user input with various widgets:</p> <pre><code># Text inputs\nui.input_text(\"text_id\", \"Label\", value=\"default\")\nui.input_text_area(\"textarea_id\", \"Description\", rows=3)\n\n# Numeric inputs\nui.input_numeric(\"number_id\", \"Number\", value=10, min=0, max=100)\nui.input_slider(\"slider_id\", \"Range\", 0, 100, value=[20, 80])\n\n# Selection inputs\nui.input_select(\"select_id\", \"Choose one\", choices=[\"A\", \"B\", \"C\"])\nui.input_selectize(\"selectize_id\", \"Type to search\", \n                   choices=data[\"column\"].unique().to_list(),\n                   multiple=True)\n\n# Boolean inputs\nui.input_checkbox(\"check_id\", \"Enable feature\", value=True)\nui.input_switch(\"switch_id\", \"Toggle mode\")\n\n# File uploads\nui.input_file(\"file_id\", \"Upload CSV\", accept=\".csv\")\n\n# Date/time inputs\nui.input_date(\"date_id\", \"Select date\")\nui.input_date_range(\"daterange_id\", \"Date range\")\n</code></pre>"},{"location":"guides/dashboards/shiny/#output-components","title":"Output Components","text":"<p>Display results with these output components:</p> <pre><code># Text outputs\nui.output_text(\"text_id\")        # Plain text\nui.output_text_verbatim(\"code_id\")  # Monospace text\nui.output_ui(\"dynamic_ui\")       # Dynamic UI elements\n\n# Tables\nui.output_table(\"table_id\")      # Basic table\nui.output_data_frame(\"df_id\")    # Interactive data frame\n\n# Plots\noutput_widget(\"plot_id\")         # For plotly/bokeh widgets\nui.output_plot(\"matplotlib_id\")  # For matplotlib plots\n\n# Downloads\nui.download_button(\"download_id\", \"Download Data\")\n</code></pre>"},{"location":"guides/dashboards/shiny/#reactive-programming","title":"Reactive Programming","text":"<p>Shiny's reactive system automatically updates outputs when inputs change:</p>"},{"location":"guides/dashboards/shiny/#reactive-calculations","title":"Reactive Calculations","text":"<p>Use <code>@reactive.Calc</code> for expensive computations that multiple outputs depend on:</p> <pre><code>@reactive.Calc\ndef processed_data():\n    # This only runs when dependencies change\n    raw_data = load_data()\n    return raw_data.filter(pl.col(\"active\") == input.show_active())\n\n@render_widget\ndef plot1():\n    data = processed_data()  # Uses cached result\n    return create_plot(data)\n\n@render.text  \ndef summary():\n    data = processed_data()  # Uses same cached result\n    return f\"Records: {len(data)}\"\n</code></pre>"},{"location":"guides/dashboards/shiny/#reactive-effects","title":"Reactive Effects","text":"<p>Use <code>@reactive.Effect</code> for side effects like updating other inputs:</p> <pre><code>@reactive.Effect\ndef update_choices():\n    # Update selectize choices when category changes\n    category = input.category()\n    new_choices = df.filter(pl.col(\"category\") == category)[\"subcategory\"].unique()\n    ui.update_selectize(\"subcategory\", choices=new_choices.to_list())\n</code></pre>"},{"location":"guides/dashboards/shiny/#event-handling","title":"Event Handling","text":"<p>Respond to button clicks and other events:</p> <pre><code>@reactive.Effect\n@reactive.event(input.reset_button)\ndef reset_filters():\n    ui.update_slider(\"threshold\", value=50)\n    ui.update_select(\"category\", selected=\"All\")\n</code></pre>"},{"location":"guides/dashboards/shiny/#data-visualization","title":"Data Visualization","text":""},{"location":"guides/dashboards/shiny/#plotly-integration","title":"Plotly Integration","text":"<p>Create interactive plots with plotly:</p> <pre><code>from shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n@render_widget\ndef scatter_plot():\n    df_plot = filtered_data().to_pandas()\n\n    fig = px.scatter(\n        df_plot, \n        x=\"x_value\", \n        y=\"y_value\",\n        color=\"category\",\n        size=\"size_value\",\n        hover_data=[\"additional_info\"],\n        title=\"Interactive Scatter Plot\"\n    )\n\n    # Customize layout\n    fig.update_layout(\n        height=500,\n        showlegend=True,\n        hovermode=\"closest\"\n    )\n\n    return fig\n\n@render_widget  \ndef time_series():\n    df_ts = time_series_data().to_pandas()\n\n    fig = go.Figure()\n\n    for category in df_ts[\"category\"].unique():\n        category_data = df_ts[df_ts[\"category\"] == category]\n        fig.add_trace(go.Scatter(\n            x=category_data[\"date\"],\n            y=category_data[\"value\"],\n            name=category,\n            mode=\"lines+markers\"\n        ))\n\n    fig.update_layout(\n        title=\"Time Series Analysis\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Value\"\n    )\n\n    return fig\n</code></pre>"},{"location":"guides/dashboards/shiny/#custom-plots","title":"Custom Plots","text":"<p>Create custom visualizations with matplotlib or other libraries:</p> <pre><code>from shiny import render\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n@render.plot\ndef correlation_heatmap():\n    df_corr = correlation_data().to_pandas()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        df_corr.corr(),\n        annot=True,\n        cmap=\"coolwarm\",\n        center=0,\n        square=True\n    )\n    plt.title(\"Correlation Matrix\")\n    plt.tight_layout()\n    return plt.gcf()\n</code></pre>"},{"location":"guides/dashboards/shiny/#data-tables","title":"Data Tables","text":"<p>Display and interact with tabular data:</p>"},{"location":"guides/dashboards/shiny/#basic-tables","title":"Basic Tables","text":"<pre><code>@render.table\ndef simple_table():\n    return filtered_data().to_pandas()\n</code></pre>"},{"location":"guides/dashboards/shiny/#interactive-data-frames","title":"Interactive Data Frames","text":"<pre><code>from shiny.render import DataGrid, DataTable\n\n@render.data_frame\ndef interactive_grid():\n    df_display = filtered_data().to_pandas()\n\n    return DataGrid(\n        df_display,\n        selection_mode=\"rows\",  # or \"none\", \"row\", \"rows\", \"col\", \"cols\"\n        filters=True,\n        width=\"100%\",\n        height=\"400px\"\n    )\n\n# Access selected rows\n@reactive.Effect\ndef handle_selection():\n    selected = interactive_grid.data_view(selected=True)\n    if len(selected) &gt; 0:\n        # Process selected data\n        pass\n</code></pre>"},{"location":"guides/dashboards/shiny/#custom-table-styling","title":"Custom Table Styling","text":"<pre><code>@render.table\ndef styled_table():\n    df = summary_stats().to_pandas()\n\n    # Format numeric columns\n    df[\"percentage\"] = df[\"percentage\"].map(\"{:.1%}\".format)\n    df[\"amount\"] = df[\"amount\"].map(\"${:,.0f}\".format)\n\n    return df\n</code></pre>"},{"location":"guides/dashboards/shiny/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/dashboards/shiny/#dynamic-ui","title":"Dynamic UI","text":"<p>Create UI elements that change based on user input:</p> <pre><code>@render.ui\ndef dynamic_controls():\n    analysis_type = input.analysis_type()\n\n    if analysis_type == \"correlation\":\n        return ui.div(\n            ui.input_selectize(\"x_var\", \"X Variable\", choices=numeric_columns),\n            ui.input_selectize(\"y_var\", \"Y Variable\", choices=numeric_columns)\n        )\n\n    if analysis_type == \"distribution\":\n        return ui.div(\n            ui.input_select(\"dist_var\", \"Variable\", choices=all_columns),\n            ui.input_numeric(\"bins\", \"Number of bins\", value=30)\n        )\n\n    eturn ui.div(\"Select an analysis type\")\n</code></pre>"},{"location":"guides/dashboards/shiny/#progress-indicators","title":"Progress Indicators","text":"<p>Show progress for long-running operations:</p> <pre><code>from shiny import ui\n\n@reactive.Effect\n@reactive.event(input.run_analysis)\ndef run_long_analysis():\n    with ui.Progress(min=0, max=100) as progress:\n        progress.set(message=\"Loading data\", value=0)\n        data = load_large_dataset()\n\n        progress.set(message=\"Processing\", value=50)\n        results = process_data(data)\n\n        progress.set(message=\"Finalizing\", value=90)\n        save_results(results)\n\n        progress.set(value=100)\n\n    ui.notification_show(\"Analysis complete!\", type=\"success\")\n</code></pre>"},{"location":"guides/dashboards/shiny/#integration-with-analyzers","title":"Integration with Analyzers","text":""},{"location":"guides/dashboards/shiny/#accessing-analyzer-data","title":"Accessing Analyzer Data","text":"<pre><code>def factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Access primary analyzer outputs\n    main_data = pl.read_parquet(\n        context.base.table(\"main_analysis\").parquet_path\n    )\n\n    # Access secondary analyzer outputs\n    summary_data = pl.read_parquet(\n        context.dependency(summary_analyzer).table(\"summary\").parquet_path\n    )\n\n    # Access parameters used in analysis\n    threshold = context.base_params.get(\"threshold\", 0.5)\n\n    # Build dashboard with this data\n    # ...\n</code></pre>"},{"location":"guides/dashboards/shiny/#parameter-integration","title":"Parameter Integration","text":"<p>Use analyzer parameters in your dashboard:</p> <pre><code>def server(input, output, session):\n    # Get analyzer parameters\n    analyzer_threshold = context.base_params.get(\"threshold\", 0.5)\n\n    @render.text\n    def analysis_info():\n        return f\"Analysis run with threshold: {analyzer_threshold}\"\n\n    @render_widget\n    def threshold_comparison():\n        # Compare user input with analyzer parameter\n        user_threshold = input.user_threshold()\n        df_comparison = main_data.with_columns([\n            (pl.col(\"value\") &gt; analyzer_threshold).alias(\"analyzer_flag\"),\n            (pl.col(\"value\") &gt; user_threshold).alias(\"user_flag\")\n        ])\n\n        return create_comparison_plot(df_comparison)\n</code></pre>"},{"location":"guides/dashboards/shiny/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/dashboards/shiny/#efficient-data-processing","title":"Efficient Data Processing","text":"<pre><code>@reactive.Calc\ndef base_data():\n    # Load once and cache\n    return pl.read_parquet(data_path)\n\n@reactive.Calc  \ndef filtered_data():\n    # Efficient filtering with Polars\n    filters = []\n\n    if input.category() != \"All\":\n        filters.append(pl.col(\"category\") == input.category())\n\n    if input.date_range() is not None:\n        start, end = input.date_range()\n        filters.append(pl.col(\"date\").is_between(start, end))\n\n    if filters:\n        return base_data().filter(pl.all_horizontal(filters))\n\n    else:\n        return base_data()\n</code></pre>"},{"location":"guides/dashboards/shiny/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code>@reactive.Calc\ndef expensive_calculation():\n    # Only runs when dependencies change\n    data = filtered_data()\n\n    # Use lazy evaluation\n    result = (\n        data\n        .group_by(\"category\")\n        .agg([\n            pl.col(\"value\").mean().alias(\"avg_value\"),\n            pl.col(\"value\").std().alias(\"std_value\"),\n            pl.col(\"value\").count().alias(\"count\")\n        ])\n        .sort(\"avg_value\", descending=True)\n    )\n\n    return result\n</code></pre>"},{"location":"guides/dashboards/shiny/#testing-shiny-dashboards","title":"Testing Shiny Dashboards","text":""},{"location":"guides/dashboards/shiny/#unit-testing-components","title":"Unit Testing Components","text":"<pre><code>import pytest\nfrom shiny.testing import ShinyAppProc\nfrom your_presenter import factory\n\ndef test_dashboard_loads():\n    \"\"\"Test that dashboard loads without errors\"\"\"\n    app = factory(mock_context)\n\n    # Test UI renders\n    assert app.shiny.panel is not None\n\n    # Test server function exists\n    assert callable(app.shiny.server_handler)\n\ndef test_data_filtering():\n    \"\"\"Test reactive data filtering\"\"\"\n    with ShinyAppProc(factory(mock_context)) as proc:\n        # Set input values\n        proc.set_inputs(category=\"TypeA\", threshold=50)\n\n        # Check outputs update correctly\n        output = proc.get_output(\"summary_stats\")\n        assert \"TypeA\" in output\n</code></pre>"},{"location":"guides/dashboards/shiny/#integration-testing","title":"Integration Testing","text":"<pre><code>def test_with_real_data():\n    \"\"\"Test dashboard with actual analyzer output\"\"\"\n    # Run analyzer to generate test data\n    context = create_test_context(test_data_path)\n\n    # Test dashboard with real data\n    app = factory(context)\n\n    # Verify data loads correctly\n    assert app.shiny.panel is not None\n</code></pre>"},{"location":"guides/dashboards/shiny/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"guides/dashboards/shiny/#resource-management","title":"Resource Management","text":"<ul> <li>Use <code>@reactive.Calc</code> for expensive operations to enable caching</li> <li>Implement pagination for large datasets</li> <li>Consider data sampling for very large visualizations</li> <li>Use lazy loading for secondary data</li> </ul>"},{"location":"guides/dashboards/shiny/#error-handling","title":"Error Handling","text":"<pre><code>@render_widget\ndef safe_plot():\n    try:\n        data = filtered_data()\n        if len(data) == 0:\n            return empty_plot_message()\n\n        return create_plot(data)\n\n    except Exception as e:\n        ui.notification_show(f\"Plot error: {str(e)}\", type=\"error\")\n        return error_plot()\n</code></pre>"},{"location":"guides/dashboards/shiny/#session-management","title":"Session Management","text":"<pre><code>def server(input, output, session):\n    # Clean up resources when session ends\n    @reactive.Effect\n    def cleanup():\n        session.on_ended(lambda: cleanup_user_data())\n</code></pre>"},{"location":"guides/dashboards/shiny/#example-complete-dashboard","title":"Example: Complete Dashboard","text":"<p>Here's a complete example of a Shiny dashboard for analyzing message sentiment:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport polars as pl\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df_sentiment = pl.read_parquet(\n        context.base.table(\"sentiment_analysis\").parquet_path\n    )\n\n    # Get unique values for inputs\n    date_range = (df_sentiment[\"date\"].min(), df_sentiment[\"date\"].max())\n    categories = [\"All\"] + df_sentiment[\"category\"].unique().to_list()\n\n    # UI Layout\n    dashboard = ui.page_sidebar(\n        ui.sidebar(\n            ui.h3(\"Analysis Controls\"),\n            ui.input_date_range(\n                \"date_filter\", \n                \"Date Range\",\n                start=date_range[0],\n                end=date_range[1]\n            ),\n            ui.input_selectize(\n                \"category_filter\",\n                \"Categories\", \n                choices=categories,\n                selected=\"All\",\n                multiple=True\n            ),\n            ui.input_slider(\n                \"sentiment_threshold\",\n                \"Sentiment Threshold\",\n                -1, 1, 0, step=0.1\n            ),\n            ui.hr(),\n            ui.input_action_button(\"reset\", \"Reset Filters\"),\n            ui.download_button(\"download\", \"Download Data\")\n        ),\n\n        ui.div(\n            ui.h2(\"Sentiment Analysis Dashboard\"),\n\n            ui.row(\n                ui.column(6, ui.value_box(\n                    title=\"Total Messages\",\n                    value=ui.output_text(\"total_count\"),\n                    theme=\"primary\"\n                )),\n                ui.column(6, ui.value_box(\n                    title=\"Avg Sentiment\", \n                    value=ui.output_text(\"avg_sentiment\"),\n                    theme=\"success\"\n                ))\n            ),\n\n            ui.navset_tab(\n                ui.nav_panel(\n                    \"Time Series\",\n                    output_widget(\"timeseries_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Distribution\", \n                    output_widget(\"distribution_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Data Table\",\n                    ui.output_data_frame(\"data_table\")\n                )\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            data = df_sentiment\n\n            # Date filtering\n            if input.date_filter() is not None:\n                start, end = input.date_filter()\n                data = data.filter(pl.col(\"date\").is_between(start, end))\n\n            # Category filtering\n            if \"All\" not in input.category_filter():\n                data = data.filter(pl.col(\"category\").is_in(input.category_filter()))\n\n            # Sentiment filtering\n            data = data.filter(pl.col(\"sentiment\") &gt;= input.sentiment_threshold())\n\n            return data\n\n        @render.text\n        def total_count():\n            return f\"{len(filtered_data()):,}\"\n\n        @render.text\n        def avg_sentiment():\n            avg = filtered_data()[\"sentiment\"].mean()\n            return f\"{avg:.3f}\"\n\n        @render_widget\n        def timeseries_plot():\n            df_plot = (\n                filtered_data()\n                .group_by(\"date\")\n                .agg(pl.col(\"sentiment\").mean().alias(\"avg_sentiment\"))\n                .sort(\"date\")\n                .to_pandas()\n            )\n\n            fig = px.line(\n                df_plot, \n                x=\"date\", \n                y=\"avg_sentiment\",\n                title=\"Sentiment Over Time\"\n            )\n            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n            return fig\n\n        @render_widget\n        def distribution_plot():\n            df_plot = filtered_data().to_pandas()\n\n            fig = px.histogram(\n                df_plot,\n                x=\"sentiment\", \n                color=\"category\",\n                title=\"Sentiment Distribution\",\n                nbins=50\n            )\n            return fig\n\n        @render.data_frame\n        def data_table():\n            return filtered_data().to_pandas()\n\n        @reactive.Effect\n        @reactive.event(input.reset)\n        def reset_filters():\n            ui.update_date_range(\"date_filter\", start=date_range[0], end=date_range[1])\n            ui.update_selectize(\"category_filter\", selected=\"All\")\n            ui.update_slider(\"sentiment_threshold\", value=0)\n\n        @render.download(filename=\"sentiment_data.csv\")\n        def download():\n            return filtered_data().write_csv()\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Sentiment Analysis\", dashboard)\n        )\n    )\n</code></pre> <p>This comprehensive guide covers all aspects of building Shiny dashboards for your analyzer platform. The reactive programming model, rich widget ecosystem, and seamless Python integration make Shiny an excellent choice for creating sophisticated data analysis interfaces.</p>"},{"location":"guides/dashboards/shiny/#next-steps","title":"Next Steps","text":"<p>Once you finish reading section be a good idea to review the section that discuss implementing React dashboards. Might also be a good idea to review the sections for each domain. </p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>React Dashboards</li> </ul>"},{"location":"guides/domains/content-domain/","title":"Content Domain","text":""},{"location":"guides/domains/content-domain/#content-domain","title":"Content Domain","text":"<p>The Content domain is where the analysis and visualization happen.</p> <p>An analysis is added to the application by defining a Primary Analyzer, which comes with an interface declaration and an implementation. The interface declaration defines the input data structure and the output tables, which the application depends on for user guidance. The implementation is made workspace-agnostic by means of the \"context\" object.</p> <p>The goal of the Primary Analyzer is to produce a set of output tables that can be used by other analyzers, including Secondary Analyzers and Web Presenters. Primary Analyzer outputs are ideally normalized, non-duplicated, and non-redundant. As such, they are not always suitable for direct user consumption. It is the job of the Secondary Analyzers to produce user-friendly outputs and the job of Web Presenters to produce interactive visualizations.</p> <p>Both Secondary Analyzers and Web Presenters are also defined using interface objects. Secondary Analyzers will depend on the output of Primary Analyzers, and Web Presenters will depend on the output of both Primary and Secondary Analyzers.</p>"},{"location":"guides/domains/content-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/domains/core-domain/","title":"Core Domain","text":""},{"location":"guides/domains/core-domain/#core-domain","title":"Core Domain","text":""},{"location":"guides/domains/core-domain/#application","title":"Application","text":"<p>The Application lives inside the <code>app</code> directory in the project root. This is responsible for defining and executing all capabilities of the application's workspace. Any extension or modification of the application's workspace capabilities should be done here.</p> <p>The application code should be free of specific storage implementation and be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the application module looks like</p> <p>./app/init.py:</p> <pre><code>from .analysis_context import AnalysisContext\nfrom .analysis_output_context import AnalysisOutputContext\nfrom .analysis_webserver_context import AnalysisWebServerContext\nfrom .app import App\nfrom .app_context import AppContext\nfrom .project_context import ProjectContext\nfrom .settings_context import SettingsContext\n</code></pre>"},{"location":"guides/domains/core-domain/#terminal-components","title":"Terminal Components","text":"<p>The Terminal Components live inside the <code>terminal_tools</code> inside the project root. Their main responsibility is user flow, rendering the terminal interface, and handling user input.</p> <p>The user flow understandably depends on the set of capabilities offered by the Application, so an adjustment there may require an adjustment here.</p> <p>Here's what the entrypoint for the termnal module looks like</p> <p>./terminal_tools/init.py</p> <pre><code>from .progress import ProgressReporter\nfrom .utils import (\n    clear_printed_lines,\n    clear_terminal,\n    draw_box,\n    enable_windows_ansi_support,\n    open_directory_explorer,\n    print_ascii_table,\n    wait_for_key,\n)\n</code></pre>"},{"location":"guides/domains/core-domain/#storage-io","title":"Storage IO","text":"<p>The Storage IO lives Inside the <code>storage</code> directory inside the project root. It is responsible for interacting directly with the file system where the workspace data and data files are stored. It makes decisions on paths, intermediate file formats, and database schema and implementation. It should know as little as possible about how the data is used and should be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the storage module looks like</p> <p>./storage/init.py:</p> <pre><code>import math\nimport os\nimport re\nimport shutil\nfrom datetime import datetime\nfrom typing import Callable, Iterable, Literal, Optional\n\nimport platformdirs\nimport polars as pl\nimport pyarrow.parquet as pq\nfrom filelock import FileLock\nfrom pydantic import BaseModel\nfrom tinydb import Query, TinyDB\nfrom xlsxwriter import Workbook\n\nfrom analyzer_interface.interface import AnalyzerOutput\nfrom analyzer_interface.params import ParamValue\n\nfrom .file_selector import FileSelectorStateManager\n\n\nclass ProjectModel(BaseModel):\n    class_: Literal[\"project\"] = \"project\"\n    id: str\n    display_name: str\n\n\nclass SettingsModel(BaseModel):\n    class_: Literal[\"settings\"] = \"settings\"\n    export_chunk_size: Optional[int | Literal[False]] = None\n\n\nclass FileSelectionState(BaseModel):\n    class_: Literal[\"file_selector_state\"] = \"file_selector_state\"\n    last_path: Optional[str] = None\n\n\nclass AnalysisModel(BaseModel):\n    class_: Literal[\"analysis\"] = \"analysis\"\n    analysis_id: str\n    project_id: str\n    display_name: str\n    primary_analyzer_id: str\n    path: str\n    column_mapping: Optional[dict[str, str]] = None\n    create_timestamp: Optional[float] = None\n    param_values: dict[str, ParamValue] = dict()\n    is_draft: bool = False\n\n    def create_time(self):\n        return (\n            datetime.fromtimestamp(self.create_timestamp)\n            if self.create_timestamp\n            else None\n        )\n\n\nSupportedOutputExtension = Literal[\"parquet\", \"csv\", \"xlsx\", \"json\"]\n\n\nclass Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return pl.read_parquet(output_path)\n\n    def get_secondary_output_parquet_path(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        return os.path.join(\n            self._get_project_secondary_output_root_path(analysis, secondary_id),\n            f\"{output_id}.parquet\",\n        )\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n\n\nclass TableStats(BaseModel):\n    num_rows: int\n\n\ndef collect_dataframe_chunks(\n    input: Iterable[pl.DataFrame], size_threshold: int\n) -&gt; Iterable[pl.DataFrame]:\n    output_buffer = []\n    size = 0\n    for df in input:\n        while True:\n            available_space = size_threshold - size\n            slice = df.head(available_space)\n            output_buffer.append(slice)\n            size = size + slice.height\n            remaining_space = available_space - slice.height\n\n            if remaining_space == 0:\n                yield pl.concat(output_buffer)\n                output_buffer = []\n                size = 0\n\n            if slice.height == df.height:\n                break\n            else:\n                df = df.tail(-available_space)\n\n    if output_buffer:\n        yield pl.concat(output_buffer)\n\n\nclass AppFileSelectorStateManager(FileSelectorStateManager):\n    def __init__(self, storage: \"Storage\"):\n        self.storage = storage\n\n    def get_current_path(self):\n        return self._load_state().last_path\n\n    def set_current_path(self, path: str):\n        self._save_state(path)\n\n    def _load_state(self):\n        q = Query()\n        state = self.storage.db.search(q[\"class_\"] == \"file_selector_state\")\n        if state:\n            return FileSelectionState(**state[0])\n        return FileSelectionState()\n\n    def _save_state(self, last_path: str):\n        self.storage.db.upsert(\n            FileSelectionState(last_path=last_path).model_dump(),\n            Query()[\"class_\"] == \"file_selector_state\",\n        )\n</code></pre>"},{"location":"guides/domains/core-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/domains/edge-domain/","title":"Edge Domain","text":""},{"location":"guides/domains/edge-domain/#edge-domain","title":"Edge Domain","text":"<p>The Edge domain governs data import and export.</p>"},{"location":"guides/domains/edge-domain/#importers","title":"Importers","text":"<p>The Importers live inside the <code>importing</code> directory inside the project root. Each importer offers a new way to import data into the workspace. The importers should be agnostic about the available analyzers. However, the Importers currently provide a terminal user flow so that their options can be customized by the user\u2014a necessity since each importer may expose different sets of options and may have different UX approaches for their configuration.</p> <p>The importers eventually write data to a parquet file, whose path is provisioned by the application.</p> <p>Here's what the entrypoint for the importer module looks like</p> <p>./importing/init.py:</p> <pre><code>from .csv import CSVImporter\nfrom .excel import ExcelImporter\nfrom .importer import Importer, ImporterSession\n\nimporters: list[Importer[ImporterSession]] = [CSVImporter(), ExcelImporter()]\n</code></pre>"},{"location":"guides/domains/edge-domain/#semantic-preprocessor","title":"Semantic Preprocessor","text":"<p>The Semantic Preprocessor lives inside the <code>preprocessing</code> directory inside the project root. It defines all the column data semantics\u2014a kind of type system that is used to guide the user in selecting the right columns for the right analysis. It is agnostic about the specific analyzers but does depend on them in a generic way\u2014the available semantics exist to support the needs of analyzers and will be extended as necessary.</p> <p>Here's what the entrypoint for the preprocessing module looks like</p> <p>./preprocessing/series_semantic.py:</p> <pre><code>from datetime import datetime\nfrom typing import Callable, Type, Union\n\nimport polars as pl\nfrom pydantic import BaseModel\n\nfrom analyzer_interface import DataType\n\n\nclass SeriesSemantic(BaseModel):\n    semantic_name: str\n    column_type: Union[Type[pl.DataType], Callable[[pl.DataType], bool]]\n    prevalidate: Callable[[pl.Series], bool] = lambda s: True\n    try_convert: Callable[[pl.Series], pl.Series]\n    validate_result: Callable[[pl.Series], pl.Series] = lambda s: s.is_not_null()\n    data_type: DataType\n\n    def check(self, series: pl.Series, threshold: float = 0.8, sample_size: int = 100):\n        if not self.check_type(series):\n            return False\n\n        sample = sample_series(series, sample_size)\n        try:\n            if not self.prevalidate(sample):\n                return False\n            result = self.try_convert(sample)\n        except Exception:\n            return False\n        return self.validate_result(result).sum() / sample.len() &gt; threshold\n\n    def check_type(self, series: pl.Series):\n        if isinstance(self.column_type, type):\n            return isinstance(series.dtype, self.column_type)\n        return self.column_type(series.dtype)\n\n\ndatetime_string = SeriesSemantic(\n    semantic_name=\"datetime\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strptime(pl.Datetime, strict=False),\n    data_type=\"datetime\",\n)\n\n\ntimestamp_seconds = SeriesSemantic(\n    semantic_name=\"timestamp_seconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: (s * 1_000).cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\ntimestamp_milliseconds = SeriesSemantic(\n    semantic_name=\"timestamp_milliseconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: s.cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\nurl = SeriesSemantic(\n    semantic_name=\"url\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(\"^https?://\").gt(0),\n    data_type=\"url\",\n)\n\nidentifier = SeriesSemantic(\n    semantic_name=\"identifier\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(r\"^@?[A-Za-z0-9_.:-]+$\").eq(1),\n    data_type=\"identifier\",\n)\n\ntext_catch_all = SeriesSemantic(\n    semantic_name=\"free_text\",\n    column_type=pl.String,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"text\",\n)\n\ninteger_catch_all = SeriesSemantic(\n    semantic_name=\"integer\",\n    column_type=lambda dt: dt.is_integer(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"integer\",\n)\n\nfloat_catch_all = SeriesSemantic(\n    semantic_name=\"float\",\n    column_type=lambda dt: dt.is_float(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"float\",\n)\n\nboolean_catch_all = SeriesSemantic(\n    semantic_name=\"boolean\",\n    column_type=pl.Boolean,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"boolean\",\n)\n\nall_semantics = [\n    datetime_string,\n    timestamp_seconds,\n    timestamp_milliseconds,\n    url,\n    identifier,\n    text_catch_all,\n    integer_catch_all,\n    float_catch_all,\n    boolean_catch_all,\n]\n\n\ndef infer_series_semantic(\n    series: pl.Series, *, threshold: float = 0.8, sample_size=100\n):\n    for semantic in all_semantics:\n        if semantic.check(series, threshold=threshold, sample_size=sample_size):\n            return semantic\n    return None\n\n\ndef sample_series(series: pl.Series, n: int = 100):\n    if series.len() &lt; n:\n        return series\n    return series.sample(n, seed=0)\n\n\ndef constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"guides/domains/edge-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/get-started/architecture/","title":"CLI design philosophy","text":"<p>Before contributing please refer to our Contributor Workflow</p>"},{"location":"guides/get-started/architecture/#application-design-overview","title":"Application Design Overview","text":"<p>The CIB \ud83e\udd6d application is a terminal-based tool for performing data analysis and visualization. It is designed to be modular and extensible, allowing developers to contribute new analysis modules and visualization components while providing a consistent user experience around data import, preprocessing, and output generation.</p> <p>This design is motivated by a common pain point when moving from a data analysis script for private use to a tool that can be shared with others: A script for private consumption carries assumptions about the desired input and output data format and structure that are convenient to its author. When such a script is made available to others, debates on these aspects often arise. For a suite of analyses that this project aims to offer, if left decentralized, this debate can lead to inconsistent UX offerings across analyses, code duplication, and even bugs.</p> <p>The architecture of the CIB \ud83e\udd6d application is designed to address this problem by providing a clear separation between the core application logic and the analysis modules, such that the analysis module does not need to be concerned with the input and output data format and structure; such responsibilities are handled by the core application, where we aim to provide a rich, consistent, and intuitive user experience.</p>"},{"location":"guides/get-started/architecture/#architecture-overview","title":"Architecture Overview","text":"<p>The application has three \"domains\": - The Core domain is responsible for workspace management, user flow, and integration of analysis runs and data import/export in a generic sense. It has three parts that correspond loosely to the MVC paradigm.   - The Application defines the workspace logic and exposes generic capabilities for importing and exporting data as well as analyses and dashboards. This is the \"controller\" part.   - The Terminal Components render the terminal interface and handle user input. This is the \"view\" part.   - The Storage IO persists the workspace data and is responsible for reading and writing data. This is the \"model\" part.</p> <p>The core application provides the context necessary for the other domains to function in a way that allows them to be agnostic about the specifics of the workspace and user flow.</p> <ul> <li>The Edge domain is responsible for data import and export while being agnostic about the specific analysis being run. Currently, this consists of the Importers and the Semantic Preprocessor.</li> </ul> <p>Note that the Storage IO is currently responsible for data export, but we should consider moving this to the Edge domain to allow for more extensibility and looser coupling.</p> <ul> <li>The Content domain is responsible for the actual data analysis and visualization and is agnostic about data import/export or workspace specifics. This consists of the Analyzers (both Primary and Secondary) as well as the Web Presenters.</li> </ul> <pre><code>flowchart TD\n    terminal[\"Terminal (core)\"]\n    application[\"Application (core)\"]\n    storage[\"Storage (core)\"]\n\n    importers[\"Importers (edge)\"]\n    semantic[\"Semantic Preprocessor (edge)\"]\n\n    content[\"Analyzers/Web Presenters (content)\"]\n\n    terminal --&gt; application\n    application --&gt; storage\n\n    application --&gt; importers\n    application --&gt; semantic\n\n    application --&gt; content\n</code></pre>"},{"location":"guides/get-started/architecture/#questions-comments-and-feedback","title":"Questions, Comments, and Feedback","text":"<p>Talk to us on the Civic Tech DC Slack workspace!</p>"},{"location":"guides/get-started/architecture/#next-steps","title":"Next Steps","text":"<p>It would be recommended to review the sections for each domain, and the section for implementing analyzers. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Implementing Analyzers</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/get-started/contributing/","title":"Contributing to The Project","text":"<p>Before following this workflow please refer to our Getting Started page for instructions on installing dependencies and setting up your development environment.</p>"},{"location":"guides/get-started/contributing/#contributor-workflow","title":"Contributor Workflow","text":""},{"location":"guides/get-started/contributing/#overview","title":"Overview","text":"<p>All changes should be made in a feature branch, merged into <code>develop</code>, and later merged into <code>main</code> for a new release.</p>"},{"location":"guides/get-started/contributing/#contributing-new-changes","title":"Contributing new changes","text":"<ol> <li>Create a Feature Branch</li> <li>Branch from <code>develop</code> using <code>feature/&lt;name&gt;</code> or <code>bugfix/&lt;name&gt;</code>.</li> <li> <p>Example:</p> <p><code>shell  git checkout develop  git pull origin develop  git checkout -b feature/new-feature</code></p> </li> <li> <p>Make Changes &amp; Push</p> </li> <li>Commit changes with clear messages.</li> <li> <p>Push the branch.</p> <p><code>shell  git add .  git commit -m \"Description of changes\"  git push origin feature/new-feature</code></p> </li> <li> <p>Create a Pull Request</p> </li> <li>Open a PR to merge into <code>develop</code>.</li> <li> <p>Address any review feedback.</p> </li> <li> <p>Merge &amp; Clean Up</p> </li> <li>After approval, merge into <code>develop</code>.</li> <li> <p>Delete the feature branch.</p> </li> <li> <p>Release</p> </li> <li>When develop is clean and ready for a new major release, we will merge <code>develop</code> into <code>main</code>.</li> </ol>"},{"location":"guides/get-started/contributing/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph TD;\n    A[Feature Branch] --&gt;|Commit &amp; Push| B[Pull Request];\n    B --&gt;|Review &amp; Merge| C[Develop Branch];\n    C --&gt;|Release| D[Main Branch];\n</code></pre>"},{"location":"guides/get-started/contributing/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/get-started/installation/","title":"Installation","text":""},{"location":"guides/get-started/installation/#setup","title":"Setup","text":""},{"location":"guides/get-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"guides/get-started/installation/#required-software","title":"Required Software","text":"<ul> <li>Python 3.12 - Required for all features to work correctly</li> <li>Node.JS (20.0.0 or above) - Required for the React dashboards   to work correctly</li> <li>Git - For version control and contributing</li> <li>Terminal/Command Line - Application runs in terminal interface</li> </ul>"},{"location":"guides/get-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Windows (PowerShell), macOS, Linux</li> <li>Memory: 4GB+ RAM (for processing large datasets)</li> <li>Storage: 1GB+ free space (for project data and virtual environment)</li> </ul>"},{"location":"guides/get-started/installation/#resources","title":"Resources","text":"<p>If you haven't installed git, node.js, and/or python yet refer to the following links for instructions on downloading and installing said packages:</p> <ul> <li>https://codefinity.com/blog/A-step-by-step-guide-to-Git-installation</li> <li>https://nodejs.org/en/download</li> <li>https://realpython.com/installing-python/</li> </ul>"},{"location":"guides/get-started/installation/#checking-dependencies","title":"Checking Dependencies","text":"<p>If you're not sure which packages you already have installed on your system, the following commands can be used to figure what packages you already installed:</p>"},{"location":"guides/get-started/installation/#linux-mac-os","title":"Linux &amp; Mac OS","text":"<pre><code>which &lt;program_name_here (node|python|git)&gt;\n</code></pre>"},{"location":"guides/get-started/installation/#windows","title":"Windows","text":"<pre><code>where.exe &lt;program_name_here (node|python|git)&gt; \n</code></pre>"},{"location":"guides/get-started/installation/#installation","title":"Installation","text":""},{"location":"guides/get-started/installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/CIB-Mango-Tree/mango-tango-cli.git\ncd mango-tango-cli\n</code></pre>"},{"location":"guides/get-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\n</code></pre> <p>Verify Python version:</p> <pre><code>python --version  # Should show Python 3.12.x\n</code></pre>"},{"location":"guides/get-started/installation/#3-bootstrap-development-environment","title":"3. Bootstrap Development Environment","text":"<p>Mac OS/Linux (Bash):</p> <pre><code>./bootstrap.sh\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>./bootstrap.ps1\n</code></pre> <p>The bootstrap script will:</p> <ul> <li>Activate the virtual environment</li> <li>Install all dependencies from <code>requirements-dev.txt</code></li> <li>Set up pre-commit hooks for code formatting</li> </ul>"},{"location":"guides/get-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python -m mangotango --noop\n</code></pre> <p>Should output: \"No-op flag detected. Exiting successfully.\"</p>"},{"location":"guides/get-started/installation/#activating-virtual-environment","title":"Activating Virtual Environment","text":"<p>After Completing the Installation the following commands can be used to activate the virtual environment in order to work with the project.</p> <p>Mac OS/Linux (Bash):</p> <pre><code>source ./venv/bin/activate\n</code></pre> <p>PowerShell (Windows):</p> <pre><code>./env/bin/Activate.ps1\n</code></pre>"},{"location":"guides/get-started/installation/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"guides/get-started/installation/#dependencies-overview","title":"Dependencies Overview","text":"<p>Production Dependencies (<code>requirements.txt</code>):</p> <ul> <li><code>polars==1.9.0</code> - Primary data processing</li> <li><code>pydantic==2.9.1</code> - Data validation and models</li> <li><code>inquirer==3.4.0</code> - Interactive terminal prompts</li> <li><code>tinydb==4.8.0</code> - Lightweight JSON database</li> <li><code>dash==2.18.1</code> - Web dashboard framework</li> <li><code>shiny==1.4.0</code> - Modern web UI framework</li> <li><code>plotly==5.24.1</code> - Data visualization</li> <li><code>XlsxWriter==3.2.0</code> - Excel export functionality</li> </ul> <p>Development Dependencies (<code>requirements-dev.txt</code>):</p> <ul> <li><code>black==24.10.0</code> - Code formatter</li> <li><code>isort==5.13.2</code> - Import organizer</li> <li><code>pytest==8.3.4</code> - Testing framework</li> <li><code>pyinstaller==6.14.1</code> - Executable building</li> </ul> <p>React Dashboard Dependencies (app/web_templates/package.json):</p> <ul> <li>typescript: 5.7.3</li> <li>vite: 6.3.5</li> <li>react: 19.0.0</li> <li>@deck.gl: 9.1.11</li> <li>@visx: 3.12.0</li> <li>@glideapps/glide-data-grid: 6.0.3</li> <li>@radix-ui: (Varies based on component being used)</li> <li>zustand: 5.0.3</li> <li>tailwindcss: 4.0.6</li> <li>lucide-react: 0.475.0</li> </ul>"},{"location":"guides/get-started/installation/#code-formatting-setup","title":"Code Formatting Setup","text":"<p>The project uses automatic code formatting:</p> <ul> <li>Black: Code style and formatting</li> <li>isort: Import organization</li> <li>Pre-commit hooks: Automatic formatting on commit</li> </ul> <p>Manual formatting:</p> <pre><code>isort .\nblack .\n</code></pre>"},{"location":"guides/get-started/installation/#project-structure-setup","title":"Project Structure Setup","text":"<p>After installation, your project structure should be:</p> <pre><code>mango-tango-cli/\n\u251c\u2500\u2500 venv/                    # Virtual environment\n\u251c\u2500\u2500 .serena/                 # Serena semantic analysis\n\u2502   \u2514\u2500\u2500 memories/           # Project knowledge base\n\u251c\u2500\u2500 docs/                    # Documentation\n\u2502   \u251c\u2500\u2500 ai-context/         # AI assistant context\n\u2502   \u2514\u2500\u2500 dev-guide.md        # Development guide\n\u251c\u2500\u2500 app/                     # Application layer\n\u251c\u2500\u2500 analyzers/              # Analysis modules\n\u251c\u2500\u2500 components/             # Terminal UI components\n\u251c\u2500\u2500 storage/                # Data persistence\n\u251c\u2500\u2500 importing/              # Data import modules\n\u251c\u2500\u2500 requirements*.txt       # Dependencies\n\u2514\u2500\u2500 mangotango.py          # Main entry point\n</code></pre>"},{"location":"guides/get-started/installation/#database-and-storage-setup","title":"Database and Storage Setup","text":""},{"location":"guides/get-started/installation/#application-data-directory","title":"Application Data Directory","text":"<p>The application automatically creates data directories:</p> <ul> <li>macOS: <code>~/Library/Application Support/MangoTango/</code></li> <li>Windows: <code>%APPDATA%/Civic Tech DC/MangoTango/</code></li> <li>Linux: <code>~/.local/share/MangoTango/</code></li> </ul>"},{"location":"guides/get-started/installation/#database-initialization","title":"Database Initialization","text":"<ul> <li>TinyDB: Automatically initialized on first run</li> <li>Project Files: Created in user data directory</li> <li>Parquet Files: Used for all analysis data storage</li> </ul> <p>No manual database setup required.</p>"},{"location":"guides/get-started/installation/#running-the-application","title":"Running the Application","text":""},{"location":"guides/get-started/installation/#basic-usage","title":"Basic Usage","text":"<pre><code># Start the application\npython -m mangotango\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode","title":"Development Mode","text":"<pre><code># Run with debugging/development flags\npython -m mangotango --noop  # Test mode, exits immediately\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode-for-the-react-dashboards","title":"Development Mode for The React Dashboards","text":"<p>The following commands can be used to start the development vite server for the react dashboards that are currently in development.</p> <p>npm:</p> <pre><code>cd ./app/web_templates\nnpm run dev\n</code></pre> <p>pnpm:</p> <pre><code>cd ./app/web_templates\npnpm dev\n</code></pre>"},{"location":"guides/get-started/installation/#testing-setup","title":"Testing Setup","text":""},{"location":"guides/get-started/installation/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest analyzers/hashtags/test_hashtags_analyzer.py\n\n# Run with verbose output\npytest -v\n\n# Run specific test function\npytest analyzers/hashtags/test_hashtags_analyzer.py::test_gini\n</code></pre>"},{"location":"guides/get-started/installation/#test-data","title":"Test Data","text":"<ul> <li>Test data is co-located with analyzers in <code>test_data/</code> directories</li> <li>Each analyzer should include its own test files</li> <li>Tests use sample data to verify functionality</li> </ul>"},{"location":"guides/get-started/installation/#build-setup-optional","title":"Build Setup (Optional)","text":""},{"location":"guides/get-started/installation/#executable-building","title":"Executable Building","text":"<pre><code># Build standalone executable\npyinstaller pyinstaller.spec\n\n# Output will be in dist/ directory\n</code></pre>"},{"location":"guides/get-started/installation/#bundle-building-for-react-dashboard","title":"Bundle Building for React Dashboard","text":"<p>npm:</p> <pre><code>npm run build\n</code></pre> <p>pnpm:</p> <pre><code>pnpm build\n</code></pre>"},{"location":"guides/get-started/installation/#build-requirements","title":"Build Requirements","text":"<ul> <li>Included in <code>requirements-dev.txt</code></li> <li>Used primarily for release distribution</li> <li>Not required for development</li> </ul>"},{"location":"guides/get-started/installation/#ide-integration","title":"IDE Integration","text":""},{"location":"guides/get-started/installation/#recommended-ide-settings","title":"Recommended IDE Settings","text":"<p>VS Code (<code>.vscode/</code> configuration):</p> <ul> <li>Python interpreter: <code>./venv/bin/python</code></li> <li>Black formatter integration</li> <li>isort integration</li> <li>pytest test discovery</li> </ul> <p>PyCharm:</p> <ul> <li>Interpreter: Project virtual environment</li> <li>Code style: Black</li> <li>Import optimizer: isort</li> </ul>"},{"location":"guides/get-started/installation/#git-configuration","title":"Git Configuration","text":"<p>Pre-commit Hooks:</p> <pre><code># Hooks are set up automatically by bootstrap script\n# Manual setup if needed:\npip install pre-commit\npre-commit install\n</code></pre> <p>Git Flow:</p> <ul> <li>Branch from <code>develop</code> (not <code>main</code>)</li> <li>Feature branches: <code>feature/name</code></li> <li>Bug fixes: <code>bugfix/name</code></li> </ul>"},{"location":"guides/get-started/installation/#version-management","title":"Version Management","text":"<p>If you already have Python and Node.JS installed but are on different versions from the versions outlined in the requirements above you can switch to the correct versions for both languages for the project using version managers. The version manager for python is pyenv. Where the version manager that is recommended for Node is nvm. Guides for installing both version managers are linked down below if you need references to go off of.</p> <ul> <li>https://www.freecodecamp.org/news/node-version-manager-nvm-install-guide/</li> <li>https://github.com/pyenv/pyenv?tab=readme-ov-file#installation</li> <li>https://github.com/pyenv-win/pyenv-win?tab=readme-ov-file#installation   (If you're on windows and want to install pyenv)</li> </ul> <p>Once you have both version managers installed the following commands can be used to switch versions.</p>"},{"location":"guides/get-started/installation/#pyenv","title":"pyenv","text":"<pre><code>pyenv install 3.12\npyenv local 3.12\n</code></pre>"},{"location":"guides/get-started/installation/#nvm","title":"nvm","text":"<pre><code>nvm install v21.0.0\nnvm use v21.0.0\n</code></pre>"},{"location":"guides/get-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/get-started/installation/#common-dependency-issues","title":"Common Dependency Issues","text":"<p>One common issue when installing the dependencies for python is the installation failing due to compatibility issues with the python package <code>pyarrow</code>. The compatibility issues are due to a version mismatch between pyarrow and python itself. To resolve this issue,you MUST be on version 3.12 for python. Refer to commands above to switch to the correct version.</p> <p>Similarly, the installation for node dependencies has been known to fail for some developers due to a version mismatch caused by the underlying dependencies for the package <code>@glideapps/glide-data-grid</code>. However, getting around this issue is more straightforward with node packages. Running the installation command for node with the flag <code>--legacy-peer-deps</code> is enough for the installation to work if you run into this issue. The commands needed to run the installation manually from the project root are as such.</p> <pre><code>cd ./app/web_templates\nnpm install --legacy-peer-deps\n</code></pre>"},{"location":"guides/get-started/installation/#other-common-issues","title":"Other Common Issues","text":"<p>Import Errors:</p> <pre><code># Ensure virtual environment is activated\nsource venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\Activate.ps1     # Windows\n\n# Reinstall dependencies\npip install -r requirements-dev.txt\n</code></pre> <p>Formatting Errors in CI:</p> <pre><code># Run formatters locally before committing\nisort .\nblack .\n</code></pre> <p>Test Failures:</p> <pre><code># Ensure test data is present\nls analyzers/*/test_data/\n\n# Check if specific analyzer tests pass\npytest analyzers/hashtags/ -v\n</code></pre>"},{"location":"guides/get-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Optional Configuration:</p> <ul> <li><code>MANGOTANGO_DATA_DIR</code> - Override default data directory</li> <li><code>MANGOTANGO_LOG_LEVEL</code> - Set logging verbosity</li> </ul>"},{"location":"guides/get-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have everything installed and running without any problems, the next step is to check out the Contributor Workflow</p>"},{"location":"guides/get-started/logging/","title":"Logging System","text":""},{"location":"guides/get-started/logging/#logging","title":"Logging","text":"<p>The application uses a structured JSON logging system that provides consistent logging across all modules. The logging system automatically separates critical alerts from diagnostic information.</p>"},{"location":"guides/get-started/logging/#logging-architecture","title":"Logging Architecture","text":"<ul> <li>Console Output: Only\u00a0<code>ERROR</code>\u00a0and\u00a0<code>CRITICAL</code>\u00a0messages are displayed on stderr</li> <li>File Output: All messages from\u00a0<code>INFO</code>\u00a0level and above are written to log files</li> <li>Log Format: All logs are structured JSON for easy parsing and analysis</li> <li>Log Rotation: Log files automatically rotate at 10MB with 5 backup files retained</li> <li>Log Location:\u00a0<code>~/.local/share/MangoTango/logs/mangotango.log</code>\u00a0(varies by platform)</li> </ul>"},{"location":"guides/get-started/logging/#using-the-logger-in-your-code","title":"Using the Logger in Your Code","text":""},{"location":"guides/get-started/logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from app.logger import get_logger\n\n# Get a logger for your module\nlogger = get_logger(__name__)\n\n# Log at different levels\nlogger.debug(\"Detailed debugging information\")\nlogger.info(\"General information about program execution\")\nlogger.warning(\"Something unexpected happened, but the program continues\")\nlogger.error(\"A serious problem occurred\")\nlogger.critical(\"A very serious error occurred, program may not be able to continue\")\n</code></pre>"},{"location":"guides/get-started/logging/#example-log-output","title":"Example Log Output","text":"<p>Console (stderr) - Only errors:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre> <p>Log File - All info and above:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,910\", \"name\": \"analyzers.hashtags\", \"levelname\": \"INFO\", \"message\": \"Starting hashtag analysis\", \"taskName\": null}\n{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre>"},{"location":"guides/get-started/logging/#logging-in-analyzers","title":"Logging in Analyzers","text":"<p>When developing analyzers, add logging to help with debugging and monitoring:</p> <pre><code>from app.logger import get_logger\n\ndef main(context):\n    logger = get_logger(__name__)\n\n    logger.info(\"Starting analysis\", extra={\n        \"input_path\": str(context.input_path),\n        \"output_path\": str(context.output_path)\n    })\n\n    try:\n        # Your analysis code here\n        result = perform_analysis(context)\n\n        logger.info(\"Analysis completed successfully\", extra={\n            \"records_processed\": len(result),\n            \"execution_time\": time.time() - start_time\n        })\n\n    except Exception as e:\n        logger.error(\"Analysis failed\", extra={\n            \"error\": str(e),\n            \"error_type\": type(e).__name__\n        }, exc_info=True)\n        raise\n</code></pre>"},{"location":"guides/get-started/logging/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li> <p>Use Appropriate Log Levels:</p> <ul> <li><code>DEBUG</code>: Detailed diagnostic information, only useful when debugging</li> <li><code>INFO</code>: General information about program execution</li> <li><code>WARNING</code>: Something unexpected happened, but the program continues</li> <li><code>ERROR</code>: A serious problem occurred</li> <li><code>CRITICAL</code>: A very serious error occurred, program may not be able to continue</li> <li>Include Context with\u00a0<code>extra</code>\u00a0Parameter:</li> </ul> <p><code>python logger.info(\"Processing file\", extra={     \"filename\": filename,     \"file_size\": file_size,     \"record_count\": record_count })</code></p> </li> <li> <p>Log Exceptions Properly:</p> <p><code>python try:     risky_operation() except Exception as e:     logger.error(\"Operation failed\", exc_info=True)  # Includes stack trace</code></p> </li> <li> <p>Avoid Logging Sensitive Information:</p> <ul> <li>Never log passwords, API keys, or personal data</li> <li>Be cautious with user-provided data</li> </ul> </li> </ol>"},{"location":"guides/get-started/logging/#debugging-with-logs","title":"Debugging with Logs","text":"<p>Users can control log verbosity when running the application:</p> <pre><code># Default INFO level\npython -m mangotango\n\n# Verbose DEBUG level for troubleshooting\npython -m mangotango --log-level DEBUG\n\n# Only show warnings and errors in log file\npython -m mangotango --log-level WARNING\n</code></pre>"},{"location":"guides/get-started/logging/#log-file-management","title":"Log File Management","text":"<ul> <li>Log files are automatically rotated when they reach 10MB</li> <li>Up to 5 backup files are kept (<code>mangotango.log.1</code>,\u00a0<code>mangotango.log.2</code>, etc.)</li> <li>Older backup files are automatically deleted</li> <li>Log directory is created automatically if it doesn't exist</li> </ul>"},{"location":"guides/get-started/logging/#testing-with-logs","title":"Testing with Logs","text":"<p>When writing tests that involve logging:</p> <pre><code>import logging\nfrom app.logger import get_logger\n\ndef test_my_function_logs_correctly(caplog):\n    with caplog.at_level(logging.INFO):\n        my_function()\n\n    assert \"Expected log message\" in caplog.text\n</code></pre>"},{"location":"guides/get-started/logging/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/get-started/overview/","title":"Overview","text":""},{"location":"guides/get-started/overview/#mango-tango-cli","title":"Mango Tango CLI","text":""},{"location":"guides/get-started/overview/#repository-overview","title":"Repository Overview","text":"<p>Mango Tango CLI is a Python terminal-based tool for social media data analysis and visualization. It provides a modular, extensible architecture that separates core application logic from analysis modules, ensuring consistent UX while allowing easy contribution of new analyzers. The following documentation in this section is meant to provide a general overview of how the codebase for the project is structured, and to provide some context on patterns used throughout the project.</p>"},{"location":"guides/get-started/overview/#purpose-domain","title":"Purpose &amp; Domain","text":"<ul> <li>Social Media Analytics: Hashtag analysis, n-gram analysis, temporal   patterns, user coordination</li> <li>Modular Architecture: Clear separation between data import/export,   analysis, and presentation</li> <li>Interactive Workflows: Terminal-based UI with web dashboard capabilities</li> <li>Extensible Design: Plugin-like analyzer system for easy expansion</li> </ul>"},{"location":"guides/get-started/overview/#tech-stack","title":"Tech Stack","text":"<ul> <li>Core: Python 3.12, Inquirer (CLI), TinyDB (metadata), Starlette &amp; Uvicorn (web-server)</li> <li>Data: Polars/Pandas, PyArrow, Parquet files</li> <li>Web: Dash, Shiny for Python, Plotly, React</li> <li>Dev Tools: Black, isort, pytest, PyInstaller</li> </ul>"},{"location":"guides/get-started/overview/#semantic-code-structure","title":"Semantic Code Structure","text":""},{"location":"guides/get-started/overview/#entry-points","title":"Entry Points","text":"<ul> <li><code>mangotango.py</code> - Main application bootstrap</li> <li><code>python -m mangotango</code> - Standard execution command</li> </ul>"},{"location":"guides/get-started/overview/#core-architecture-mvc-like","title":"Core Architecture (MVC-like)","text":"<ul> <li>Application Layer (<code>app/</code>): Workspace logic, analysis orchestration</li> <li>View Layer (<code>components/</code>): Terminal UI components using inquirer</li> <li>Model Layer (<code>storage/</code>): Data persistence, project/analysis models</li> </ul>"},{"location":"guides/get-started/overview/#domain-separation","title":"Domain Separation","text":"<ol> <li>Core Domain: Application, Terminal Components, Storage IO</li> <li>Edge Domain: Data import/export (<code>importing/</code>), preprocessing</li> <li>Content Domain: Analyzers (<code>analyzers/</code>), web presenters</li> </ol>"},{"location":"guides/get-started/overview/#key-data-flow","title":"Key Data Flow","text":"<ol> <li>Import (CSV/Excel) \u2192 Parquet \u2192 Semantic preprocessing</li> <li>Primary Analysis \u2192 Secondary Analysis \u2192 Web Presentation</li> <li>Export \u2192 User-selected formats (XLSX, CSV, etc.)</li> </ol>"},{"location":"guides/get-started/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/get-started/overview/#analyzer-system","title":"Analyzer System","text":"<ul> <li>Primary Analyzers: Core data processing (hashtags, ngrams, temporal)</li> <li>Secondary Analyzers: User-friendly output transformation</li> <li>Web Presenters: Interactive dashboards using Dash/Shiny/React   (Also used for providing data to backend APIs)</li> <li>Interface Pattern: Declarative input/output schema definitions</li> </ul>"},{"location":"guides/get-started/overview/#context-pattern","title":"Context Pattern","text":"<p>Dependency injection through context objects:</p> <ul> <li><code>AppContext</code>: Application-wide dependencies</li> <li><code>ViewContext</code>: UI state and terminal context</li> <li><code>AnalysisContext</code>: Analysis execution environment</li> <li>Analyzer contexts: File paths, preprocessing, app hooks</li> </ul>"},{"location":"guides/get-started/overview/#data-semantics","title":"Data Semantics","text":"<ul> <li>Column semantic types guide user in analysis selection</li> <li>Preprocessing maps user data to expected analyzer inputs</li> <li>Type-safe data models using Pydantic</li> </ul>"},{"location":"guides/get-started/overview/#development-patterns","title":"Development Patterns","text":""},{"location":"guides/get-started/overview/#code-organization","title":"Code Organization","text":"<ul> <li>Domain-driven module structure</li> <li>Interface-first analyzer design  </li> <li>Context-based dependency injection</li> <li>Test co-location with implementation</li> </ul>"},{"location":"guides/get-started/overview/#key-conventions","title":"Key Conventions","text":"<ul> <li>Black + isort formatting (enforced by pre-commit)</li> <li>Type hints throughout (modern Python syntax)</li> <li>Parquet for data persistence</li> <li>Pydantic models for validation</li> </ul>"},{"location":"guides/get-started/overview/#getting-started","title":"Getting Started","text":""},{"location":"guides/get-started/overview/#for-development","title":"For Development","text":"<ol> <li>Setup: See Setup Guide</li> <li>Contribution Workflow: See The Contribution Workflow Guide</li> <li>Development Guide: See The Development Guide</li> </ol>"},{"location":"guides/get-started/overview/#for-ai-assistants","title":"For AI Assistants","text":"<ul> <li>Claude Code users: See CLAUDE.md (includes Serena integration)</li> <li>Cursor users: See .cursorrules</li> <li>Deep semantic analysis: Explore .serena/memories/</li> </ul>"},{"location":"guides/get-started/overview/#quick-references","title":"Quick References","text":"<ul> <li>Commands: .serena/memories/suggested_commands.md</li> <li>Style Guide: .serena/memories/code_style_conventions.md</li> <li>Task Checklist: .serena/memories/task_completion_checklist.md</li> </ul>"},{"location":"guides/get-started/overview/#external-dependencies","title":"External Dependencies","text":""},{"location":"guides/get-started/overview/#data-processing","title":"Data Processing","text":"<ul> <li><code>polars</code> - Primary data processing library</li> <li><code>pandas</code> - Secondary support for Plotly integration</li> <li><code>pyarrow</code> - Parquet file format support</li> </ul>"},{"location":"guides/get-started/overview/#web-framework","title":"Web Framework","text":"<ul> <li><code>dash</code> - Interactive web dashboards</li> <li><code>shiny</code> - Python Shiny for modern web UIs</li> <li><code>plotly</code> - Visualization library</li> <li><code>React</code> - Stylized Interactive Dashboards for the end-user</li> <li><code>Starlette</code> - Web Framework for providing the dashboards and handling backend logic</li> <li><code>Uvicorn</code> - ASGI web-server that handles running the Starlette backend</li> </ul>"},{"location":"guides/get-started/overview/#cli-storage","title":"CLI &amp; Storage","text":"<ul> <li><code>inquirer</code> - Interactive terminal prompts</li> <li><code>tinydb</code> - Lightweight JSON database</li> <li><code>platformdirs</code> - Cross-platform data directories</li> </ul>"},{"location":"guides/get-started/overview/#development","title":"Development","text":"<ul> <li><code>black</code> - Code formatter</li> <li><code>isort</code> - Import organizer</li> <li><code>pytest</code> - Testing framework</li> <li><code>pyinstaller</code> - Executable building</li> </ul>"},{"location":"guides/get-started/overview/#project-status","title":"Project Status","text":"<ul> <li>License: PolyForm Noncommercial License 1.0.0</li> <li>Author: CIB Mango Tree / Civic Tech DC</li> <li>Branch Strategy: feature branches \u2192 develop \u2192 main</li> <li>CI/CD: GitHub Actions for testing, formatting, builds</li> </ul>"},{"location":"guides/get-started/testing/","title":"Testing","text":""},{"location":"guides/get-started/testing/#testing","title":"Testing","text":"<p>The <code>testing</code> module provides testers for the primary and secondary analyzer modules. See the example for further references.</p>"},{"location":"reference/analyzer_interface/","title":"Analyzer Interface","text":""},{"location":"reference/analyzer_interface/#analyzer_interface","title":"<code>analyzer_interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap","title":"<code>column_automap</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.check_name_hint","title":"<code>check_name_hint(name, hint)</code>","text":"<p>Returns true if every word in the hint (split by spaces) is present in the name, in a case insensitive manner.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def check_name_hint(name: str, hint: str):\n    \"\"\"\n    Returns true if every word in the hint (split by spaces) is present in the name,\n    in a case insensitive manner.\n    \"\"\"\n    return all(word.lower().strip() in name.lower() for word in hint.split(\" \"))\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.column_automap","title":"<code>column_automap(user_columns, input_schema_columns)</code>","text":"<p>Matches user-provided columns to the expected columns based on the name hints.</p> <p>The resulting dictionary is keyed by the expected input column name.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def column_automap(\n    user_columns: list[UserInputColumn], input_schema_columns: list[InputColumn]\n):\n    \"\"\"\n    Matches user-provided columns to the expected columns based on the name hints.\n\n    The resulting dictionary is keyed by the expected input column name.\n    \"\"\"\n    matches: dict[str, str] = {}\n    for input_column in input_schema_columns:\n        max_score = None\n        best_match_user_column = None\n        for user_column in user_columns:\n            current_score = get_data_type_compatibility_score(\n                input_column.data_type, user_column.data_type\n            )\n\n            # Don't consider type-incompatible columns\n            if current_score is None:\n                continue\n\n            # Boost the score if we have a name hint match such that\n            # - among similarly compatible matches, those with name hints are preferred\n            # - among name hint matches, those with the best data type compatibility are preferred\n            if any(\n                check_name_hint(user_column.name, hint)\n                for hint in input_column.name_hints\n            ):\n                current_score += 10\n\n            if max_score is None or current_score &gt; max_score:\n                max_score = current_score\n                best_match_user_column = user_column\n\n        if best_match_user_column is not None:\n            matches[input_column.name] = best_match_user_column.name\n\n    return matches\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context","title":"<code>context</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.context.AssetsReader","title":"<code>AssetsReader</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class AssetsReader(ABC):\n    @abstractmethod\n    def table(self, output_id: str) -&gt; \"TableReader\":\n        \"\"\"\n        Gets the table reader for the specified output.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.AssetsReader.table","title":"<code>table(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the table reader for the specified output.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef table(self, output_id: str) -&gt; \"TableReader\":\n    \"\"\"\n    Gets the table reader for the specified output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext","title":"<code>BaseDerivedModuleContext</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Common interface for secondary analyzers and web presenters runtime contexts.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class BaseDerivedModuleContext(ABC, BaseModel):\n    \"\"\"\n    Common interface for secondary analyzers and web presenters runtime contexts.\n    \"\"\"\n\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def base_params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the primary analysis parameters.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def base(self) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the base primary analyzer's context, which lets you inspect and load its\n        outputs.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dependency(\n        self, secondary_interface: SecondaryAnalyzerInterface\n    ) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the context of a secondary analyzer the current module depends on, which\n        lets you inspect and load its outputs.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.base","title":"<code>base</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the base primary analyzer's context, which lets you inspect and load its outputs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.base_params","title":"<code>base_params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the primary analysis parameters.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.temp_dir","title":"<code>temp_dir</code>  <code>instance-attribute</code>","text":"<p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext.dependency","title":"<code>dependency(secondary_interface)</code>  <code>abstractmethod</code>","text":"<p>Gets the context of a secondary analyzer the current module depends on, which lets you inspect and load its outputs.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef dependency(\n    self, secondary_interface: SecondaryAnalyzerInterface\n) -&gt; \"AssetsReader\":\n    \"\"\"\n    Gets the context of a secondary analyzer the current module depends on, which\n    lets you inspect and load its outputs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext","title":"<code>FactoryOutputContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for both factory and api_facotry functions for web presenters.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class FactoryOutputContext(BaseModel):\n    \"\"\"\n    Output interface for both factory and api_facotry functions for web\n    presenters.\n    \"\"\"\n\n    shiny: Optional[ShinyContext] = None\n    \"\"\"\n    Factory oputput for shiny dashboards\n    \"\"\"\n\n    api: Optional[dict[str, Any]] = None\n    \"\"\"\n    API factory output for React dashboard REST API\n    \"\"\"\n\n    data_frames: Optional[dict[str, DataFrame]] = None\n    \"\"\"\n    API factory dataframe output for React dashboard REST API\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.api","title":"<code>api = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>API factory output for React dashboard REST API</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.data_frames","title":"<code>data_frames = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>API factory dataframe output for React dashboard REST API</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext.shiny","title":"<code>shiny = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Factory oputput for shiny dashboards</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.InputTableReader","title":"<code>InputTableReader</code>","text":"<p>               Bases: <code>TableReader</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class InputTableReader(TableReader):\n    @abstractmethod\n    def preprocess[PolarsDataFrameLike](\n        self, df: PolarsDataFrameLike\n    ) -&gt; PolarsDataFrameLike:\n        \"\"\"\n        Given the manually loaded user input dataframe, apply column mapping and\n        semantic transformations to give the input dataframe that the analyzer\n        expects.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.InputTableReader.preprocess","title":"<code>preprocess(df)</code>  <code>abstractmethod</code>","text":"<p>Given the manually loaded user input dataframe, apply column mapping and semantic transformations to give the input dataframe that the analyzer expects.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef preprocess[PolarsDataFrameLike](\n    self, df: PolarsDataFrameLike\n) -&gt; PolarsDataFrameLike:\n    \"\"\"\n    Given the manually loaded user input dataframe, apply column mapping and\n    semantic transformations to give the input dataframe that the analyzer\n    expects.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext","title":"<code>PrimaryAnalyzerContext</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class PrimaryAnalyzerContext(ABC, BaseModel):\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    @abstractmethod\n    def input(self) -&gt; \"InputTableReader\":\n        \"\"\"\n        Gets the input reader context.\n\n        **Note that this is in function form** even though one input is expected,\n        in anticipation that we may want to support multiple inputs in the future.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the analysis parameters.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context for the specified output ID.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.params","title":"<code>params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the analysis parameters.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.temp_dir","title":"<code>temp_dir</code>  <code>instance-attribute</code>","text":"<p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.input","title":"<code>input()</code>  <code>abstractmethod</code>","text":"<p>Gets the input reader context.</p> <p>Note that this is in function form even though one input is expected, in anticipation that we may want to support multiple inputs in the future.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef input(self) -&gt; \"InputTableReader\":\n    \"\"\"\n    Gets the input reader context.\n\n    **Note that this is in function form** even though one input is expected,\n    in anticipation that we may want to support multiple inputs in the future.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext.output","title":"<code>output(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the output writer context for the specified output ID.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context for the specified output ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.SecondaryAnalyzerContext","title":"<code>SecondaryAnalyzerContext</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class SecondaryAnalyzerContext(BaseDerivedModuleContext):\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.SecondaryAnalyzerContext.output","title":"<code>output(output_id)</code>  <code>abstractmethod</code>","text":"<p>Gets the output writer context</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext","title":"<code>ShinyContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for Shiny dashboards</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class ShinyContext(BaseModel):\n    \"\"\"\n    Output interface for Shiny dashboards\n    \"\"\"\n\n    panel: NavPanel = None\n    \"\"\"\n    UI navigation panel to be added to shiny dashboard\n    \"\"\"\n\n    server_handler: Optional[ServerCallback] = None\n    \"\"\"\n    Server handler callback to be called by the shiny application instance\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext.panel","title":"<code>panel = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>UI navigation panel to be added to shiny dashboard</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext.server_handler","title":"<code>server_handler = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Server handler callback to be called by the shiny application instance</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableReader","title":"<code>TableReader</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableReader(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should expect a parquet\n        file here.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableReader.parquet_path","title":"<code>parquet_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the path to the table's parquet file. The module should expect a parquet file here.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableWriter(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should write a parquet\n        file to it.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableWriter.parquet_path","title":"<code>parquet_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the path to the table's parquet file. The module should write a parquet file to it.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext","title":"<code>WebPresenterContext</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class WebPresenterContext(BaseDerivedModuleContext):\n    dash_app: Dash\n    \"\"\"\n  The Dash app that is being built.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def state_dir(self) -&gt; str:\n        \"\"\"\n        Gets the directory where the web presenter can store state that persists\n        between runs. This state space is unique for each\n        project/primary analyzer/web presenter combination.\n        \"\"\"\n        pass\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext.dash_app","title":"<code>dash_app</code>  <code>instance-attribute</code>","text":"<p>The Dash app that is being built.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext.state_dir","title":"<code>state_dir</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Gets the directory where the web presenter can store state that persists between runs. This state space is unique for each project/primary analyzer/web presenter combination.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility","title":"<code>data_type_compatibility</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.data_type_mapping_preference","title":"<code>data_type_mapping_preference = {'text': [['text'], ['identifier', 'url']], 'integer': [['integer']], 'float': [['float', 'integer']], 'boolean': [['boolean']], 'datetime': [['datetime']], 'time': [['time'], ['datetime']], 'identifier': [['identifier'], ['url', 'datetime'], ['integer'], ['text']], 'url': [['url']]}</code>  <code>module-attribute</code>","text":"<p>For each data type, a list of lists of data types that are considered compatible with it. The first list is the most preferred, the last list is the least. The items in each list are considered equally compatible.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.get_data_type_compatibility_score","title":"<code>get_data_type_compatibility_score(expected_data_type, actual_data_type)</code>","text":"<p>Returns a score for the compatibility of the actual data type with the expected data type. Higher (less negative) scores are better. <code>None</code> means the data types are not compatible.</p> Source code in <code>analyzer_interface/data_type_compatibility.py</code> <pre><code>def get_data_type_compatibility_score(\n    expected_data_type: DataType, actual_data_type: DataType\n):\n    \"\"\"\n    Returns a score for the compatibility of the actual data type with the\n    expected data type. Higher (less negative) scores are better.\n    `None` means the data types are not compatible.\n    \"\"\"\n    if expected_data_type == actual_data_type:\n        return 0\n\n    for i, preference_list in enumerate(\n        data_type_mapping_preference[expected_data_type]\n    ):\n        if actual_data_type in preference_list:\n            return -(i + 1)\n\n    return None\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration","title":"<code>declaration</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration","title":"<code>AnalyzerDeclaration</code>","text":"<p>               Bases: <code>AnalyzerInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class AnalyzerDeclaration(AnalyzerInterface):\n    entry_point: Callable[[PrimaryAnalyzerContext], None]\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]]\n    is_distributed: bool\n\n    def __init__(\n        self,\n        interface: AnalyzerInterface,\n        main: Callable,\n        *,\n        is_distributed: bool = False,\n        default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n            lambda _: dict()\n        )\n    ):\n        \"\"\"Creates a primary analyzer declaration\n\n        Args:\n          interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n          main (Callable):\n            The entry point function for the primary analyzer. This function should\n            take a single argument of type `PrimaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n\n          is_distributed (bool):\n            Set this explicitly to `True` once the analyzer is ready to be shipped\n            to end users; it will make the analyzer available in the distributed\n            executable.\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(),\n            entry_point=main,\n            default_params=default_params,\n            is_distributed=is_distributed\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__","title":"<code>__init__(interface, main, *, is_distributed=False, default_params=lambda _: dict())</code>","text":"<p>Creates a primary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The metadata interface for the primary analyzer.</p> required <code>Callable</code> <p>The entry point function for the primary analyzer. This function should take a single argument of type <code>PrimaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required <code>bool</code> <p>Set this explicitly to <code>True</code> once the analyzer is ready to be shipped to end users; it will make the analyzer available in the distributed executable.</p> <code>False</code> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: AnalyzerInterface,\n    main: Callable,\n    *,\n    is_distributed: bool = False,\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n        lambda _: dict()\n    )\n):\n    \"\"\"Creates a primary analyzer declaration\n\n    Args:\n      interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n      main (Callable):\n        The entry point function for the primary analyzer. This function should\n        take a single argument of type `PrimaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n\n      is_distributed (bool):\n        Set this explicitly to `True` once the analyzer is ready to be shipped\n        to end users; it will make the analyzer available in the distributed\n        executable.\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(),\n        entry_point=main,\n        default_params=default_params,\n        is_distributed=is_distributed\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(main)","title":"<code>main</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration.__init__(is_distributed)","title":"<code>is_distributed</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration","title":"<code>SecondaryAnalyzerDeclaration</code>","text":"<p>               Bases: <code>SecondaryAnalyzerInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class SecondaryAnalyzerDeclaration(SecondaryAnalyzerInterface):\n    entry_point: Callable[[\"SecondaryAnalyzerContext\"], None]\n\n    def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n        \"\"\"Creates a secondary analyzer declaration\n\n        Args:\n          interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n          main (Callable):\n            The entry point function for the secondary analyzer. This function should\n            take a single argument of type `SecondaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n        \"\"\"\n        super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__","title":"<code>__init__(interface, main)</code>","text":"<p>Creates a secondary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>SecondaryAnalyzerInterface</code> <p>The metadata interface for the secondary analyzer.</p> required <code>Callable</code> <p>The entry point function for the secondary analyzer. This function should take a single argument of type <code>SecondaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n    \"\"\"Creates a secondary analyzer declaration\n\n    Args:\n      interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n      main (Callable):\n        The entry point function for the secondary analyzer. This function should\n        take a single argument of type `SecondaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n    \"\"\"\n    super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration.__init__(main)","title":"<code>main</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration","title":"<code>WebPresenterDeclaration</code>","text":"<p>               Bases: <code>WebPresenterInterface</code></p> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class WebPresenterDeclaration(WebPresenterInterface):\n    factory: Callable[[\"WebPresenterContext\"], Union[FactoryOutputContext, None]]\n    shiny: bool\n    server_name: str\n\n    def __init__(\n        self,\n        interface: WebPresenterInterface,\n        factory: Callable,\n        name: str,\n        shiny: bool,\n    ):\n        \"\"\"Creates a web presenter declaration\n\n        Args:\n          interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n          factory (Callable):\n            The factory function that creates a Dash app for the web presenter. It should\n            modify the Dash app in the context to add whatever plotting interface\n            the web presenter needs.\n\n          server_name (str):\n            The server name for the Dash app. Typically, you will use the global\n            variable `__name__` here.\n\n            If your web presenter has assets like images, CSS or JavaScript files,\n            you can put them in a folder named `assets` in the same directory\n            as the file where `__name__` is used. The Dash app will serve these\n            files at the `/assets/` URL, using the python module name in `__name__`\n            to determine the absolute path to the assets folder.\n\n            See Dash documentation for more details: https://dash.plotly.com\n            See also Python documentation for the `__name__` variable:\n            https://docs.python.org/3/tutorial/modules.html\n\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__","title":"<code>__init__(interface, factory, name, shiny)</code>","text":"<p>Creates a web presenter declaration</p> <p>Parameters:</p> Name Type Description Default <code>WebPresenterInterface</code> <p>The metadata interface for the web presenter.</p> required <code>Callable</code> <p>The factory function that creates a Dash app for the web presenter. It should modify the Dash app in the context to add whatever plotting interface the web presenter needs.</p> required <code>str</code> <p>The server name for the Dash app. Typically, you will use the global variable <code>__name__</code> here.</p> <p>If your web presenter has assets like images, CSS or JavaScript files, you can put them in a folder named <code>assets</code> in the same directory as the file where <code>__name__</code> is used. The Dash app will serve these files at the <code>/assets/</code> URL, using the python module name in <code>__name__</code> to determine the absolute path to the assets folder.</p> <p>See Dash documentation for more details: https://dash.plotly.com See also Python documentation for the <code>__name__</code> variable: https://docs.python.org/3/tutorial/modules.html</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: WebPresenterInterface,\n    factory: Callable,\n    name: str,\n    shiny: bool,\n):\n    \"\"\"Creates a web presenter declaration\n\n    Args:\n      interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n      factory (Callable):\n        The factory function that creates a Dash app for the web presenter. It should\n        modify the Dash app in the context to add whatever plotting interface\n        the web presenter needs.\n\n      server_name (str):\n        The server name for the Dash app. Typically, you will use the global\n        variable `__name__` here.\n\n        If your web presenter has assets like images, CSS or JavaScript files,\n        you can put them in a folder named `assets` in the same directory\n        as the file where `__name__` is used. The Dash app will serve these\n        files at the `/assets/` URL, using the python module name in `__name__`\n        to determine the absolute path to the assets folder.\n\n        See Dash documentation for more details: https://dash.plotly.com\n        See also Python documentation for the `__name__` variable:\n        https://docs.python.org/3/tutorial/modules.html\n\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(interface)","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(factory)","title":"<code>factory</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration.__init__(server_name)","title":"<code>server_name</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface","title":"<code>interface</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DataType","title":"<code>DataType = Literal['text', 'integer', 'float', 'boolean', 'datetime', 'identifier', 'url', 'time']</code>  <code>module-attribute</code>","text":"<p>The semantic data type for a data column. This is not quite the same as structural data types like polars or pandas or even arrow types, but they represent how the data is intended to be interpreted.</p> <ul> <li><code>text</code> is expected to be a free-form human-readable text content.</li> <li><code>integer</code> and <code>float</code> are meant to be manipulated arithmetically.</li> <li><code>boolean</code> is a binary value.</li> <li><code>datetime</code> represents time and are meant to be manipulated as time values.</li> <li><code>time</code> represents time within a day, not including the date information.</li> <li><code>identifier</code> is a unique identifier for a record. It is not expected to be manipulated in any way.</li> <li><code>url</code> is a string that represents a URL.</li> </ul>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface","title":"<code>AnalyzerInterface</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerInterface(BaseAnalyzerInterface):\n    input: AnalyzerInput\n    \"\"\"\n  Specifies the input data schema for the analyzer.\n  \"\"\"\n\n    params: list[AnalyzerParam] = []\n    \"\"\"\n  A list of parameters that the analyzer accepts.\n  \"\"\"\n\n    outputs: list[\"AnalyzerOutput\"]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"primary\"] = \"primary\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>Specifies the input data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.outputs","title":"<code>outputs</code>  <code>instance-attribute</code>","text":"<p>Specifies the output data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface.params","title":"<code>params = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of parameters that the analyzer accepts.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput","title":"<code>AnalyzerOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerOutput(BaseModel):\n    id: str\n    \"\"\"\n  Uniquely identifies the output data schema for the analyzer. The analyzer\n  must include this key in the output dictionary.\n  \"\"\"\n\n    name: str\n    \"\"\"The human-friendly for the output.\"\"\"\n\n    description: Optional[str] = None\n\n    columns: list[\"OutputColumn\"]\n\n    internal: bool = False\n\n    def get_column_by_name(self, name: str):\n        for column in self.columns:\n            if column.name == name:\n                return column\n        return None\n\n    def transform_output(self, output_df: pl.LazyFrame | pl.DataFrame):\n        output_columns = output_df.lazy().collect_schema().names()\n        return output_df.select(\n            [\n                pl.col(col_name).alias(\n                    output_spec.human_readable_name_or_fallback()\n                    if output_spec\n                    else col_name\n                )\n                for col_name in output_columns\n                if (output_spec := self.get_column_by_name(col_name)) or True\n            ]\n        )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>Uniquely identifies the output data schema for the analyzer. The analyzer must include this key in the output dictionary.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The human-friendly for the output.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam","title":"<code>AnalyzerParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerParam(BaseModel):\n    id: str\n    \"\"\"\n    The name of the parameter. This becomes the key in the parameters dictionary\n    that is passed to the analyzer.\n    \"\"\"\n\n    human_readable_name: Optional[str] = None\n    \"\"\"\n    The human-friendly name for the parameter. This is used in the UI to\n    represent the parameter.\n    \"\"\"\n\n    description: Optional[str] = None\n    \"\"\"\n    A short description of the parameter. This is used in the UI to represent\n    the parameter.\n    \"\"\"\n\n    type: ParamType\n    \"\"\"\n    The type of the parameter. This is used for validation and for customizing\n    the UX for parameter input.\n    \"\"\"\n\n    default: Optional[ParamValue] = None\n    \"\"\"\n    Optional: define a static default value for this parameter. A parameter\n    without a default will need to be chosen explicitly by the user.\n    \"\"\"\n\n    backfill_value: Optional[ParamValue] = None\n    \"\"\"\n    Recommended if this is a parameter that is newly introduced in a previously\n    released analyzer. The backfill is show what this parameter was before it\n    became customizable.\n    \"\"\"\n\n    @property\n    def print_name(self):\n        return self.human_readable_name or self.id\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.backfill_value","title":"<code>backfill_value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Recommended if this is a parameter that is newly introduced in a previously released analyzer. The backfill is show what this parameter was before it became customizable.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.default","title":"<code>default = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional: define a static default value for this parameter. A parameter without a default will need to be chosen explicitly by the user.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A short description of the parameter. This is used in the UI to represent the parameter.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.human_readable_name","title":"<code>human_readable_name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The human-friendly name for the parameter. This is used in the UI to represent the parameter.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>The name of the parameter. This becomes the key in the parameters dictionary that is passed to the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>The type of the parameter. This is used for validation and for customizing the UX for parameter input.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface","title":"<code>BaseAnalyzerInterface</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class BaseAnalyzerInterface(BaseModel):\n    id: str\n    \"\"\"\n  The static ID for the analyzer that, with the version, uniquely identifies the\n  analyzer and will be stored as metadata as part of the output data.\n  \"\"\"\n\n    version: str\n    \"\"\"\n  The version ID for the analyzer. In future, we may choose to support output\n  migration between versions of the same analyzer.\n  \"\"\"\n\n    name: str\n    \"\"\"\n  The short human-readable name of the analyzer.\n  \"\"\"\n\n    short_description: str\n    \"\"\"\n  A short, one-liner description of what the analyzer does.\n  \"\"\"\n\n    long_description: Optional[str] = None\n    \"\"\"\n  A longer description of what the analyzer does that will be shown separately.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.id","title":"<code>id</code>  <code>instance-attribute</code>","text":"<p>The static ID for the analyzer that, with the version, uniquely identifies the analyzer and will be stored as metadata as part of the output data.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.long_description","title":"<code>long_description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A longer description of what the analyzer does that will be shown separately.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The short human-readable name of the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.short_description","title":"<code>short_description</code>  <code>instance-attribute</code>","text":"<p>A short, one-liner description of what the analyzer does.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface.version","title":"<code>version</code>  <code>instance-attribute</code>","text":"<p>The version ID for the analyzer. In future, we may choose to support output migration between versions of the same analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface","title":"<code>DerivedAnalyzerInterface</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class DerivedAnalyzerInterface(BaseAnalyzerInterface):\n    base_analyzer: AnalyzerInterface\n    \"\"\"\n  The base analyzer that this secondary analyzer extends. This is always a primary\n  analyzer. If your module depends on other secondary analyzers (which must have\n  the same base analyzer), you can specify them in the `depends_on` field.\n  \"\"\"\n\n    depends_on: list[\"SecondaryAnalyzerInterface\"] = []\n    \"\"\"\n  A dictionary of secondary analyzers that must be run before the current analyzer\n  secondary analyzer is run. These secondary analyzers must have the same\n  primary base.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface.base_analyzer","title":"<code>base_analyzer</code>  <code>instance-attribute</code>","text":"<p>The base analyzer that this secondary analyzer extends. This is always a primary analyzer. If your module depends on other secondary analyzers (which must have the same base analyzer), you can specify them in the <code>depends_on</code> field.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface.depends_on","title":"<code>depends_on = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of secondary analyzers that must be run before the current analyzer secondary analyzer is run. These secondary analyzers must have the same primary base.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.InputColumn","title":"<code>InputColumn</code>","text":"<p>               Bases: <code>Column</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class InputColumn(Column):\n    name_hints: list[str] = []\n    \"\"\"\n  Specifies a list of space-separated words that are likely to be found in the\n  column name of the user-provided data. This is used to help the user map the\n  input columns to the expected columns.\n\n  Any individual hint matching is sufficient for a match to be called. The hint\n  in turn is matched if every word matches some part of the column name.\n  \"\"\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.InputColumn.name_hints","title":"<code>name_hints = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specifies a list of space-separated words that are likely to be found in the column name of the user-provided data. This is used to help the user map the input columns to the expected columns.</p> <p>Any individual hint matching is sufficient for a match to be called. The hint in turn is matched if every word matches some part of the column name.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.SecondaryAnalyzerInterface","title":"<code>SecondaryAnalyzerInterface</code>","text":"<p>               Bases: <code>DerivedAnalyzerInterface</code></p> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class SecondaryAnalyzerInterface(DerivedAnalyzerInterface):\n    outputs: list[AnalyzerOutput]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"secondary\"] = \"secondary\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.SecondaryAnalyzerInterface.outputs","title":"<code>outputs</code>  <code>instance-attribute</code>","text":"<p>Specifies the output data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.params","title":"<code>params</code>","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.params.IntegerParam","title":"<code>IntegerParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an integer value</p> <p>The corresponding value will be of type <code>int</code>.</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class IntegerParam(BaseModel):\n    \"\"\"\n    Represents an integer value\n\n    The corresponding value will be of type `int`.\n    \"\"\"\n\n    type: Literal[\"integer\"] = \"integer\"\n    min: int\n    max: int\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningParam","title":"<code>TimeBinningParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a time bin.</p> <p>The corresponding value will be of type <code>TimeBinningValue</code>.</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningParam(BaseModel):\n    \"\"\"\n    Represents a time bin.\n\n    The corresponding value will be of type `TimeBinningValue`.\n    \"\"\"\n\n    type: Literal[\"time_binning\"] = \"time_binning\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningValue","title":"<code>TimeBinningValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningValue(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    unit: TimeBinningUnit\n    amount: int\n\n    def to_polars_truncate_spec(self) -&gt; str:\n        \"\"\"\n        Converts the value to a string that can be used in Polars truncate spec.\n        See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n        \"\"\"\n        amount = self.amount\n        unit = self.unit\n        if unit == \"year\":\n            return f\"{amount}y\"\n        if unit == \"month\":\n            return f\"{amount}mo\"\n        if unit == \"week\":\n            return f\"{amount}w\"\n        if unit == \"day\":\n            return f\"{amount}d\"\n        if unit == \"hour\":\n            return f\"{amount}h\"\n        if unit == \"minute\":\n            return f\"{amount}m\"\n        if unit == \"second\":\n            return f\"{amount}s\"\n\n        raise ValueError(\"Invalid time binning value\")\n\n    def to_human_readable_text(self) -&gt; str:\n        amount = self.amount\n        unit = self.unit\n\n        if unit == \"year\":\n            return f\"{amount} year{'s' if amount &gt; 1 else ''}\"\n        if unit == \"month\":\n            return f\"{amount} month{'s' if amount &gt; 1 else ''}\"\n        if unit == \"week\":\n            return f\"{amount} week{'s' if amount &gt; 1 else ''}\"\n        if unit == \"day\":\n            return f\"{amount} day{'s' if amount &gt; 1 else ''}\"\n        if unit == \"hour\":\n            return f\"{amount} hour{'s' if amount &gt; 1 else ''}\"\n        if unit == \"minute\":\n            return f\"{amount} minute{'s' if amount &gt; 1 else ''}\"\n        if unit == \"second\":\n            return f\"{amount} second{'s' if amount &gt; 1 else ''}\"\n\n        raise ValueError(\"Invalid time binning value\")\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningValue.to_polars_truncate_spec","title":"<code>to_polars_truncate_spec()</code>","text":"<p>Converts the value to a string that can be used in Polars truncate spec. See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>def to_polars_truncate_spec(self) -&gt; str:\n    \"\"\"\n    Converts the value to a string that can be used in Polars truncate spec.\n    See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n    \"\"\"\n    amount = self.amount\n    unit = self.unit\n    if unit == \"year\":\n        return f\"{amount}y\"\n    if unit == \"month\":\n        return f\"{amount}mo\"\n    if unit == \"week\":\n        return f\"{amount}w\"\n    if unit == \"day\":\n        return f\"{amount}d\"\n    if unit == \"hour\":\n        return f\"{amount}h\"\n    if unit == \"minute\":\n        return f\"{amount}m\"\n    if unit == \"second\":\n        return f\"{amount}s\"\n\n    raise ValueError(\"Invalid time binning value\")\n</code></pre>"},{"location":"reference/app/","title":"App","text":""},{"location":"reference/app/#app","title":"<code>app</code>","text":""},{"location":"reference/components/","title":"Components","text":""},{"location":"reference/components/#components","title":"<code>components</code>","text":"<p>The application's terminal components that will be accessed by the entry module.</p>"},{"location":"reference/components/#components.splash","title":"<code>splash</code>","text":""},{"location":"reference/importing/","title":"Importing","text":""},{"location":"reference/importing/#importing","title":"<code>importing</code>","text":""},{"location":"reference/importing/#importing.importer","title":"<code>importer</code>","text":""},{"location":"reference/importing/#importing.importer.Importer","title":"<code>Importer</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>importing/importer.py</code> <pre><code>class Importer[SessionType](ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"\n        The name of the importer. It will be quoted in the UI in texts such as\n        \"Imported as `name`, so keep it to a format name.\"\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def suggest(self, input_path: str) -&gt; bool:\n        \"\"\"\n        Check if the importer can handle the given file. This should be fairly\n        restrictive based on reasonable assumptions, as it is only used for the\n        initial importer suggestion. The user can always override the suggestion.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        \"\"\"\n        Produces an initial import session object that contains all the configuration\n        needed for the import. The user can either accept this configuration or\n        customize it.\n\n        Return None here if the importer cannot figure out how to configure the\n        import parameters. This doesn't necessarily mean that the file cannot be\n        loaded; the UI will force the user to customize the import session if the\n        user wants to proceed with this importer.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def manual_init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        pass\n\n    @abstractmethod\n    def modify_session(\n        self,\n        input_path: str,\n        import_session: SessionType,\n        reset_screen: Callable[[SessionType], None],\n    ) -&gt; Optional[SessionType]:\n        \"\"\"\n        Performs the interactive UI sequence that customizes the import session\n        from the initial one.\n\n        Return None here if the user interrupts the customization process.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The name of the importer. It will be quoted in the UI in texts such as \"Imported as <code>name</code>, so keep it to a format name.\"</p>"},{"location":"reference/importing/#importing.importer.Importer.init_session","title":"<code>init_session(input_path)</code>  <code>abstractmethod</code>","text":"<p>Produces an initial import session object that contains all the configuration needed for the import. The user can either accept this configuration or customize it.</p> <p>Return None here if the importer cannot figure out how to configure the import parameters. This doesn't necessarily mean that the file cannot be loaded; the UI will force the user to customize the import session if the user wants to proceed with this importer.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef init_session(self, input_path: str) -&gt; Optional[SessionType]:\n    \"\"\"\n    Produces an initial import session object that contains all the configuration\n    needed for the import. The user can either accept this configuration or\n    customize it.\n\n    Return None here if the importer cannot figure out how to configure the\n    import parameters. This doesn't necessarily mean that the file cannot be\n    loaded; the UI will force the user to customize the import session if the\n    user wants to proceed with this importer.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.modify_session","title":"<code>modify_session(input_path, import_session, reset_screen)</code>  <code>abstractmethod</code>","text":"<p>Performs the interactive UI sequence that customizes the import session from the initial one.</p> <p>Return None here if the user interrupts the customization process.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef modify_session(\n    self,\n    input_path: str,\n    import_session: SessionType,\n    reset_screen: Callable[[SessionType], None],\n) -&gt; Optional[SessionType]:\n    \"\"\"\n    Performs the interactive UI sequence that customizes the import session\n    from the initial one.\n\n    Return None here if the user interrupts the customization process.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.Importer.suggest","title":"<code>suggest(input_path)</code>  <code>abstractmethod</code>","text":"<p>Check if the importer can handle the given file. This should be fairly restrictive based on reasonable assumptions, as it is only used for the initial importer suggestion. The user can always override the suggestion.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef suggest(self, input_path: str) -&gt; bool:\n    \"\"\"\n    Check if the importer can handle the given file. This should be fairly\n    restrictive based on reasonable assumptions, as it is only used for the\n    initial importer suggestion. The user can always override the suggestion.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession","title":"<code>ImporterSession</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The ImporterSession interface handles the ongoing configuration of an import. It keeps the configuration state, knows how to print the configuration to the console, and can load a preview of the data from the input file.</p> Source code in <code>importing/importer.py</code> <pre><code>class ImporterSession(ABC):\n    \"\"\"\n    The ImporterSession interface handles the ongoing configuration of an import.\n    It keeps the configuration state, knows how to print the configuration to the\n    console, and can load a preview of the data from the input file.\n    \"\"\"\n\n    @abstractmethod\n    def print_config(self) -&gt; None:\n        \"\"\"\n        Print the configuration of the import session to the console.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n        \"\"\"\n        Attempt to load a preview of the data from the input file.\n\n        Return None here if it is sure that the file cannot be loaded with the current\n        configuration. Only throw an execption in the case of unexpected errors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def import_as_parquet(self, output_path: str) -&gt; None:\n        \"\"\"\n        Import the data from the input file to the output file in the Parquet format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.import_as_parquet","title":"<code>import_as_parquet(output_path)</code>  <code>abstractmethod</code>","text":"<p>Import the data from the input file to the output file in the Parquet format.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef import_as_parquet(self, output_path: str) -&gt; None:\n    \"\"\"\n    Import the data from the input file to the output file in the Parquet format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.load_preview","title":"<code>load_preview(n_records)</code>  <code>abstractmethod</code>","text":"<p>Attempt to load a preview of the data from the input file.</p> <p>Return None here if it is sure that the file cannot be loaded with the current configuration. Only throw an execption in the case of unexpected errors.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n    \"\"\"\n    Attempt to load a preview of the data from the input file.\n\n    Return None here if it is sure that the file cannot be loaded with the current\n    configuration. Only throw an execption in the case of unexpected errors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession.print_config","title":"<code>print_config()</code>  <code>abstractmethod</code>","text":"<p>Print the configuration of the import session to the console.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef print_config(self) -&gt; None:\n    \"\"\"\n    Print the configuration of the import session to the console.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/meta/","title":"Meta","text":""},{"location":"reference/meta/#meta","title":"<code>meta</code>","text":""},{"location":"reference/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/preprocessing/#preprocessing","title":"<code>preprocessing</code>","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic","title":"<code>series_semantic</code>","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic.constant_series","title":"<code>constant_series(series, constant)</code>","text":"<p>Create a series with a constant value for each row of <code>series</code>.</p> Source code in <code>preprocessing/series_semantic.py</code> <pre><code>def constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"reference/storage/","title":"Storage","text":""},{"location":"reference/storage/#storage","title":"<code>storage</code>","text":""},{"location":"reference/storage/#storage.Storage","title":"<code>Storage</code>","text":"Source code in <code>storage/__init__.py</code> <pre><code>class Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return pl.read_parquet(output_path)\n\n    def get_secondary_output_parquet_path(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        return os.path.join(\n            self._get_project_secondary_output_root_path(analysis, secondary_id),\n            f\"{output_id}.parquet\",\n        )\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n</code></pre>"},{"location":"reference/terminal_tools/","title":"Terminal Tools","text":""},{"location":"reference/terminal_tools/#terminal-tools","title":"Terminal tools","text":""},{"location":"reference/terminal_tools/#terminal_tools","title":"<code>terminal_tools</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.inception","title":"<code>inception</code>","text":"<p>The inception module aids in creating nested terminal blocks (hence the name \"inception\"). It provides a <code>Context</code> class that manages a list of <code>Scope</code> instances. Each <code>Scope</code> instance represents a block of text that are buffered in memory and printed to the terminal at each refresh.</p>"},{"location":"reference/terminal_tools/#terminal_tools.inception.Scope","title":"<code>Scope</code>","text":"Source code in <code>terminal_tools/inception.py</code> <pre><code>class Scope:\n    def __init__(self, context: TerminalContext, text: str):\n        self.context = context\n        self.text = text\n\n    def print(self):\n        print(self.text)\n\n    def refresh(self):\n        \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n        self.context._refresh()\n\n    def __enter__(self):\n        self.context._append_scope(self)\n        self.context._refresh()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.context._remove_scope(self)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception.Scope.refresh","title":"<code>refresh()</code>","text":"<p>Clear the terminal and repaint with every scope up and including to this one</p> Source code in <code>terminal_tools/inception.py</code> <pre><code>def refresh(self):\n    \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n    self.context._refresh()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts","title":"<code>prompts</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.checkbox","title":"<code>checkbox(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s checkbox and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def checkbox(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s checkbox and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_checkbox(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.confirm","title":"<code>confirm(message, *, cancel_fallback=False, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s confirm input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def confirm(message: str, *, cancel_fallback: Optional[bool] = False, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s confirm input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(\n        lambda: inquirer_confirm(message, **kwargs), cancel_fallback\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector","title":"<code>file_selector(message='select a file', *, state=None)</code>","text":"<p>Lets the user select a file from the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt message. Defaults to \"select a file\".</p> <code>'select a file'</code> <code>str</code> <p>Where to start the directory listing. Defaults to current working directory.</p> required <p>Returns:</p> Type Description <code>(str, optional)</code> <p>The absolute path selected by the user, or None if the user cancels the prompt.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def file_selector(\n    message: str = \"select a file\", *, state: Optional[FileSelectorStateManager] = None\n):\n    \"\"\"Lets the user select a file from the filesystem.\n\n    Args:\n        message (str, optional): The prompt message. Defaults to \"select a file\".\n        initial_path (str, optional): Where to start the directory listing.\n          Defaults to current working directory.\n\n    Returns:\n        (str, optional): The absolute path selected by the user, or None if the\n          user cancels the prompt.\n    \"\"\"\n    initial_dir = state and state.get_current_path()\n    if initial_dir and not os.path.isdir(initial_dir):\n        initial_dir = None\n\n    current_path = os.path.realpath(initial_dir or os.curdir)\n\n    if os.name == \"nt\":\n        drives = get_drives()\n        drive_choices = [(drive, drive) for drive in drives]\n\n    def is_dir(entry: str):\n        return os.path.isdir(os.path.join(current_path, entry))\n\n    while True:\n        print(f\"current path: {current_path}\")\n        choices = [\n            (\"[..]\", \"..\"),\n            *(\n                (f\"[{entry}]\" if is_dir(entry) else entry, entry)\n                for entry in sorted(os.listdir(current_path))\n            ),\n        ]\n\n        # Add change drive option to the list of choices if on Windows\n        if os.name == \"nt\":\n            cur_drive = os.path.splitdrive(current_path)[0]\n            choices.insert(\n                0, (f\"[Change Drive (current - {cur_drive})]\", \"change_drive\")\n            )\n\n        selected_entry = list_input(message, choices=choices)\n\n        if selected_entry is not None and selected_entry == \"change_drive\":\n            selected_drive = list_input(\"Select a drive:\", choices=drive_choices)\n            if selected_drive is None:\n                return None\n\n            current_path = selected_entry = f\"{selected_drive}\\\\\"\n            # clear the prompted lines\n            clear_printed_lines(len(drives) + 1)\n\n        # inquirer will show up to 14 lines including the header\n        # we have one line for the current path to rewrite\n        clear_printed_lines(min(len(choices), 13) + 2)\n\n        if selected_entry is None:\n            return None\n\n        if is_dir(selected_entry):\n            current_path = os.path.realpath(os.path.join(current_path, selected_entry))\n        else:\n            if state is not None:\n                state.set_current_path(current_path)\n            return os.path.join(current_path, selected_entry)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector(message)","title":"<code>message</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector(initial_path)","title":"<code>initial_path</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.get_drives","title":"<code>get_drives()</code>","text":"<p>Returns a list of the logically assigned drives on a windows system.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of drive letters available and accessible on the system.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def get_drives():\n    \"\"\"\n    Returns a list of the logically assigned drives on a windows system.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of drive letters available and accessible on the system.\n    \"\"\"\n\n    drives = []\n    bitmask = windll.kernel32.GetLogicalDrives()\n\n    for letter in ascii_uppercase:\n        if bitmask &amp; 1:\n            drives.append(letter + \":\")\n        bitmask &gt;&gt;= 1\n\n    return drives\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.int_input","title":"<code>int_input(message, *, min=None, max=None, default=None, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def int_input(\n    message: str,\n    *,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n    default: Optional[int] = None,\n    **kwargs,\n) -&gt; Optional[int]:\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n\n    def validate_value(value):\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValidationError(\"Please enter a valid integer.\")\n\n        if min is not None and value &lt; min:\n            raise ValidationError(\n                f\"Please enter a value greater than or equal to {min}.\"\n            )\n\n        if max is not None and value &gt; max:\n            raise ValidationError(f\"Please enter a value less than or equal to {max}.\")\n\n        return True\n\n    result = wrap_keyboard_interrupt(\n        lambda: inquirer_text(\n            message,\n            validate=lambda previous_answers, value: validate_value(value),\n            default=str(default) if default is not None else None,\n            **kwargs,\n        ),\n        None,\n    )\n    return int(result) if result is not None else None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.list_input","title":"<code>list_input(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s list input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def list_input(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s list input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_list_input(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.text","title":"<code>text(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def text(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_text(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.wrap_keyboard_interrupt","title":"<code>wrap_keyboard_interrupt(fn, fallback=None)</code>","text":"<p>Calls <code>fn</code> and catches KeyboardInterrupt, returning <code>fallback</code> if it occurs.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def wrap_keyboard_interrupt(fn, fallback=None):\n    \"\"\"\n    Calls `fn` and catches KeyboardInterrupt, returning `fallback` if it occurs.\n    \"\"\"\n    try:\n        return fn()\n    except KeyboardInterrupt:\n        return fallback\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils","title":"<code>utils</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_printed_lines","title":"<code>clear_printed_lines(count)</code>","text":"<p>Clear the last <code>count</code> lines of the terminal. Useful for repainting terminal output.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The number of lines to clear</p> required Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_printed_lines(count: int):\n    \"\"\"\n    Clear the last `count` lines of the terminal. Useful for repainting\n    terminal output.\n\n    Args:\n        count (int): The number of lines to clear\n    \"\"\"\n    for _ in range(count + 1):\n        sys.stdout.write(\"\\033[2K\")  # Clear the current line\n        sys.stdout.write(\"\\033[F\")  # Move cursor up one line\n    sys.stdout.write(\"\\033[2K\\r\")  # Clear the last line and move to start\n    sys.stdout.flush()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_printed_lines(count)","title":"<code>count</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_terminal","title":"<code>clear_terminal()</code>","text":"<p>Clears the terminal</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_terminal():\n    \"\"\"Clears the terminal\"\"\"\n    if os.name == \"nt\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box","title":"<code>draw_box(text, *, padding_spaces=5, padding_lines=1)</code>","text":"<p>Draw a box around the given text, which will be centered in the box.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to be drawn, may be multiline. ANSI formatting and emojis are not supported, as they mess with both the character count calculation and the monospace font.</p> required <code>int</code> <p>Extra spaces on either side of the longest line. Defaults to 5.</p> <code>5</code> <code>int</code> <p>Extra lines above and below the text. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text surrounded by a box.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def draw_box(text: str, *, padding_spaces: int = 5, padding_lines: int = 1) -&gt; str:\n    \"\"\"\n    Draw a box around the given text, which will be centered in the box.\n\n    Args:\n        text (str): The text to be drawn, may be multiline.\n          ANSI formatting and emojis are not supported, as they mess with\n          both the character count calculation and the monospace font.\n\n        padding_spaces (int, optional): Extra spaces on either side of the longest line. Defaults to 5.\n        padding_lines (int, optional): Extra lines above and below the text. Defaults to 1.\n\n    Returns:\n        str: The text surrounded by a box.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    width = max(len(line) for line in lines) + padding_spaces * 2\n\n    box = \"\"\n    box += \"\u250c\" + \"\u2500\" * width + \"\u2510\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    for line in lines:\n        padding = \" \" * padding_spaces\n        box += \"\u2502\" + padding + line.center(width - 2 * padding_spaces) + padding + \"\u2502\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    box += \"\u2514\" + \"\u2500\" * width + \"\u2518\\n\"\n    return box\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(text)","title":"<code>text</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(padding_spaces)","title":"<code>padding_spaces</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box(padding_lines)","title":"<code>padding_lines</code>","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.enable_windows_ansi_support","title":"<code>enable_windows_ansi_support()</code>","text":"<p>Set up the Windows terminal to support ANSI escape codes, which will be needed for colored text, line clearing, and other terminal features.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def enable_windows_ansi_support():\n    \"\"\"\n    Set up the Windows terminal to support ANSI escape codes, which will be needed\n    for colored text, line clearing, and other terminal features.\n    \"\"\"\n    if os.name == \"nt\":\n        # Enable ANSI escape code support for Windows\n        # On Windows, calling os.system('') with an empty string doesn't\n        # run any actual command. However, there's an undocumented side\n        # effect: it forces the Windows terminal to initialize or refresh\n        # its state, enabling certain features like the processing of ANSI\n        # escape codes, which might not otherwise be active.\n        os.system(\"\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.is_wsl","title":"<code>is_wsl()</code>","text":"<p>Check if the environment is WSL2.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def is_wsl() -&gt; bool:\n    \"\"\"Check if the environment is WSL2.\"\"\"\n    try:\n        with open(\"/proc/version\", \"r\") as f:\n            return \"microsoft\" in f.read().lower()\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.wait_for_key","title":"<code>wait_for_key(prompt=False)</code>","text":"<p>Waits for the user to press any key</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>If true, a default text</p> <code>False</code> Source code in <code>terminal_tools/utils.py</code> <pre><code>def wait_for_key(prompt: bool = False):\n    \"\"\"Waits for the user to press any key\n\n    Args:\n        prompt (bool, optional): If true, a default text\n        `Press any key to continue` will be shown. Defaults to False.\n    \"\"\"\n    if prompt:\n        print(\"Press any key to continue...\", end=\"\", flush=True)\n    _wait_for_key()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.wait_for_key(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/testing/","title":"Testing","text":""},{"location":"reference/testing/#testing","title":"<code>testing</code>","text":""},{"location":"reference/testing/#testing.testers","title":"<code>testers</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer","title":"<code>test_primary_analyzer(interface, main, *, input, outputs, params=dict())</code>","text":"<p>Runs the primary analyzer test.</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The interface of the analyzer.</p> required <code>Callable[[PrimaryAnalyzerContext], None]</code> <p>The main function of the analyzer.</p> required <code>TestData</code> <p>The input data.</p> required <code>dict[str, ParamValue]</code> <p>(Optional) The analysis parameters.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The output data, keyed by output ID.</p> required Source code in <code>testing/testers.py</code> <pre><code>@pytest.mark.skip()\ndef test_primary_analyzer(\n    interface: AnalyzerInterface,\n    main: Callable[[PrimaryAnalyzerContext], None],\n    *,\n    input: TestData,\n    outputs: dict[str, TestData],\n    params: dict[str, ParamValue] = dict(),\n):\n    \"\"\"\n    Runs the primary analyzer test.\n\n    Args:\n        interface (AnalyzerInterface): The interface of the analyzer.\n        main (Callable[[PrimaryAnalyzerContext], None]): The main function of the analyzer.\n        input (TestData): The input data.\n        params (dict[str, ParamValue]): (Optional) The analysis parameters.\n        outputs (dict[str, TestData]): The output data, keyed by output ID.\n    \"\"\"\n    with ExitStack() as exit_stack:\n        temp_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_output_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_input_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n\n        input_path = os.path.join(actual_input_dir, \"input.parquet\")\n        input.convert_to_parquet(input_path)\n\n        context = TestPrimaryAnalyzerContext(\n            temp_dir=temp_dir,\n            input_parquet_path=input_path,\n            param_values=params,\n            output_parquet_root_path=actual_output_dir,\n        )\n        main(context)\n\n        specified_outputs = [output_spec.id for output_spec in interface.outputs]\n        unused_outputs = [\n            output_id\n            for output_id in outputs.keys()\n            if output_id not in specified_outputs\n        ]\n        if unused_outputs:\n            raise ValueError(\n                f\"The test case provided outputs that are not specified in the interface: {unused_outputs}\"\n            )\n\n        has_compared_output = any(\n            outputs.get(output_spec.id) is not None for output_spec in interface.outputs\n        )\n        if not has_compared_output:\n            raise ValueError(\"The test case did not compare any outputs.\")\n\n        for output_spec in interface.outputs:\n            expected_output_data = outputs.get(output_spec.id)\n            if expected_output_data is None:\n                continue\n\n            actual_output_path = context.output_path(output_spec.id)\n\n            expected_output = expected_output_data.load()\n            actual_output = pl.read_parquet(actual_output_path)\n            compare_dfs(actual_output, expected_output)\n</code></pre>"},{"location":"reference/testing/#testing.testers.test_primary_analyzer(interface)","title":"<code>interface</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(main)","title":"<code>main</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(input)","title":"<code>input</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(params)","title":"<code>params</code>","text":""},{"location":"reference/testing/#testing.testers.test_primary_analyzer(outputs)","title":"<code>outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer","title":"<code>test_secondary_analyzer(interface, main, *, primary_params=dict(), primary_outputs, dependency_outputs=dict(), expected_outputs)</code>","text":"<p>Runs the secondary analyzer test.</p> <p>Parameters:</p> Name Type Description Default <code>AnalyzerInterface</code> <p>The interface of the analyzer.</p> required <code>Callable[[SecondaryAnalyzerInterface], None]</code> <p>The main function of the analyzer.</p> required <code>dict[str, ParamValue]</code> <p>(Optional) The primary analysis parameters.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The primary output data, keyed by output ID.</p> required <code>dict[str, dict[str, TestData]]</code> <p>The dependency output data, keyed by dependency ID and then by output ID.</p> <code>dict()</code> <code>dict[str, TestData]</code> <p>The expected output data, keyed by output ID.</p> required Source code in <code>testing/testers.py</code> <pre><code>@pytest.mark.skip()\ndef test_secondary_analyzer(\n    interface: AnalyzerInterface,\n    main: Callable[[SecondaryAnalyzerContext], None],\n    *,\n    primary_params: dict[str, ParamValue] = dict(),\n    primary_outputs: dict[str, TestData],\n    dependency_outputs: dict[str, dict[str, TestData]] = dict(),\n    expected_outputs: dict[str, TestData],\n):\n    \"\"\"\n    Runs the secondary analyzer test.\n\n    Args:\n        interface (AnalyzerInterface): The interface of the analyzer.\n        main (Callable[[SecondaryAnalyzerInterface], None]): The main function of the analyzer.\n        primary_params (dict[str, ParamValue]): (Optional) The primary analysis parameters.\n        primary_outputs (dict[str, TestData]): The primary output data, keyed by output ID.\n        dependency_outputs (dict[str, dict[str, TestData]]): The dependency output data, keyed by dependency ID and then by output ID.\n        expected_outputs (dict[str, TestData]): The expected output data, keyed by output ID.\n    \"\"\"\n    with ExitStack() as exit_stack:\n        temp_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_output_dir = exit_stack.enter_context(TemporaryDirectory(delete=True))\n        actual_base_output_dir = exit_stack.enter_context(\n            TemporaryDirectory(delete=True)\n        )\n        actual_dependency_output_dirs = {\n            dependency_id: exit_stack.enter_context(TemporaryDirectory(delete=True))\n            for dependency_id in dependency_outputs.keys()\n        }\n\n        for output_id, output_data in primary_outputs.items():\n            output_data.convert_to_parquet(\n                os.path.join(actual_base_output_dir, f\"{output_id}.parquet\")\n            )\n\n        for dependency_id, dependency_output in dependency_outputs.items():\n            for output_id, output_data in dependency_output.items():\n                output_data.convert_to_parquet(\n                    os.path.join(\n                        actual_dependency_output_dirs[dependency_id],\n                        f\"{output_id}.parquet\",\n                    )\n                )\n\n        context = TestSecondaryAnalyzerContext(\n            temp_dir=temp_dir,\n            primary_param_values=primary_params,\n            primary_output_parquet_paths={\n                output_id: os.path.join(actual_base_output_dir, f\"{output_id}.parquet\")\n                for output_id in primary_outputs.keys()\n            },\n            dependency_output_parquet_paths={\n                dependency_id: {\n                    output_id: os.path.join(\n                        actual_dependency_output_dirs[dependency_id],\n                        f\"{output_id}.parquet\",\n                    )\n                    for output_id in dependency_output.keys()\n                }\n                for dependency_id, dependency_output in dependency_outputs.items()\n            },\n            output_parquet_root_path=actual_output_dir,\n        )\n        main(context)\n\n        specified_outputs = [output_spec.id for output_spec in interface.outputs]\n        unused_outputs = [\n            output_id\n            for output_id in expected_outputs.keys()\n            if output_id not in specified_outputs\n        ]\n        if unused_outputs:\n            raise ValueError(\n                f\"The test case provided outputs that are not specified in the interface: {unused_outputs}\"\n            )\n\n        has_compared_output = any(\n            expected_outputs.get(output_spec.id) is not None\n            for output_spec in interface.outputs\n        )\n        if not has_compared_output:\n            raise ValueError(\"The test case did not compare any outputs.\")\n\n        for output_spec in interface.outputs:\n            expected_output_data = expected_outputs.get(output_spec.id)\n            if expected_output_data is None:\n                continue\n\n            actual_output_path = context.output_path(output_spec.id)\n\n            expected_output = expected_output_data.load()\n            actual_output = pl.read_parquet(actual_output_path)\n            compare_dfs(actual_output, expected_output)\n</code></pre>"},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(interface)","title":"<code>interface</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(main)","title":"<code>main</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(primary_params)","title":"<code>primary_params</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(primary_outputs)","title":"<code>primary_outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(dependency_outputs)","title":"<code>dependency_outputs</code>","text":""},{"location":"reference/testing/#testing.testers.test_secondary_analyzer(expected_outputs)","title":"<code>expected_outputs</code>","text":""}]}